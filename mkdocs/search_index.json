{
    "docs": [
        {
            "location": "/",
            "text": "Introduction\n\n\nWelcome to the \nRed Hat Gluster Storage\n Hands-on Test Drive. To make your Gluster experience awesome, the contents of this test drive have been divided into the following modules.\n\n\n\n\nModule 1 :\n Introduction to Gluster concepts\n\n\nModule 2 :\n Volume Setup and Client Access\n\n\nModule 3 :\n Volume Operations and Administration\n\n\nComing Soon\n \nModule 4 :\n Disperse Volumes (Erasure Coding)\n\n\nComing Soon\n \nModule 5 :\n Tiered Volumes (Cache Tiering)\n\n\nComing Soon\n \nModule 6 :\n Gluster Internals\n\n\n\n\nWhat is Gluster?\n\n\nGluster provides open, software-defined file storage that scales out as much as you need. You can easily and securely manage large, unstructured, and semi-structured data at a fraction of the cost of traditional, monolithic storage. And only Red Hat lets you deploy the same storage on premise; in private, public, or hybrid clouds; and in Linux\u00ae containers. You can read more about gluster here: \nhttps://www.redhat.com/en/technologies/storage/gluster\n\n\nAbout the Test Drive\n\n\nThe \nRed Hat Gluster Storage\n Hands-on Test Drive is designed in a progressive modular format. Newcomers to Gluster will generally have the best experience by following the modules and steps in order. To faciliate skipping modules or resuming progress at a later date, the modules are also designed to be independent and not reliant on the activities of any preceeding module.\n\n\nWhile the guided lab processes is designed to offer a progressive educational experience, you are also encouraged to use the lab in a free-form manner to explore Gluster and its features. For your convenience, a reference of the lab resources is provided below.\n\n\nLab Resources\n\n\n\n\n\n\n\n\nLab Node\n\n\nInternal IP Address\n\n\nFunction\n\n\n\n\n\n\n\n\n\n\nrhgs1\n\n\n10.100.1.11\n\n\nLocal Gluster server 1\n\n\n\n\n\n\nrhgs2\n\n\n10.100.1.12\n\n\nLocal Gluster server 2\n\n\n\n\n\n\nrhgs3\n\n\n10.100.1.13\n\n\nLocal Gluster server 3\n\n\n\n\n\n\nrhgs4\n\n\n10.100.1.14\n\n\nLocal Gluster server 4\n\n\n\n\n\n\nrhgs5\n\n\n10.100.1.15\n\n\nLocal Gluster server 5\n\n\n\n\n\n\nrhgs6\n\n\n10.100.1.16\n\n\nLocal Gluster server 6\n\n\n\n\n\n\nclient1\n\n\n10.100.1.101\n\n\nRHEL client 1\n\n\n\n\n\n\nclient2\n\n\n10.100.1.102\n\n\nRHEL client 2\n\n\n\n\n\n\nwinclient\n\n\n10.100.1.103\n\n\nWindows client\n\n\n\n\n\n\n\n\nTest Drive Prerequisites\n\n\nSSH and RDP\n\n\nIf you are a Windows user, you will need a Secure Shell client like \nPuTTY\n to connect to your instance. If you do not have it already, you can download the PuTTY client here: \nhttp://the.earth.li/~sgtatham/putty/latest/x86/putty.exe\n\n\nMac and Linux users, you will use your preferred terminal application (this should already be installed on your machine). For accessing the Windows lab client system interface via RDP, you will need a RDP client program such as \nVinagre\n on Linux or the \nMicrosoft Remote Desktop\n client for Mac. Vinagre is likely available for your Linux distribution using your standard package manager. You can download the Mac RDP client here: \nhttps://www.microsoft.com/en-us/download/details.aspx?id=18140\n\n\nGetting to Know Your Lab Environment\n\n\nStarting the Lab\n\n\n\n\nNOTE\n Module 1 is not hands-on and does not require the lab to be started. Because the time limit will begin after clicking the \nStart\n button, you may wish to wait until you are ready to begin a later module.\n\n\n\n\n\n\nOn the \nLab Details\n tab to the right, notice the lab properties:\n\n\n\n\n\n\n\n\nSetup Time -\n The estimated time for the lab to start your instance so you can access the lab environment.\n\n\nDuration -\n The estimated time the lab should take to complete.\n\n\nAccess -\n The time the lab will run before automatically shutting down.\n\n\n\n\nClick the \n button in the navigation bar above to launch your lab. If you are prompted for a token, use the one distributed to you (or credits you've purchased).\n\n\n\n\n\nNOTE\n It may take \nup to 10 minutes\n for your lab systems to start up before you can access them.\n\n\n\n\nA status bar will then show the progress of the lab environment creation process. Your lab resources may not be fully available until the process is complete.\n\n\n\n\nThe \nConnect\n panel to the right is automatically opened when the lab starts. Practice closing and re-opening it. While open it may obscure some of these lab instructions temporarily.\n\n\n\n\n\nTIP\n If the \nConnect\n tab is unavailable, make sure you have clicked \nStart Lab\n at the top of your screen.\n\n\n\n\nAccessing the lab\n\n\nAll of the information you need to access your test drive lab instances is available via the \nAddl. Info\n tab to the right. There you will find relevant public IP addresses, usernames, and passwords for your personal lab environment.\n\n\n\n\nTIP\n If the \nAddl. Info\n tab is not visible, make sure you have clicked the \nStart Lab\n button at the top of your screen and that the lab build has completed.\n\n\n\n\nAll Linux instances may be accessed via your local SSH client.\n\n\nAll Windows instances may be accessed via your local RDP client. \nWindows instances will be logged into with the\n \nAdministrator\n \nusername and with the password available in the\n \nAddl. Info\n \ntab.",
            "title": "Introduction"
        },
        {
            "location": "/#introduction",
            "text": "Welcome to the  Red Hat Gluster Storage  Hands-on Test Drive. To make your Gluster experience awesome, the contents of this test drive have been divided into the following modules.   Module 1 :  Introduction to Gluster concepts  Module 2 :  Volume Setup and Client Access  Module 3 :  Volume Operations and Administration  Coming Soon   Module 4 :  Disperse Volumes (Erasure Coding)  Coming Soon   Module 5 :  Tiered Volumes (Cache Tiering)  Coming Soon   Module 6 :  Gluster Internals",
            "title": "Introduction"
        },
        {
            "location": "/#what-is-gluster",
            "text": "Gluster provides open, software-defined file storage that scales out as much as you need. You can easily and securely manage large, unstructured, and semi-structured data at a fraction of the cost of traditional, monolithic storage. And only Red Hat lets you deploy the same storage on premise; in private, public, or hybrid clouds; and in Linux\u00ae containers. You can read more about gluster here:  https://www.redhat.com/en/technologies/storage/gluster",
            "title": "What is Gluster?"
        },
        {
            "location": "/#about-the-test-drive",
            "text": "The  Red Hat Gluster Storage  Hands-on Test Drive is designed in a progressive modular format. Newcomers to Gluster will generally have the best experience by following the modules and steps in order. To faciliate skipping modules or resuming progress at a later date, the modules are also designed to be independent and not reliant on the activities of any preceeding module.  While the guided lab processes is designed to offer a progressive educational experience, you are also encouraged to use the lab in a free-form manner to explore Gluster and its features. For your convenience, a reference of the lab resources is provided below.",
            "title": "About the Test Drive"
        },
        {
            "location": "/#lab-resources",
            "text": "Lab Node  Internal IP Address  Function      rhgs1  10.100.1.11  Local Gluster server 1    rhgs2  10.100.1.12  Local Gluster server 2    rhgs3  10.100.1.13  Local Gluster server 3    rhgs4  10.100.1.14  Local Gluster server 4    rhgs5  10.100.1.15  Local Gluster server 5    rhgs6  10.100.1.16  Local Gluster server 6    client1  10.100.1.101  RHEL client 1    client2  10.100.1.102  RHEL client 2    winclient  10.100.1.103  Windows client",
            "title": "Lab Resources"
        },
        {
            "location": "/#test-drive-prerequisites",
            "text": "",
            "title": "Test Drive Prerequisites"
        },
        {
            "location": "/#ssh-and-rdp",
            "text": "If you are a Windows user, you will need a Secure Shell client like  PuTTY  to connect to your instance. If you do not have it already, you can download the PuTTY client here:  http://the.earth.li/~sgtatham/putty/latest/x86/putty.exe  Mac and Linux users, you will use your preferred terminal application (this should already be installed on your machine). For accessing the Windows lab client system interface via RDP, you will need a RDP client program such as  Vinagre  on Linux or the  Microsoft Remote Desktop  client for Mac. Vinagre is likely available for your Linux distribution using your standard package manager. You can download the Mac RDP client here:  https://www.microsoft.com/en-us/download/details.aspx?id=18140",
            "title": "SSH and RDP"
        },
        {
            "location": "/#getting-to-know-your-lab-environment",
            "text": "",
            "title": "Getting to Know Your Lab Environment"
        },
        {
            "location": "/#starting-the-lab",
            "text": "NOTE  Module 1 is not hands-on and does not require the lab to be started. Because the time limit will begin after clicking the  Start  button, you may wish to wait until you are ready to begin a later module.    On the  Lab Details  tab to the right, notice the lab properties:     Setup Time -  The estimated time for the lab to start your instance so you can access the lab environment.  Duration -  The estimated time the lab should take to complete.  Access -  The time the lab will run before automatically shutting down.   Click the   button in the navigation bar above to launch your lab. If you are prompted for a token, use the one distributed to you (or credits you've purchased).   NOTE  It may take  up to 10 minutes  for your lab systems to start up before you can access them.   A status bar will then show the progress of the lab environment creation process. Your lab resources may not be fully available until the process is complete.   The  Connect  panel to the right is automatically opened when the lab starts. Practice closing and re-opening it. While open it may obscure some of these lab instructions temporarily.   TIP  If the  Connect  tab is unavailable, make sure you have clicked  Start Lab  at the top of your screen.",
            "title": "Starting the Lab"
        },
        {
            "location": "/#accessing-the-lab",
            "text": "All of the information you need to access your test drive lab instances is available via the  Addl. Info  tab to the right. There you will find relevant public IP addresses, usernames, and passwords for your personal lab environment.   TIP  If the  Addl. Info  tab is not visible, make sure you have clicked the  Start Lab  button at the top of your screen and that the lab build has completed.   All Linux instances may be accessed via your local SSH client.  All Windows instances may be accessed via your local RDP client.  Windows instances will be logged into with the   Administrator   username and with the password available in the   Addl. Info   tab.",
            "title": "Accessing the lab"
        },
        {
            "location": "/gluster-module-1/",
            "text": "Gluster Test Drive Module 1 \n Introduction to Gluster Concepts\n\n\n\n\nModule 1 is an introduction to Gluster and does not include any hands-on work in the lab. When you are ready, please proceed to \nModule 2\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Placement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Accessibility\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeployment",
            "title": "Module 1 - Introduction to Gluster concepts"
        },
        {
            "location": "/gluster-module-1/#gluster-test-drive-module-1-introduction-to-gluster-concepts",
            "text": "Module 1 is an introduction to Gluster and does not include any hands-on work in the lab. When you are ready, please proceed to  Module 2 .",
            "title": "Gluster Test Drive Module 1  Introduction to Gluster Concepts"
        },
        {
            "location": "/gluster-module-1/#data-placement",
            "text": "",
            "title": "Data Placement"
        },
        {
            "location": "/gluster-module-1/#data-accessibility",
            "text": "",
            "title": "Data Accessibility"
        },
        {
            "location": "/gluster-module-1/#deployment",
            "text": "",
            "title": "Deployment"
        },
        {
            "location": "/gluster-module-2/",
            "text": "Lab Guide \n Gluster Test Drive Module 2 \n Volume Setup and Client Access\n\n\nLab Agenda\n\n\nWelcome to the Gluster Test Drive Module 2 - Volume Setup and Client Access. In this lab you will:\n\n\n\n\nCreate a Gluster trusted pool\n\n\nManually create a Gluster distributed volume\n\n\nAutomatically create a Gluster replicated volume using \ngdeploy\n\n\nUnderstand basic Gluster CLI commands\n\n\nWrite files to your volumes with the Gluter native client, NFS, and SMB\n\n\nObserve the file layout on the Gluster bricks\n\n\n\n\nGetting Started\n\n\nIf you have not already done so, click the \n button in the navigation bar above to launch your lab. If you are prompted for a token, use the one distributed to you (or credits you\u2019ve purchased).\n\n\n\n\nNOTE\n It may take \nup to 10 minutes\n for your lab systems to start up before you can access them.\n\n\n\n\nCreating the Trusted Pool\n\n\nConnect to the Lab\n\n\nConnect to the \nrhgs1\n server instance using its public IP address from the \nAddl. Info\n tab to the right (Linux/Mac example below).\n\n\nssh gluster@<rhgs1PublicIP>\n\n\n\n\nNode Peering\n\n\nThe first step in creating a Gluster trusted pool is node peering. From one node, all of the other nodes should be peered using the gluster CLI. Your lab has 6 Gluster nodes in the local subnet. You will run \npeer probe\n commands from only node \nrhgs1\n.\n\n\nsudo gluster peer probe rhgs2\nsudo gluster peer probe rhgs3\nsudo gluster peer probe rhgs4\nsudo gluster peer probe rhgs5\nsudo gluster peer probe rhgs6\n\n\n\n\nNote the success Messages:\n\n\npeer probe: success.\n\n\nObserve the Trusted Pool\n\n\nA \ntrusted pool\n is defined as a group of Gluster nodes peered together for the purpose of sharing their local storage and compute resources for one or more logical filesystem namespaces. The state of a trusted pool and its members can be viewed with two important commands: \ngluster peer status\n and \ngluster pool list\n.\n\n\nsudo gluster peer status\n\n\n\n\nNote that the \npeer status\n command only reports the remote peers of the local node from which the command is run, excluding itself (localhost) from the list. This can be confusing for first-time users. In our case, there are 6 members of the trusted pool but only 5 peers reported by the command.\n\n\nNumber of Peers: 5\n\n\nHostname: rhgs2\n\n\nUuid: 64edb288-524e-4918-b421-74b76b80a44b\n\n\nState: Peer in Cluster (Connected)\n\n\nHostname: rhgs3\n\n\nUuid: e22261c4-3e34-49e3-b4e3-da9a8c449471\n\n\nState: Peer in Cluster (Connected)\n\n\nHostname: rhgs4\n\n\nUuid: fbd4ab10-f50b-4ca8-b973-a9a54dfed47d\n\n\nState: Peer in Cluster (Connected)\n\n\nHostname: rhgs5\n\n\nUuid: c351dead-7326-4370-a6fe-90d7b5ac8b45\n\n\nState: Peer in Cluster (Connected)\n\n\nHostname: rhgs6\n\n\nUuid: 8b8384ae-cc6e-4403-ab3b-b7cc3c21029f\n\n\nState: Peer in Cluster (Connected)\n\n\nThe \npool list\n command provides similar output in a tabular format and includes the local node in the list, thus giving a complete view of the Gluster trusted pool.\n\n\nsudo gluster pool list\n\n\n\n\nUUID                  Hostname    State\n\n\n64edb288-524e-4918-b421-74b76b80a44b  rhgs2       Connected\n\n\ne22261c4-3e34-49e3-b4e3-da9a8c449471  rhgs3       Connected\n\n\nfbd4ab10-f50b-4ca8-b973-a9a54dfed47d  rhgs4       Connected\n\n\nc351dead-7326-4370-a6fe-90d7b5ac8b45  rhgs5       Connected\n\n\n8b8384ae-cc6e-4403-ab3b-b7cc3c21029f  rhgs6       Connected\n\n\nc0f78e93-ad8f-4849-b8f3-ebdb2f7f4d17  localhost   Connected\n\n\nCreating a Distributed Volume\n\n\nAbout Bricks and Distribution\n\n\nA unit of storage on a node is referred to as a \nbrick\n. A brick is simply a local filesystem that has been presented to the Gluster pool for consumption. Each brick has an associated \nglusterfsd\n process on its system.\n\n\nWhen a Gluster volume is created, its default architecture is \ndistribution\n. A distributed volume simply groups storage from different bricks together into one unified namespace, resulting in a Gluster volume that is as large as the sum of all of its bricks. This architecture uses a \nhashing algorithm\n for pseudo-random placement of files, resulting in statistically even distribution of files across the bricks.\n\n\nYou will use the gluster CLI to create a 6-brick distributed volume named \ndistvol\n.\n\n\n\n\nNote that for the sake of this lab the backing filesystems on the nodes have been pre-configured and mounted to \n/rhgs/brick_xvdb\n. You can view the existing LVM and filesystem configurations using the commands below.\n\n\n\n\nsudo vgdisplay -v /dev/rhgs_vg1\n\n\n\n\nUsing volume group(s) on command line.\n\n\n--- Volume group ---\n\n\nVG Name               rhgs_vg1\n\n\nSystem ID\n\n\nFormat                lvm2\n\n\nMetadata Areas        1\n\n\nMetadata Sequence No  7\n\n\nVG Access             read/write\n\n\nVG Status             resizable\n\n\nMAX LV                0\n\n\nCur LV                2\n\n\nOpen LV               1\n\n\nMax PV                0\n\n\nCur PV                1\n\n\nAct PV                1\n\n\nVG Size               10.00 GiB\n\n\nPE Size               4.00 MiB\n\n\nTotal PE              2559\n\n\nAlloc PE / Size       2559 / 10.00 GiB\n\n\nFree  PE / Size       0 / 0\n\n\nVG UUID               sHGNaI-ODzz-aZV3-j602-aDZQ-DUUU-ddbK7X\n\n\n--- Logical volume ---\n\n\nLV Name                rhgs_thinpool1\n\n\nVG Name                rhgs_vg1\n\n\nLV UUID                A3FlQv-X8K7-IQE0-cYUZ-eEJk-ROVe-hkboxV\n\n\nLV Write Access        read/write\n\n\nLV Creation host, time rhgs1.gluster.lab, 2016-07-27 10:56:26 -0400\n\n\nLV Pool metadata       rhgs_thinpool1_tmeta\n\n\nLV Pool data           rhgs_thinpool1_tdata\n\n\nLV Status              available\n\n\n# open                 2\n\n\nLV Size                9.97 GiB\n\n\nAllocated pool data    0.11%\n\n\nAllocated metadata     0.65%\n\n\nCurrent LE             2553\n\n\nSegments               1\n\n\nAllocation             inherit\n\n\nRead ahead sectors     auto\n\n\n- currently set to     8192\n\n\nBlock device           253:2\n\n\n--- Logical volume ---\n\n\nLV Path                /dev/rhgs_vg1/rhgs_lv1\n\n\nLV Name                rhgs_lv1\n\n\nVG Name                rhgs_vg1\n\n\nLV UUID                L2f6yD-NhfH-2Mm7-gX5S-b8Ee-2dXL-Pb0HGK\n\n\nLV Write Access        read/write\n\n\nLV Creation host, time rhgs1.gluster.lab, 2016-07-27 10:56:49 -0400\n\n\nLV Pool name           rhgs_thinpool1\n\n\nLV Status              available\n\n\n# open                 1\n\n\nLV Size                10.00 GiB\n\n\nMapped size            0.11%\n\n\nCurrent LE             2560\n\n\nSegments               1\n\n\nAllocation             inherit\n\n\nRead ahead sectors     auto\n\n\n- currently set to     8192\n\n\nBlock device           253:4\n\n\n--- Physical volumes ---\n\n\nPV Name               /dev/xvdb\n\n\nPV UUID               OmPpve-V6qZ-fcvx-DFMq-nrPr-Aeoh-0X2FFg\n\n\nPV Status             allocatable\n\n\nTotal PE / Free PE    2559 / 0\n\n\nsudo df -h /rhgs/brick_xvdb\n\n\n\n\nFilesystem                     Size  Used Avail Use% Mounted on\n\n\n/dev/mapper/rhgs_vg1-rhgs_lv1   10G   33M   10G   1% /rhgs/brick_xvdb\n\n\nCreate the Distributed Volume\n\n\nNow create the 6-brick \ndistvol\n Gluster volume.\n\n\n\n\nNOTE that the \n\\\n characters below are for line continuation to aid copy-and-paste of the command. The \ngluster volume create\n is a single-line command.\n\n\n\n\nsudo gluster volume create distvol \\\nrhgs1:/rhgs/brick_xvdb/distvol \\\nrhgs2:/rhgs/brick_xvdb/distvol \\\nrhgs3:/rhgs/brick_xvdb/distvol \\\nrhgs4:/rhgs/brick_xvdb/distvol \\\nrhgs5:/rhgs/brick_xvdb/distvol \\\nrhgs6:/rhgs/brick_xvdb/distvol\n\n\n\n\nNote in the output that the volume was created successfuly and you now need to start the volume:\n\n\nvolume create: distvol: success: please start the volume to access data\n\n\nStart the volume\n\n\nsudo gluster volume start distvol\n\n\n\n\nvolume start: distvol: success\n\n\nAutomated Creation of a Replicate Volume\n\n\nAbout gdeploy and Replication\n\n\nGluster volume configurations can become much more complicated than the basic example above as the scale of the environment grows and additional features are leveraged. In order to simplify and automate the deployment process, Red Hat has introduced an \nAnsible\n-based deployment tool called \ngdeploy\n. With gdeploy, an end-to-end Gluster architecture can be defined in a configuration file, and the entire deployment can be orchestrated with a single command.\n\n\nYou will now build a 2-brick \nreplicated\n volume using the gdeploy method. A replicated volume architecture groups Gluster bricks into \nreplica peers\n, synchronously storing multiple copies of the files as they are being written by the Gluster clients.\n\n\n\n\nFor this replicated volume deployment, the backing filesystems have \nnot\n been pre-configured as in the above distributed volume example. The provided gdeploy configuration file includes all of the information needed to setup the \n/dev/xvdc\n block devices with \nLVM thin provisioning\n and format and mount an \nXFS filesystem\n. It then further defines the Gluster volume architecture for your \nrepvol\n volume.\n\n\n\n\nView the gdeploy configuration file with the below command.\n\n\ncat ~/repvol.conf\n\n\n\n\n[hosts]\n\n\nrhgs1\n\n\nrhgs2\n\n\n\n\n# Common backend setup for 2 of the hosts.\n\n\n[backend-setup]\n\n\ndevices=xvdc\n\n\nvgs=rhgs_vg2\n\n\npools=rhgs_thinpool2\n\n\nlvs=rhgs_lv2\n\n\nmountpoints=/rhgs/brick_xvdc\n\n\nbrick_dirs=/rhgs/brick_xvdc/repvol\n\n\n\n\n[volume]\n\n\naction=create\n\n\nvolname=repvol\n\n\nreplica=yes\n\n\nreplica_count=2\n\n\nforce=yes\n\n\nIn order to use \ngdeploy\n, the node from which it is run requires passwordless ssh access to the root account on all nodes in the Gluster trusted pool (including itself, if the gdeploy node is also a Gluster pool node, as it is in this example).\n\n\n\n\nNOTE:\n \nYour Amazon AWS lab by default uses only keypairs for SSH authentication and configures no password-based access to the instances. Because of this, we have pre-populated keys to allow the gluster user on each node to login as the root user on all nodes using the \n~/.ssh/id_rsa\n private key.\n \nThe commands below are for reference only and do not need to be run for this lab.\n\n\n\n\nssh-keygen -f ~/.ssh/id_rsa -t rsa -N ''\n\n\nfor i in {1..6}; do ssh-copy-id -i ~/.ssh/id_rsa root@rhgs$i; done\n\n\nCreate the Replicated Volume\n\n\nWith passwordless ssh configured, you can deploy the \nrepvol\n volume using the \ngdeploy\n command (NOTE because we rely on the ssh keys, you do not need to use \nsudo\n for this command).\n\n\ngdeploy -vv -c ~/repvol.conf\n\n\n\n\nViewing Volume Details\n\n\nInformation about Gluster volumes, including their architecture, configuration specifics, processes, and ports can be viewed with the below two commands.\n\n\nThe \ngluster volume info\n command shows configuration and operational details about your Gluster volumes. You can also pass a specific volume name at the end of the command to show output for only one volume.\n\n\nsudo gluster volume info\n\n\n\n\nVolume Name: distvol\n\n\nType: Distribute\n\n\nVolume ID: 95b1dc7f-277d-47b4-95db-e1981a8f18b9\n\n\nStatus: Started\n\n\nNumber of Bricks: 6\n\n\nTransport-type: tcp\n\n\nBricks:\n\n\nBrick1: rhgs1:/rhgs/brick_xvdb/distvol\n\n\nBrick2: rhgs2:/rhgs/brick_xvdb/distvol\n\n\nBrick3: rhgs3:/rhgs/brick_xvdb/distvol\n\n\nBrick4: rhgs4:/rhgs/brick_xvdb/distvol\n\n\nBrick5: rhgs5:/rhgs/brick_xvdb/distvol\n\n\nBrick6: rhgs6:/rhgs/brick_xvdb/distvol\n\n\nOptions Reconfigured:\n\n\nperformance.readdir-ahead: on\n\n\n \n\n\nVolume Name: repvol\n\n\nType: Replicate\n\n\nVolume ID: c16eaa3d-7ccc-42ad-bd71-d43bb16229b2\n\n\nStatus: Started\n\n\nNumber of Bricks: 1 x 2 = 2\n\n\nTransport-type: tcp\n\n\nBricks:\n\n\nBrick1: rhgs1:/rhgs/brick_xvdc/repvol\n\n\nBrick2: rhgs2:/rhgs/brick_xvdc/repvol\n\n\nOptions Reconfigured:\n\n\nperformance.readdir-ahead: on\n\n\nThe \ngluster volume status\n command provides other details about the operational state of volume, including process IDs and TCP ports. Here you will view the output specifically for volume \nrepvol\n.\n\n\nsudo gluster volume status repvol\n\n\n\n\nStatus of volume: repvol\n\n\nGluster process                             TCP Port  RDMA Port  Online  Pid\n\n\n------------------------------------------------------------------------------\n\n\nBrick rhgs1:/rhgs/brick_xvdc/repvol         49153     0          Y       13363\n\n\nBrick rhgs2:/rhgs/brick_xvdc/repvol         49153     0          Y       12465\n\n\nNFS Server on localhost                     2049      0          Y       13385\n\n\nSelf-heal Daemon on localhost               N/A       N/A        Y       13390\n\n\nNFS Server on rhgs5                         2049      0          Y       11945\n\n\nSelf-heal Daemon on rhgs5                   N/A       N/A        Y       11950\n\n\nNFS Server on rhgs3                         2049      0          Y       11945\n\n\nSelf-heal Daemon on rhgs3                   N/A       N/A        Y       11950\n\n\nNFS Server on rhgs2                         2049      0          Y       12487\n\n\nSelf-heal Daemon on rhgs2                   N/A       N/A        Y       12492\n\n\nNFS Server on rhgs4                         2049      0          Y       11974\n\n\nSelf-heal Daemon on rhgs4                   N/A       N/A        Y       11979\n\n\nNFS Server on rhgs6                         2049      0          Y       11833\n\n\nSelf-heal Daemon on rhgs6                   N/A       N/A        Y       11838\n\n\n\n\nTask Status of Volume repvol\n\n\n------------------------------------------------------------------------------\n\n\nThere are no active volume tasks\n\n\nNFS Client Access\n\n\nA Gluster volume can be accessed through multiple standard client protocols, as well as through specialized methods including the OpenStack Swift protocol and a direct API.\n\n\nFor many common use cases, the well-established NFS protocol is used for ease of implementation and compatibility with existing applications and architectures. For some use cases, the NFS protocol may also offer a performance benefit over other access methods.\n\n\nYou can connect to the \nclient1\n system via \nssh\n directly from the rhgs1 system.\n\n\nssh gluster@client1\n\n\n\n\nHere, on the RHEL client, you will mount via NFS the Gluster \ndistvol\n volume you created above.\n\n\nsudo mkdir -p /rhgs/client/nfs/distvol\nsudo mount -t nfs rhgs1:/distvol /rhgs/client/nfs/distvol\n\n\n\n\nCheck the mount and observe the output.\n\n\ndf -h /rhgs/client/nfs/distvol\n\n\n\n\nFilesystem      Size  Used Avail Use% Mounted on\n\n\nrhgs1:/distvol   60G  198M   60G   1% /rhgs/client/nfs/distvol\n\n\nmount | grep distvol\n\n\n\n\nrhgs1:/distvol on /rhgs/client/nfs/distvol type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.100.1.11,mountvers=3,mountport=38465,mountproto=tcp,local_lock=none,addr=10.100.1.11)\n\n\nCreate and set permissions on a subdirectory to hold your data.\n\n\nsudo mkdir /rhgs/client/nfs/distvol/mydir\nsudo chmod 777 /rhgs/client/nfs/distvol/mydir\n\n\n\n\nAdd 100 files to the directory.\n\n\n\n\nFeel free to perform other file operations, but note that the examples below assume that 100 files have been written to the \ndistvol\n volume.\n\n\n\n\nfor i in {001..100}; do echo hello$i > /rhgs/client/nfs/distvol/mydir/file$i; done\n\n\n\n\nList the directory, counting its contents to confirm the 100 files written.\n\n\nls /rhgs/client/nfs/distvol/mydir/ | wc -l\n\n\n\n\n100\n\n\nNative Client Access\n\n\nThe Gluster native client utilizes \nFilesystem in Userspace (FUSE)\n technology to implement a client access protocol that is POSIX-compatible and has the distinct advantage that the client is fully aware of the Gluster volume architecture. This means that for a redundant volume architecture (replicate or disperse), the client automatically has \nhighly-available\n and \nload-balanced\n access to the Gluster volume data without any special configuration or external tooling.\n\n\nAdditionally, the native client can benefit from multiple data paths and specific protocol tunings, allowing for greater overall throughput and lower latency under most workloads.\n\n\nHere on the same \nclient1\n system you will mount the Gluster \ndistvol\n volume a second time, this time using the Gluster native client.\n\n\nsudo mkdir -p /rhgs/client/native/distvol\nsudo mount -t glusterfs rhgs1:distvol /rhgs/client/native/distvol/\n\n\n\n\nExamine the new mount.\n\n\ndf -h /rhgs/client/native/distvol/\n\n\n\n\nFilesystem      Size  Used Avail Use% Mounted on\n\n\nrhgs1:distvol    60G  199M   60G   1% /rhgs/client/native/distvol\n\n\nYou can see now that the Gluster volume is \nmounted twice by the two different protocols\n.\n\n\nmount | grep distvol\n\n\n\n\nrhgs1:/distvol on /rhgs/client/nfs/distvol type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.100.1.11,mountvers=3,mountport=38465,mountproto=tcp,local_lock=none,addr=10.100.1.11)\n\n\nrhgs1:distvol on /rhgs/client/native/distvol type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072)\n\n\nListing the files from your new native mount point, you can see that the files you created through the original NFS mount point are visible, confirming multi-protocol access to the same data.\n\n\nls /rhgs/client/native/distvol/mydir/ | wc -l\n\n\n\n\n100\n\n\nNow you\u2019ll create an additional set of 100 files, this time through the new native client mount point.\n\n\nfor i in {101..200}; do echo hello$i > /rhgs/client/native/distvol/mydir/file$i; done\n\n\n\n\nYou can now see that the 200 total files are available through \nboth\n the native and NFS mount points.\n\n\nls /rhgs/client/native/distvol/mydir/ | wc -l\n\n\n\n\n200\n\n\nls /rhgs/client/nfs/distvol/mydir/ | wc -l\n\n\n\n\n200\n\n\nWindows Client Access\n\n\nAdjust the Volume for SMB Access\n\n\nIn order to make your Gluster volume available to Windows clients, you need to make a few configuration changes. Re-connect to the \nrhgs1\n node from your local ssh client. (NOTE - After following the above instructions, you may simply type \nexit\n from \nclient1\n to return to \nrhgs1\n)\n\n\nssh gluster@<rhgs1PublicIP>\n\n\n\n\nFrom node \nrhgs1\n, run the below commands to modify the \nrepvol\n volume.\n\n\nsudo gluster volume set repvol stat-prefetch off\n\n\n\n\nvolume set: success\n\n\nsudo gluster volume set repvol server.allow-insecure on\n\n\n\n\nvolume set: success\n\n\nsudo sed -i '/^end-volume/i option rpc-auth-allow-insecure on' /etc/glusterfs/glusterd.vol\nsudo systemctl restart glusterd.service\n\n\n\n\nsudo gluster volume set repvol storage.batch-fsync-delay-usec 0\n\n\n\n\nvolume set: success\n\n\nsudo adduser samba-user\necho -ne \"redhat\\nredhat\\n\" | sudo smbpasswd -s -a samba-user\n\n\n\n\nAdded user samba-user.\n\n\nHere you need to temporarily mount the client interface on the \nrhgs1\n server in order to pre-create a directory for your Windows client to write to.\n\n\nsudo mount -t glusterfs rhgs1:repvol /mnt\nsudo mkdir /mnt/mysmbdir\nsudo chmod 777 /mnt/mysmbdir\nsudo umount /mnt\n\n\n\n\nAccessing the Volume from Windows\n\n\nUsing your local RDP client, connect to the public IP address of the \nwinclient\n Windows client system, available in the \nAddl. Info\n tab to the right.\n\n\n\n\nThe username is \nAdministrator\n and the password is \nRedHat1\n. You can leave the domain field blank.\n\n\n\n\nUsing Windows PowerShell, mount the Gluster volume to the Z: drive.\n\n\nnet use Z: \\\\10.100.1.11\\gluster-repvol redhat /USER:samba-user\n\n\n\n\nCreate 100 new files in the mysmbdir subdirectory.\n\n\nfor($i=1; $i -le 100; $i++){echo hello$i > Z:\\mysmbdir\\winfile$i}\n\n\n\n\nConfirm that there are 100 new files in place.\n\n\ndir Z:\\mysmbdir | measure-object -line\n\n\n\n\nLines Words          Characters          Property\n\n\n----- -----          ----------          --------\n\n\n100\n\n\n\n\nNOTE:\n \nDue to locking incompatibilities, it is \nNOT\n supported to to use the SMB/CIFS protocol (Windows client) at the same time as another client access method on the same Gluster volume. \nDoing so will result in data corruption.\n For the sake of this lab, we do this only to illustrate Gluster\u2019s multi-protocol functionality.\n\n\n\n\nConnect to \nclient1\n again via SSH from \nrhgs1\n.\n\n\nssh gluster@client1\n\n\n\n\nFrom here, you can mount the \nrepvol\n volume and see that the 100 new files you added from the Windows client are visible.\n\n\nsudo mkdir -p /rhgs/client/native/repvol\nsudo mount -t glusterfs rhgs1:repvol /rhgs/client/native/repvol\n\n\n\n\nls /rhgs/client/native/repvol/mysmbdir | wc -l\n\n\n\n\n100\n\n\nAnalysis of Volume Types\n\n\nConnect to \nrhgs1\n again with your local ssh client (simply type \nexit\n to return to \nrhgs1\n from \nclient1\n).\n\n\nssh gluster@<rhgs1PublicIP>\n\n\n\n\nLooking at the brick backend for the \ndistvol\n volume on Gluster node \nrhgs1\n, you can see that only a subset of the 200 files that you created are present on this brick. (Note the exact files you see may be different than the examples below.)\n\n\nls /rhgs/brick_xvdb/distvol/mydir/\n\n\n\n\nfile006  file033  file044  file078  file096  file120  file158  file176  file191\n\n\nfile015  file034  file050  file081  file097  file130  file164  file179  file193\n\n\nfile018  file036  file073  file088  file113  file143  file167  file185  file197\n\n\nfile026  file042  file076  file093  file115  file157  file168  file190\n\n\nNow connect to \nrhgs2\n \u2013 As a convenience, you can do this directly from node \nrhgs1\n.\n\n\nssh gluster@rhgs2\n\n\n\n\nLooking at the brick backend for the \ndistvol\n volume on Gluster node \nrhgs2\n, you can see that \na different portion of the 200 files are located on this brick\n.\n\n\nls /rhgs/brick_xvdb/distvol/mydir/\n\n\n\n\nfile003  file017  file040  file085  file109  file127  file145  file180\n\n\nfile007  file025  file046  file090  file111  file129  file163  file182\n\n\nfile013  file030  file053  file095  file119  file133  file166  file195\n\n\nfile014  file032  file061  file106  file126  file139  file169\n\n\nThis is the natural effect of the \nDistributed Hash Algorithm\n. Given the bricks in a distribute volume, the files will be pseudo-randomly placed among those bricks in a statistically even pattern.\n\n\n\n\nNOTE that at the small scale of your lab the file distribution may not seem particularly even among the bricks of the distribute volume. Under a larger environment and larger file count you will find all of the bricks to be closer in relative file count.\n\n\n\n\nWhile still on node \nrhgs2\n, take a look at the count of files in the brick backend for the \nrepvol\n volume.\n\n\nls /rhgs/brick_xvdc/repvol/mysmbdir/ | wc -l\n\n\n\n\n100\n\n\nAbove you created 100 files in this directory from the Windows client, and this time you see \nexactly 100 files on the brick backend\n. This is the effect of the \nAutomatic File Replication\n, which synchronously places copies of the files written by the clients on the replica peer bricks.\n\n\nExit from node \nrhgs2\n (simply type \nexit\n at the command line), returning to node \nrhgs1\n. Look at the \nrepvol\n brick backend on this node and confirm that the file count matches that of node \nrhgs2\n.\n\n\nls /rhgs/brick_xvdc/repvol/mysmbdir/ | wc -l\n\n\n\n\n100\n\n\nEnd of Module 2\n\n\nThis concludes \nGluster Test Drive Module 2 - Volume Setup and Client Access\n. You may continue now with Module 3, or return at any time to access the modules in any order you wish.",
            "title": "Module 2 - Volume Setup and Client Access"
        },
        {
            "location": "/gluster-module-2/#lab-guide-gluster-test-drive-module-2-volume-setup-and-client-access",
            "text": "",
            "title": "Lab Guide  Gluster Test Drive Module 2  Volume Setup and Client Access"
        },
        {
            "location": "/gluster-module-2/#lab-agenda",
            "text": "Welcome to the Gluster Test Drive Module 2 - Volume Setup and Client Access. In this lab you will:   Create a Gluster trusted pool  Manually create a Gluster distributed volume  Automatically create a Gluster replicated volume using  gdeploy  Understand basic Gluster CLI commands  Write files to your volumes with the Gluter native client, NFS, and SMB  Observe the file layout on the Gluster bricks",
            "title": "Lab Agenda"
        },
        {
            "location": "/gluster-module-2/#getting-started",
            "text": "If you have not already done so, click the   button in the navigation bar above to launch your lab. If you are prompted for a token, use the one distributed to you (or credits you\u2019ve purchased).   NOTE  It may take  up to 10 minutes  for your lab systems to start up before you can access them.",
            "title": "Getting Started"
        },
        {
            "location": "/gluster-module-2/#creating-the-trusted-pool",
            "text": "",
            "title": "Creating the Trusted Pool"
        },
        {
            "location": "/gluster-module-2/#connect-to-the-lab",
            "text": "Connect to the  rhgs1  server instance using its public IP address from the  Addl. Info  tab to the right (Linux/Mac example below).  ssh gluster@<rhgs1PublicIP>",
            "title": "Connect to the Lab"
        },
        {
            "location": "/gluster-module-2/#node-peering",
            "text": "The first step in creating a Gluster trusted pool is node peering. From one node, all of the other nodes should be peered using the gluster CLI. Your lab has 6 Gluster nodes in the local subnet. You will run  peer probe  commands from only node  rhgs1 .  sudo gluster peer probe rhgs2\nsudo gluster peer probe rhgs3\nsudo gluster peer probe rhgs4\nsudo gluster peer probe rhgs5\nsudo gluster peer probe rhgs6  Note the success Messages:  peer probe: success.",
            "title": "Node Peering"
        },
        {
            "location": "/gluster-module-2/#observe-the-trusted-pool",
            "text": "A  trusted pool  is defined as a group of Gluster nodes peered together for the purpose of sharing their local storage and compute resources for one or more logical filesystem namespaces. The state of a trusted pool and its members can be viewed with two important commands:  gluster peer status  and  gluster pool list .  sudo gluster peer status  Note that the  peer status  command only reports the remote peers of the local node from which the command is run, excluding itself (localhost) from the list. This can be confusing for first-time users. In our case, there are 6 members of the trusted pool but only 5 peers reported by the command.  Number of Peers: 5  Hostname: rhgs2  Uuid: 64edb288-524e-4918-b421-74b76b80a44b  State: Peer in Cluster (Connected)  Hostname: rhgs3  Uuid: e22261c4-3e34-49e3-b4e3-da9a8c449471  State: Peer in Cluster (Connected)  Hostname: rhgs4  Uuid: fbd4ab10-f50b-4ca8-b973-a9a54dfed47d  State: Peer in Cluster (Connected)  Hostname: rhgs5  Uuid: c351dead-7326-4370-a6fe-90d7b5ac8b45  State: Peer in Cluster (Connected)  Hostname: rhgs6  Uuid: 8b8384ae-cc6e-4403-ab3b-b7cc3c21029f  State: Peer in Cluster (Connected)  The  pool list  command provides similar output in a tabular format and includes the local node in the list, thus giving a complete view of the Gluster trusted pool.  sudo gluster pool list  UUID                  Hostname    State  64edb288-524e-4918-b421-74b76b80a44b  rhgs2       Connected  e22261c4-3e34-49e3-b4e3-da9a8c449471  rhgs3       Connected  fbd4ab10-f50b-4ca8-b973-a9a54dfed47d  rhgs4       Connected  c351dead-7326-4370-a6fe-90d7b5ac8b45  rhgs5       Connected  8b8384ae-cc6e-4403-ab3b-b7cc3c21029f  rhgs6       Connected  c0f78e93-ad8f-4849-b8f3-ebdb2f7f4d17  localhost   Connected",
            "title": "Observe the Trusted Pool"
        },
        {
            "location": "/gluster-module-2/#creating-a-distributed-volume",
            "text": "",
            "title": "Creating a Distributed Volume"
        },
        {
            "location": "/gluster-module-2/#about-bricks-and-distribution",
            "text": "A unit of storage on a node is referred to as a  brick . A brick is simply a local filesystem that has been presented to the Gluster pool for consumption. Each brick has an associated  glusterfsd  process on its system.  When a Gluster volume is created, its default architecture is  distribution . A distributed volume simply groups storage from different bricks together into one unified namespace, resulting in a Gluster volume that is as large as the sum of all of its bricks. This architecture uses a  hashing algorithm  for pseudo-random placement of files, resulting in statistically even distribution of files across the bricks.  You will use the gluster CLI to create a 6-brick distributed volume named  distvol .   Note that for the sake of this lab the backing filesystems on the nodes have been pre-configured and mounted to  /rhgs/brick_xvdb . You can view the existing LVM and filesystem configurations using the commands below.   sudo vgdisplay -v /dev/rhgs_vg1  Using volume group(s) on command line.  --- Volume group ---  VG Name               rhgs_vg1  System ID  Format                lvm2  Metadata Areas        1  Metadata Sequence No  7  VG Access             read/write  VG Status             resizable  MAX LV                0  Cur LV                2  Open LV               1  Max PV                0  Cur PV                1  Act PV                1  VG Size               10.00 GiB  PE Size               4.00 MiB  Total PE              2559  Alloc PE / Size       2559 / 10.00 GiB  Free  PE / Size       0 / 0  VG UUID               sHGNaI-ODzz-aZV3-j602-aDZQ-DUUU-ddbK7X  --- Logical volume ---  LV Name                rhgs_thinpool1  VG Name                rhgs_vg1  LV UUID                A3FlQv-X8K7-IQE0-cYUZ-eEJk-ROVe-hkboxV  LV Write Access        read/write  LV Creation host, time rhgs1.gluster.lab, 2016-07-27 10:56:26 -0400  LV Pool metadata       rhgs_thinpool1_tmeta  LV Pool data           rhgs_thinpool1_tdata  LV Status              available  # open                 2  LV Size                9.97 GiB  Allocated pool data    0.11%  Allocated metadata     0.65%  Current LE             2553  Segments               1  Allocation             inherit  Read ahead sectors     auto  - currently set to     8192  Block device           253:2  --- Logical volume ---  LV Path                /dev/rhgs_vg1/rhgs_lv1  LV Name                rhgs_lv1  VG Name                rhgs_vg1  LV UUID                L2f6yD-NhfH-2Mm7-gX5S-b8Ee-2dXL-Pb0HGK  LV Write Access        read/write  LV Creation host, time rhgs1.gluster.lab, 2016-07-27 10:56:49 -0400  LV Pool name           rhgs_thinpool1  LV Status              available  # open                 1  LV Size                10.00 GiB  Mapped size            0.11%  Current LE             2560  Segments               1  Allocation             inherit  Read ahead sectors     auto  - currently set to     8192  Block device           253:4  --- Physical volumes ---  PV Name               /dev/xvdb  PV UUID               OmPpve-V6qZ-fcvx-DFMq-nrPr-Aeoh-0X2FFg  PV Status             allocatable  Total PE / Free PE    2559 / 0  sudo df -h /rhgs/brick_xvdb  Filesystem                     Size  Used Avail Use% Mounted on  /dev/mapper/rhgs_vg1-rhgs_lv1   10G   33M   10G   1% /rhgs/brick_xvdb",
            "title": "About Bricks and Distribution"
        },
        {
            "location": "/gluster-module-2/#create-the-distributed-volume",
            "text": "Now create the 6-brick  distvol  Gluster volume.   NOTE that the  \\  characters below are for line continuation to aid copy-and-paste of the command. The  gluster volume create  is a single-line command.   sudo gluster volume create distvol \\\nrhgs1:/rhgs/brick_xvdb/distvol \\\nrhgs2:/rhgs/brick_xvdb/distvol \\\nrhgs3:/rhgs/brick_xvdb/distvol \\\nrhgs4:/rhgs/brick_xvdb/distvol \\\nrhgs5:/rhgs/brick_xvdb/distvol \\\nrhgs6:/rhgs/brick_xvdb/distvol  Note in the output that the volume was created successfuly and you now need to start the volume:  volume create: distvol: success: please start the volume to access data  Start the volume  sudo gluster volume start distvol  volume start: distvol: success",
            "title": "Create the Distributed Volume"
        },
        {
            "location": "/gluster-module-2/#automated-creation-of-a-replicate-volume",
            "text": "",
            "title": "Automated Creation of a Replicate Volume"
        },
        {
            "location": "/gluster-module-2/#about-gdeploy-and-replication",
            "text": "Gluster volume configurations can become much more complicated than the basic example above as the scale of the environment grows and additional features are leveraged. In order to simplify and automate the deployment process, Red Hat has introduced an  Ansible -based deployment tool called  gdeploy . With gdeploy, an end-to-end Gluster architecture can be defined in a configuration file, and the entire deployment can be orchestrated with a single command.  You will now build a 2-brick  replicated  volume using the gdeploy method. A replicated volume architecture groups Gluster bricks into  replica peers , synchronously storing multiple copies of the files as they are being written by the Gluster clients.   For this replicated volume deployment, the backing filesystems have  not  been pre-configured as in the above distributed volume example. The provided gdeploy configuration file includes all of the information needed to setup the  /dev/xvdc  block devices with  LVM thin provisioning  and format and mount an  XFS filesystem . It then further defines the Gluster volume architecture for your  repvol  volume.   View the gdeploy configuration file with the below command.  cat ~/repvol.conf  [hosts]  rhgs1  rhgs2   # Common backend setup for 2 of the hosts.  [backend-setup]  devices=xvdc  vgs=rhgs_vg2  pools=rhgs_thinpool2  lvs=rhgs_lv2  mountpoints=/rhgs/brick_xvdc  brick_dirs=/rhgs/brick_xvdc/repvol   [volume]  action=create  volname=repvol  replica=yes  replica_count=2  force=yes  In order to use  gdeploy , the node from which it is run requires passwordless ssh access to the root account on all nodes in the Gluster trusted pool (including itself, if the gdeploy node is also a Gluster pool node, as it is in this example).   NOTE:   Your Amazon AWS lab by default uses only keypairs for SSH authentication and configures no password-based access to the instances. Because of this, we have pre-populated keys to allow the gluster user on each node to login as the root user on all nodes using the  ~/.ssh/id_rsa  private key.   The commands below are for reference only and do not need to be run for this lab.   ssh-keygen -f ~/.ssh/id_rsa -t rsa -N ''  for i in {1..6}; do ssh-copy-id -i ~/.ssh/id_rsa root@rhgs$i; done",
            "title": "About gdeploy and Replication"
        },
        {
            "location": "/gluster-module-2/#create-the-replicated-volume",
            "text": "With passwordless ssh configured, you can deploy the  repvol  volume using the  gdeploy  command (NOTE because we rely on the ssh keys, you do not need to use  sudo  for this command).  gdeploy -vv -c ~/repvol.conf",
            "title": "Create the Replicated Volume"
        },
        {
            "location": "/gluster-module-2/#viewing-volume-details",
            "text": "Information about Gluster volumes, including their architecture, configuration specifics, processes, and ports can be viewed with the below two commands.  The  gluster volume info  command shows configuration and operational details about your Gluster volumes. You can also pass a specific volume name at the end of the command to show output for only one volume.  sudo gluster volume info  Volume Name: distvol  Type: Distribute  Volume ID: 95b1dc7f-277d-47b4-95db-e1981a8f18b9  Status: Started  Number of Bricks: 6  Transport-type: tcp  Bricks:  Brick1: rhgs1:/rhgs/brick_xvdb/distvol  Brick2: rhgs2:/rhgs/brick_xvdb/distvol  Brick3: rhgs3:/rhgs/brick_xvdb/distvol  Brick4: rhgs4:/rhgs/brick_xvdb/distvol  Brick5: rhgs5:/rhgs/brick_xvdb/distvol  Brick6: rhgs6:/rhgs/brick_xvdb/distvol  Options Reconfigured:  performance.readdir-ahead: on     Volume Name: repvol  Type: Replicate  Volume ID: c16eaa3d-7ccc-42ad-bd71-d43bb16229b2  Status: Started  Number of Bricks: 1 x 2 = 2  Transport-type: tcp  Bricks:  Brick1: rhgs1:/rhgs/brick_xvdc/repvol  Brick2: rhgs2:/rhgs/brick_xvdc/repvol  Options Reconfigured:  performance.readdir-ahead: on  The  gluster volume status  command provides other details about the operational state of volume, including process IDs and TCP ports. Here you will view the output specifically for volume  repvol .  sudo gluster volume status repvol  Status of volume: repvol  Gluster process                             TCP Port  RDMA Port  Online  Pid  ------------------------------------------------------------------------------  Brick rhgs1:/rhgs/brick_xvdc/repvol         49153     0          Y       13363  Brick rhgs2:/rhgs/brick_xvdc/repvol         49153     0          Y       12465  NFS Server on localhost                     2049      0          Y       13385  Self-heal Daemon on localhost               N/A       N/A        Y       13390  NFS Server on rhgs5                         2049      0          Y       11945  Self-heal Daemon on rhgs5                   N/A       N/A        Y       11950  NFS Server on rhgs3                         2049      0          Y       11945  Self-heal Daemon on rhgs3                   N/A       N/A        Y       11950  NFS Server on rhgs2                         2049      0          Y       12487  Self-heal Daemon on rhgs2                   N/A       N/A        Y       12492  NFS Server on rhgs4                         2049      0          Y       11974  Self-heal Daemon on rhgs4                   N/A       N/A        Y       11979  NFS Server on rhgs6                         2049      0          Y       11833  Self-heal Daemon on rhgs6                   N/A       N/A        Y       11838   Task Status of Volume repvol  ------------------------------------------------------------------------------  There are no active volume tasks",
            "title": "Viewing Volume Details"
        },
        {
            "location": "/gluster-module-2/#nfs-client-access",
            "text": "A Gluster volume can be accessed through multiple standard client protocols, as well as through specialized methods including the OpenStack Swift protocol and a direct API.  For many common use cases, the well-established NFS protocol is used for ease of implementation and compatibility with existing applications and architectures. For some use cases, the NFS protocol may also offer a performance benefit over other access methods.  You can connect to the  client1  system via  ssh  directly from the rhgs1 system.  ssh gluster@client1  Here, on the RHEL client, you will mount via NFS the Gluster  distvol  volume you created above.  sudo mkdir -p /rhgs/client/nfs/distvol\nsudo mount -t nfs rhgs1:/distvol /rhgs/client/nfs/distvol  Check the mount and observe the output.  df -h /rhgs/client/nfs/distvol  Filesystem      Size  Used Avail Use% Mounted on  rhgs1:/distvol   60G  198M   60G   1% /rhgs/client/nfs/distvol  mount | grep distvol  rhgs1:/distvol on /rhgs/client/nfs/distvol type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.100.1.11,mountvers=3,mountport=38465,mountproto=tcp,local_lock=none,addr=10.100.1.11)  Create and set permissions on a subdirectory to hold your data.  sudo mkdir /rhgs/client/nfs/distvol/mydir\nsudo chmod 777 /rhgs/client/nfs/distvol/mydir  Add 100 files to the directory.   Feel free to perform other file operations, but note that the examples below assume that 100 files have been written to the  distvol  volume.   for i in {001..100}; do echo hello$i > /rhgs/client/nfs/distvol/mydir/file$i; done  List the directory, counting its contents to confirm the 100 files written.  ls /rhgs/client/nfs/distvol/mydir/ | wc -l  100",
            "title": "NFS Client Access"
        },
        {
            "location": "/gluster-module-2/#native-client-access",
            "text": "The Gluster native client utilizes  Filesystem in Userspace (FUSE)  technology to implement a client access protocol that is POSIX-compatible and has the distinct advantage that the client is fully aware of the Gluster volume architecture. This means that for a redundant volume architecture (replicate or disperse), the client automatically has  highly-available  and  load-balanced  access to the Gluster volume data without any special configuration or external tooling.  Additionally, the native client can benefit from multiple data paths and specific protocol tunings, allowing for greater overall throughput and lower latency under most workloads.  Here on the same  client1  system you will mount the Gluster  distvol  volume a second time, this time using the Gluster native client.  sudo mkdir -p /rhgs/client/native/distvol\nsudo mount -t glusterfs rhgs1:distvol /rhgs/client/native/distvol/  Examine the new mount.  df -h /rhgs/client/native/distvol/  Filesystem      Size  Used Avail Use% Mounted on  rhgs1:distvol    60G  199M   60G   1% /rhgs/client/native/distvol  You can see now that the Gluster volume is  mounted twice by the two different protocols .  mount | grep distvol  rhgs1:/distvol on /rhgs/client/nfs/distvol type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.100.1.11,mountvers=3,mountport=38465,mountproto=tcp,local_lock=none,addr=10.100.1.11)  rhgs1:distvol on /rhgs/client/native/distvol type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072)  Listing the files from your new native mount point, you can see that the files you created through the original NFS mount point are visible, confirming multi-protocol access to the same data.  ls /rhgs/client/native/distvol/mydir/ | wc -l  100  Now you\u2019ll create an additional set of 100 files, this time through the new native client mount point.  for i in {101..200}; do echo hello$i > /rhgs/client/native/distvol/mydir/file$i; done  You can now see that the 200 total files are available through  both  the native and NFS mount points.  ls /rhgs/client/native/distvol/mydir/ | wc -l  200  ls /rhgs/client/nfs/distvol/mydir/ | wc -l  200",
            "title": "Native Client Access"
        },
        {
            "location": "/gluster-module-2/#windows-client-access",
            "text": "",
            "title": "Windows Client Access"
        },
        {
            "location": "/gluster-module-2/#adjust-the-volume-for-smb-access",
            "text": "In order to make your Gluster volume available to Windows clients, you need to make a few configuration changes. Re-connect to the  rhgs1  node from your local ssh client. (NOTE - After following the above instructions, you may simply type  exit  from  client1  to return to  rhgs1 )  ssh gluster@<rhgs1PublicIP>  From node  rhgs1 , run the below commands to modify the  repvol  volume.  sudo gluster volume set repvol stat-prefetch off  volume set: success  sudo gluster volume set repvol server.allow-insecure on  volume set: success  sudo sed -i '/^end-volume/i option rpc-auth-allow-insecure on' /etc/glusterfs/glusterd.vol\nsudo systemctl restart glusterd.service  sudo gluster volume set repvol storage.batch-fsync-delay-usec 0  volume set: success  sudo adduser samba-user\necho -ne \"redhat\\nredhat\\n\" | sudo smbpasswd -s -a samba-user  Added user samba-user.  Here you need to temporarily mount the client interface on the  rhgs1  server in order to pre-create a directory for your Windows client to write to.  sudo mount -t glusterfs rhgs1:repvol /mnt\nsudo mkdir /mnt/mysmbdir\nsudo chmod 777 /mnt/mysmbdir\nsudo umount /mnt",
            "title": "Adjust the Volume for SMB Access"
        },
        {
            "location": "/gluster-module-2/#accessing-the-volume-from-windows",
            "text": "Using your local RDP client, connect to the public IP address of the  winclient  Windows client system, available in the  Addl. Info  tab to the right.   The username is  Administrator  and the password is  RedHat1 . You can leave the domain field blank.   Using Windows PowerShell, mount the Gluster volume to the Z: drive.  net use Z: \\\\10.100.1.11\\gluster-repvol redhat /USER:samba-user  Create 100 new files in the mysmbdir subdirectory.  for($i=1; $i -le 100; $i++){echo hello$i > Z:\\mysmbdir\\winfile$i}  Confirm that there are 100 new files in place.  dir Z:\\mysmbdir | measure-object -line  Lines Words          Characters          Property  ----- -----          ----------          --------  100   NOTE:   Due to locking incompatibilities, it is  NOT  supported to to use the SMB/CIFS protocol (Windows client) at the same time as another client access method on the same Gluster volume.  Doing so will result in data corruption.  For the sake of this lab, we do this only to illustrate Gluster\u2019s multi-protocol functionality.   Connect to  client1  again via SSH from  rhgs1 .  ssh gluster@client1  From here, you can mount the  repvol  volume and see that the 100 new files you added from the Windows client are visible.  sudo mkdir -p /rhgs/client/native/repvol\nsudo mount -t glusterfs rhgs1:repvol /rhgs/client/native/repvol  ls /rhgs/client/native/repvol/mysmbdir | wc -l  100",
            "title": "Accessing the Volume from Windows"
        },
        {
            "location": "/gluster-module-2/#analysis-of-volume-types",
            "text": "Connect to  rhgs1  again with your local ssh client (simply type  exit  to return to  rhgs1  from  client1 ).  ssh gluster@<rhgs1PublicIP>  Looking at the brick backend for the  distvol  volume on Gluster node  rhgs1 , you can see that only a subset of the 200 files that you created are present on this brick. (Note the exact files you see may be different than the examples below.)  ls /rhgs/brick_xvdb/distvol/mydir/  file006  file033  file044  file078  file096  file120  file158  file176  file191  file015  file034  file050  file081  file097  file130  file164  file179  file193  file018  file036  file073  file088  file113  file143  file167  file185  file197  file026  file042  file076  file093  file115  file157  file168  file190  Now connect to  rhgs2  \u2013 As a convenience, you can do this directly from node  rhgs1 .  ssh gluster@rhgs2  Looking at the brick backend for the  distvol  volume on Gluster node  rhgs2 , you can see that  a different portion of the 200 files are located on this brick .  ls /rhgs/brick_xvdb/distvol/mydir/  file003  file017  file040  file085  file109  file127  file145  file180  file007  file025  file046  file090  file111  file129  file163  file182  file013  file030  file053  file095  file119  file133  file166  file195  file014  file032  file061  file106  file126  file139  file169  This is the natural effect of the  Distributed Hash Algorithm . Given the bricks in a distribute volume, the files will be pseudo-randomly placed among those bricks in a statistically even pattern.   NOTE that at the small scale of your lab the file distribution may not seem particularly even among the bricks of the distribute volume. Under a larger environment and larger file count you will find all of the bricks to be closer in relative file count.   While still on node  rhgs2 , take a look at the count of files in the brick backend for the  repvol  volume.  ls /rhgs/brick_xvdc/repvol/mysmbdir/ | wc -l  100  Above you created 100 files in this directory from the Windows client, and this time you see  exactly 100 files on the brick backend . This is the effect of the  Automatic File Replication , which synchronously places copies of the files written by the clients on the replica peer bricks.  Exit from node  rhgs2  (simply type  exit  at the command line), returning to node  rhgs1 . Look at the  repvol  brick backend on this node and confirm that the file count matches that of node  rhgs2 .  ls /rhgs/brick_xvdc/repvol/mysmbdir/ | wc -l  100",
            "title": "Analysis of Volume Types"
        },
        {
            "location": "/gluster-module-2/#end-of-module-2",
            "text": "This concludes  Gluster Test Drive Module 2 - Volume Setup and Client Access . You may continue now with Module 3, or return at any time to access the modules in any order you wish.",
            "title": "End of Module 2"
        },
        {
            "location": "/gluster-module-3/",
            "text": "Lab Guide \n Gluster Test Drive Module 3 \n Volume Operations and Administration\n\n\nLab Agenda\n\n\nWelcome to the Gluster Test Drive Module 3 - Volume Operations and Administration. In this lab you will:\n\n\n\n\nInitiate and observe volume self-heal behavior\n\n\nExpand a distribute-replicate volume and observe rebalance\n\n\nUnderstand basic brick monitoring and performance diagnostics\n\n\nSet and observe volume and directory quotas\n\n\n\n\nGetting Started\n\n\nIf you have not already done so, click the \n button in the navigation bar above to launch your lab. If you are prompted for a token, use the one distributed to you (or credits you\u2019ve purchased).\n\n\n\n\nNOTE\n It may take \nup to 10 minutes\n for your lab systems to start up before you can access them.\n\n\n\n\nLab Setup\n\n\nConnect to the Lab\n\n\nConnect to the \nrhgs1\n server instance using its public IP address from the \nAddl. Info\n tab to the right (Linux/Mac example below).\n\n\nssh gluster@<rhgs1PublicIP>\n\n\n\n\nIf Needed, Create the repvol Volume\n\n\nIf you have not already done so as part of \nModule 2\n, deploy the \nrepvol\n volume using the provided gdeploy configuraiton file.\n\n\ngdeploy -c ~/repvol.conf\n\n\n\n\nConfirm the volume configuration.\n\n\nsudo gluster volume info repvol\n\n\n\n\nVolume Name: repvol\n\n\nType: Replicate\n\n\nVolume ID: 6fb61bd8-4642-44e9-a5ec-4f15c8740b6f\n\n\nStatus: Started\n\n\nNumber of Bricks: 1 x 2 = 2\n\n\nTransport-type: tcp\n\n\nBricks:\n\n\nBrick1: rhgs1:/rhgs/brick_xvdc/repvol\n\n\nBrick2: rhgs2:/rhgs/brick_xvdc/repvol\n\n\nOptions Reconfigured:\n\n\nperformance.readdir-ahead: on\n\n\nVolume Self-Healing\n\n\nAbout Self-Healing\n\n\nA Gluster replicated volume maintains multiple copies of files synchronously on the volume bricks. This can provide high availability, load balancing, and increased read throughput. When a member of a replica set becomes unavailable for any reason, Gluster tracks the changes made to the online bricks in order to facilitate a set of self-healing processes when the offline bricks return to service.\n\n\nThere are two types of self-heal that operate concurrently: \nclient-side\n and \nserver-side\n (also known as \nproactive\n). Client-side heals are triggered when a client performs a file operation on a file or directory marked as needing healed. Server-side heals are managed by background processes that run on each Gluster node, periodically scouring the bricks and healing any files that they find as marked.\n\n\nOfflining a Brick\n\n\nYour \nrepvol\n volume has two bricks and a replication value of 2, and therefore a single replica set between bricks on nodes \nrhgs1\n and \nrhgs2\n. On node \nrhgs1\n you will stop all Gluster services and processes to ensure its bricks are offline.\n\n\nsudo systemctl stop glusterd.service\nsudo pkill glusterfs\nsudo pkill glusterfsd\n\n\n\n\nConfirm there are no gluster processes running. The below command should return nothing.\n\n\nsudo ps -ef |grep glusterfs | grep -v grep\n\n\n\n\nWriting Files to the Degraded Volume\n\n\nFrom \nrhgs1\n connect via SSH to \nclient1\n.\n\n\nssh gluster@client1\n\n\n\n\nIf you did not already mount the \nrepvol\n volume as part of Module 2, do it now.\n\n\n\n\nNOTE\n Below we mount the volume on \nclient1\n using node \nrhgs2\n as the server because the Gluster services on node \nrhgs1\n were offlined above.\n\n\n\n\nsudo mkdir -p /rhgs/client/native/repvol\nsudo mount -t glusterfs rhgs2:repvol /rhgs/client/native/repvol\n\n\n\n\nConfirm the volume is mounted.\n\n\ndf -h | grep repvol\n\n\n\n\nrhgs2:repvol     10G   34M   10G   1% /rhgs/client/native/repvol\n\n\nCreate a directory to hold your files and set permissions appropriately.\n\n\nsudo mkdir /rhgs/client/native/repvol/mydir\nsudo chmod 777 /rhgs/client/native/repvol/mydir\n\n\n\n\nWrite 10 new files to the directory you created in the mounted volume.\n\n\n\n\nNOTE\n There is a default timeout of 42 seconds before which the client will continue trying to contact the offline brick. If that timeout has not expired before you issue a file operation on the mounted volume, you may experience a delay at the client.\n\n\n\n\nfor i in {001..010}; do echo hello$i > /rhgs/client/native/repvol/mydir/healme$i; done\n\n\n\n\nConfirm the new files were written.\n\n\nls /rhgs/client/native/repvol/mydir/ | wc -l\n\n\n\n\n10\n\n\nViewing the Volume State\n\n\nReturn to lab node \nrhgs1\n.\n\n\nexit\n\n\n\n\nOn node \nrhgs1\n, note that the new directory you just created is not visible on the brick backend because this brick was offline at the time of the write.\n\n\nls /rhgs/brick_xvdc/repvol/mydir\n\n\n\n\nls: cannot access /rhgs/brick_xvdc/repvol/mydir: No such file or directory\n\n\nConnect to node \nrhgs2\n via SSH.\n\n\nssh gluster@rhgs2\n\n\n\n\nOn node \nrhgs2\n, note that the \nvolume status\n output only shows the processes for itself and nothing for node \nrhgs1\n.\n\n\nsudo gluster volume status repvol\n\n\n\n\nStatus of volume: repvol\n\n\nGluster process                             TCP Port  RDMA Port  Online  Pid\n\n\n------------------------------------------------------------------------------\n\n\nBrick rhgs2:/rhgs/brick_xvdc/repvol         49152     0          Y       11468\n\n\nNFS Server on localhost                     2049      0          Y       11490\n\n\nSelf-heal Daemon on localhost               N/A       N/A        Y       11495\n\n\n \n\n\nTask Status of Volume repvol\n\n\n------------------------------------------------------------------------------\n\n\nThere are no active volume tasks\n\n\nConfirm that the files you created at the client are visible on the brick backend.\n\n\nls /rhgs/brick_xvdc/repvol/mydir | wc -l\n\n\n\n\n10\n\n\nYou can view the volume\u2019s knowledge of the pending file heals with the below command.\n\n\nsudo gluster volume heal repvol info\n\n\n\n\nBrick rhgs1:/rhgs/brick_xvdc/repvol\n\n\nStatus: Transport endpoint is not connected\n\n\n\n\nBrick rhgs2:/rhgs/brick_xvdc/repvol\n\n\n/\n\n\n/mydir\n\n\n/mydir/healme001\n\n\n/mydir/healme002\n\n\n/mydir/healme003\n\n\n/mydir/healme004\n\n\n/mydir/healme005\n\n\n/mydir/healme006\n\n\n/mydir/healme007\n\n\n/mydir/healme008\n\n\n/mydir/healme009\n\n\n/mydir/healme010\n\n\nNumber of entries: 12\n\n\nReturn to lab node \nrhgs1\n.\n\n\nexit\n\n\n\n\nTriggering the Self-Heal\n\n\nRe-start the gluster services. Note that starting the \nglusterd\n management daemon will automatically start the \nglusterfsd\n brick and \nglusterfs\n supporting processes.\n\n\nsudo systemctl start glusterd.service\n\n\n\n\nConnect again to \nclient1\n via SSH.\n\n\nssh gluster@client1\n\n\n\n\nStat a file that you created above. This will trigger client-side self-heal.\n\n\nstat /rhgs/client/native/repvol/mydir/healme001\n\n\n\n\nReturn to lab node \nrhgs1\n.\n\n\nexit\n\n\n\n\nLooking again at the brick backend for the \nrepvol\n volume on node \nrhgs1\n we can now see the healed files are present.\n\n\nls /rhgs/brick_xvdc/repvol/mydir/ | wc -l\n\n\n\n\n10\n\n\nVolume Expansion and Rebalance\n\n\nAbout Rebalance\n\n\nWhen a Gluster volume is scaled horizontally by adding additional bricks, the overall architecture of the volume changes fundamentally and affects data placement by the \nDistributed Hash Algorithm\n. New file writes can easily account for the new bricks by including them in the random file placement calculation. However, any existing files will remain on their original bricks until a \nrebalance\n is initiated by the administrator.\n\n\nA rebalance triggers a re-calculation of data placement for existing files in the volume. A background operation then takes responsibility for moving the files to their new bricks, as needed. This, of course, is one of the heavier operations that Gluster performs, consuming additional system resources across the trusted pool until the rebalance is complete.\n\n\nAbout Distributed-Replicated Volumes\n\n\nIn the lab modules so far you have worked separately with \nDistributed\n and \nReplicated\n volumes. Here you will expand your \nrepvol\n volume by adding additional bricks. The replica count remains 2, meaning that every file is written synchronously to two bricks. By adding additional bricks (which you must do in sets of 2 to maintain the replica count), you are creating a distribution set out of multiple replica sets. Upon a file write, first a hashing calculation will be made to determine under which branch of the distribute set to place the file, then the write is made synchronously to the replica peers in that branch. This will be further illustrated in the commands below.\n\n\nAdd Files to the repvol Volume\n\n\nFrom \nrhgs1\n connect via SSH to \nclient1\n.\n\n\nssh gluster@client1\n\n\n\n\nAdd 200 new files to the \nrepvol\n volume.\n\n\nfor i in {001..200}; do echo hello$i > /rhgs/client/native/repvol/mydir/rebalanceme$i; done\n\n\n\n\nConfim the file count from the client. Note that we added 10 files in the self-heal section above, so the total file count should be 210.\n\n\nls /rhgs/client/native/repvol/mydir | wc -l\n\n\n\n\n210\n\n\nExit \nclient1\n, returning to node \nrhgs1\n.\n\n\nexit\n\n\n\n\nOn node \nrhgs1\n, list the backend brick contents and notice that the file count matches that of the client\u2019s view of the volume. Currently, there is only one branch to the distribute set, so all files will be located on this brick and on its replica peer, \nrhgs2\n.\n\n\nls /rhgs/brick_xvdc/repvol/mydir | wc -l\n\n\n\n\n210\n\n\nExpand the repvol Volume\n\n\nFirst, examine the existing configuration for the \nrepvol\n volume. Notice in particular the \nType\n and \nNumber of Bricks\n fields.\n\n\nsudo gluster volume info repvol\n\n\n\n\nVolume Name: repvol\n\n\nType: Replicate\n\n\nVolume ID: 2ec69e5b-0d04-4a3e-94c3-337b4302fbe8\n\n\nStatus: Started\n\n\nNumber of Bricks: 1 x 2 = 2\n\n\nTransport-type: tcp\n\n\nBricks:\n\n\nBrick1: rhgs1:/rhgs/brick_xvdc/repvol\n\n\nBrick2: rhgs2:/rhgs/brick_xvdc/repvol\n\n\nOptions Reconfigured:\n\n\nperformance.readdir-ahead: on\n\n\nYou will use the \ngdeploy\n command to create the new brick backends for nodes rhgs3 through rhgs6, and to add these new bricks to the layout of the \nrepvol\n volume. Take a look at the contents of the configuration file.\n\n\ncat ~/repvol-expand.conf\n\n\n\n\n[hosts]\n\n\nrhgs3\n\n\nrhgs4\n\n\nrhgs5\n\n\nrhgs6\n\n\n\n\n[backend-setup]\n\n\ndevices=xvdc\n\n\nvgs=rhgs_vg2\n\n\npools=rhgs_thinpool2\n\n\nlvs=rhgs_lv2\n\n\nmountpoints=/rhgs/brick_xvdc\n\n\nbrick_dirs=/rhgs/brick_xvdc/repvol\n\n\n\n\n[volume]\n\n\naction=add-brick\n\n\nvolname=rhgs1:repvol\n\n\nbricks=rhgs3:/rhgs/brick_xvdc/repvol,rhgs4:/rhgs/brick_xvdc/repvol,rhgs5:/rhgs/brick_xvdc/repvol,rhgs6:/rhgs/brick_xvdc/repvol\n\n\nUse \ngdeploy\n to make the volume change.\n\n\ngdeploy -c ~/repvol-expand.conf \n\n\n\n\nTake a look at the updated volume configuration. Note the changes to \nType\n and \nNumber of Bricks\n, as well as the additional bricks listed.\n\n\nsudo gluster volume info repvol\n\n\n\n\nVolume Name: repvol\n\n\nType: Distributed-Replicate\n\n\nVolume ID: 2ec69e5b-0d04-4a3e-94c3-337b4302fbe8\n\n\nStatus: Started\n\n\nNumber of Bricks: 3 x 2 = 6\n\n\nTransport-type: tcp\n\n\nBricks:\n\n\nBrick1: rhgs1:/rhgs/brick_xvdc/repvol\n\n\nBrick2: rhgs2:/rhgs/brick_xvdc/repvol\n\n\nBrick3: rhgs3:/rhgs/brick_xvdc/repvol\n\n\nBrick4: rhgs4:/rhgs/brick_xvdc/repvol\n\n\nBrick5: rhgs5:/rhgs/brick_xvdc/repvol\n\n\nBrick6: rhgs6:/rhgs/brick_xvdc/repvol\n\n\nOptions Reconfigured:\n\n\nperformance.readdir-ahead: on\n\n\nStart the rebalance operation on the \nrepvol\n volume.\n\n\nsudo gluster volume rebalance repvol start\n\n\n\n\nvolume rebalance: repvol: success: Rebalance on repvol has been started successfully. Use rebalance status command to check status of the rebalance process.\n\n\nID: e13d3c36-9531-4411-98aa-c974de5e2219\n\n\nTake a look at the status of the rebalance. If you run this command quickly after the \nrebalance start\n above you may catch the \nlocalhost\n in the \nin progress\n status, but due to the limited scale of the lab the rebalance may complete before you run this command and instead show the \ncompleted\n status for this node.\n\n\nsudo gluster volume rebalance repvol status\n\n\n\n\nNode Rebalanced-files          size       scanned      failures       skipped               status   run time in secs\n\n\n---------      -----------   -----------   -----------   -----------   -----------         ------------     --------------\n\n\nlocalhost               79         1.1KB           210             0             0          in progress               1.00\n\n\nrhgs2                0        0Bytes             0             0             0            completed               0.00\n\n\nrhgs3                0        0Bytes             0             0             0            completed               0.00\n\n\nrhgs4                0        0Bytes             0             0             0            completed               0.00\n\n\nrhgs5                0        0Bytes             0             0             0            completed               0.00\n\n\nrhgs6                0        0Bytes             0             0             0            completed               0.00\n\n\nvolume rebalance: repvol: success\n\n\nNow take a look again at the count of files in the brick backend for \nrepvol\n on node \nrhgs1\n. You will find that the number of files has reduced considerably.\n\n\nls /rhgs/brick_xvdc/repvol/mydir | wc -l\n\n\n\n\n75\n\n\nTo further illustrate the effect of the rebalance, connect to node \nrhgs5\n via SSH and look at the file count in the brick backend there.\n\n\nssh gluster@rhgs5\n\n\n\n\nls /rhgs/brick_xvdc/repvol/mydir | wc -l\n\n\n\n\n66\n\n\nReturn to node \nrhgs1\n.\n\n\nexit\n\n\n\n\nThe gstatus Command\n\n\nA good summary view of your volume is available through the \ngstatus\n command. Passing the \n-l\n flag to this command will also provide a visual of the volume layout in ASCII format, which can be very handy for understanding the distribute branching and replica pairing.\n\n\nsudo gstatus -v repvol -l -w\n\n\n\n\n \n     Product: RHGS Server v3.1Update3  Capacity:  60.00 GiB(raw bricks)\n      Status: HEALTHY                      201.00 MiB(raw used)\n   Glusterfs: 3.7.5                         30.00 GiB(usable from volumes)\n  OverCommit: No                Snapshots:   0\n\nVolume Information\n    repvol           UP - 6/6 bricks up - Distributed-Replicate\n                     Capacity: (0% used) 100.00 MiB/30.00 GiB (used/total)\n                     Snapshots: 0\n                     Self Heal:  6/ 6\n                     Tasks Active: None\n                     Protocols: glusterfs:on  NFS:on  SMB:on\n                     Gluster Connectivty: 7 hosts, 78 tcp connections\n\n    repvol---------- +\n                     |\n                Distribute (dht)\n                         |\n                         +-- Replica Set0 (afr)\n                         |     |\n                         |     +--rhgs1:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB \n                         |     |\n                         |     +--rhgs2:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB \n                         |\n                         +-- Replica Set1 (afr)\n                         |     |\n                         |     +--rhgs3:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB \n                         |     |\n                         |     +--rhgs4:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB \n                         |\n                         +-- Replica Set2 (afr)\n                               |\n                               +--rhgs5:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB \n                               |\n                               +--rhgs6:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB \n\n\n\n\nAnalyzing Volume Performance\n\n\nVolume Profiling\n\n\nThe \nvolume profile\n command provides an interface to get the per-brick or NFS server I/O information for each File Operation (FOP) of a volume. This information helps in identifying the bottlenecks in the storage system.\n\n\n\n\nNOTE\n The \nvolume profile\n tool consumes system resources when it is started. It is recommended that this only be enabled when needed for diagnostic purposes.\n\n\n\n\nStart profiling for the \nrepvol\n volume.\n\n\nsudo gluster volume profile repvol start\n\n\n\n\nStarting volume profile on repvol has been successful\n\n\nConnect to node \nclient1\n via SSH from node \nrhgs1\n.\n\n\nssh gluster@client1\n\n\n\n\nFor your convenience, a script called \nfopmaker.sh\n has been included to generate some interesting file operations on the volume. Run the command, passing the \nrepvol\n volume name to it.\n\n\n/home/gluster/fopmaker.sh repvol\n\n\n\n\nGenerating interesting file operations. Please wait...\n\n\nDone!\n\n\nExit node \nclient1\n, returning to node \nrhgs1\n.\n\n\nexit\n\n\n\n\nThe \nprofile info\n Gluster command will provide many statistics about the file operations performed on the individual bricks. The output can be lengthy, so only a truncated sample is included below.\n\n\nsudo gluster volume profile repvol info\n\n\n\n\n\nBrick: rhgs1:/rhgs/brick_xvdc/repvol\n------------------------------------\nCumulative Stats:\n   Block Size:                  8b+                  32b+                  64b+ \n No. of Reads:                  135                     0                     0 \nNo. of Writes:                  210                     7                     7\n\n   Block Size:                128b+                 256b+                 512b+ \n No. of Reads:                    0                     0                     0 \nNo. of Writes:                  341                    20                    14 \n\n   Block Size:               1024b+                2048b+                4096b+ \n No. of Reads:                    0                     0                     0 \nNo. of Writes:                  446                    11                     6 \n\n~~~ OUTPUT TRUNCATED ~~~\n\n\n\n\nWhen finished, stop the volume profiling.\n\n\nsudo gluster volume profile repvol stop\n\n\n\n\nStopping volume profile on repvol has been successful\n\n\nVolume Top\n\n\nThe \nvolume top\n command allows you to view the brick performance metrics, including read, write, file open calls, file read calls, file write calls, directory open calls, and directory real calls. The volume top command displays up to 100 results.\n\n\nThere are many different data sets you can view with the \nvolume top\n command. A couple of examples are included here, but feel free to try other command options. Lengthy command outputs have been truncated in the examples provided.\n\n\n\n\nTIP\n Run the \nsudo gluster volume top help\n command to see a command reference.\n\n\n\n\nView the files with the highest read calls across all bricks in the \nrepvol\n volume.\n\n\nsudo gluster volume top repvol read\n\n\n\n\n\nBrick: rhgs1:/rhgs/brick_xvdc/repvol\nCount       filename\n=======================\n5       /mydir/profile256\n2       /mydir/profile32/file32\n1       /mydir/rebalanceme199\n1       /mydir/rebalanceme198\n1       /mydir/rebalanceme196\n\n~~~ OUTPUT TRUNCATED ~~~\n\n\n\n\nRunning the above command without specifying a brick will return results for all bricks, and the output will be lenghty.\n\n\nView the files with the highest open calls on brick \nrhgs3:/rhgs/brick_xvdc/repvol\n.\n\n\nsudo gluster volume top repvol open brick rhgs3:/rhgs/brick_xvdc/repvol\n\n\n\n\n\nBrick: rhgs3:/rhgs/brick_xvdc/repvol\nCurrent open fds: 0, Max open fds: 4, Max openfd time: 2016-11-02 18:51:09.259480\nCount       filename\n=======================\n3       /mydir/profile64k\n1       /mydir/rebalanceme199\n1       /mydir/rebalanceme198\n1       /mydir/rebalanceme194\n1       /mydir/rebalanceme190\n\n~~~ OUTPUT TRUNCATED ~~~\n\n\n\n\nThe \nvolume top\n command can additionally perform some basic direct performance tests on the bricks.\n\n\n\n\nNOTE\n Using the performance analysis functionality of the \nvolume top\n command will impact the volume performance for users. It is recommended that this only be used for diagnostic purposes.\n\n\n\n\nsudo gluster volume top repvol write-perf bs 4096 count 128 brick rhgs1:/rhgs/brick_xvdc/repvol\n\n\n\n\n\nBrick: rhgs1:/rhgs/brick_xvdc/repvol\nThroughput 1307.45 MBps time 0.0004 secs\nMBps Filename                                        Time                      \n==== ========                                        ====                      \n1081 /mydir/profile2k                                2016-11-02 19:19:41.113567\n 365 /mydir/profile256                               2016-11-02 19:19:41.86411 \n 148 /mydir/profile32/file32                         2016-11-02 19:40:57.134769\n  81 /mydir/profile256/file256                       2016-11-02 19:40:57.323656\n  66 /mydir/profile32                                2016-11-02 19:19:51.352239\n\n~~~ OUTPUT TRUNCATED ~~~\n\n\n\n\nAdministration of Directory Quotas\n\n\nAbout Quotas\n\n\nDirectory quotas allow you to set limits on disk space used by directories or the volume. Storage administrators can control the disk space utilization at the directory or the volume level, or both. This is particularly useful in cloud deployments to facilitate the use of utility billing models.\n\n\nsudo gluster volume quota repvol enable\n\n\n\n\nvolume quota : success\n\n\n\n\nNOTE\n Due to the limited scale of this lab and the distributed nature of Gluster volumes, it may be easy to exceed relatively low quota values. Because of this, and \nonly for the purpose of this lab\n, you will set the quota timeout values to 0 below.\n\n\n\n\nsudo gluster volume set repvol features.quota-timeout 0\nsudo gluster volume set repvol features.hard-timeout 0\nsudo gluster volume set repvol features.soft-timeout 0\n\n\n\n\n\n\nNOTE\n Directory quotas can only be set on directories that have previously been created by a Gluster client. Below you will set a quota on the \nmydir\n subdirectory of the \nrepvol\n volume, which was created previously in the steps above. If this directory does not currently exist in your voulume, you will need to connect to \nclient1\n and create it before proceeding.\n\n\n\n\nSet a quota hard limit of 200MB for the \nmydir\n subdirectory of the \nrepvol\n volume, and then view the settings.\n\n\nsudo gluster volume quota repvol limit-usage /mydir 200MB\n\n\n\n\nvolume quota : success\n\n\nsudo gluster volume quota repvol list\n\n\n\n\n\n                  Path                   Hard-limit  Soft-limit      Used  Available  Soft-limit exceeded? Hard-limit exceeded?\n-------------------------------------------------------------------------------------------------------------------------------\n/mydir                                   200.0MB     80%(160.0MB)   82.4MB 117.6MB              No                   No\n\n\n\n\nConnect again to node \nclient1\n via SSH.\n\n\nssh gluster@client1\n\n\n\n\nAttempt to create 200 10MB files in the \nmydir\n subdirectory of the volume. At some point during this command loop, the writes should fail with a \nDisk quota exceeded\n error message.\n\n\nfor i in {001..200}; do dd if=/dev/zero of=/rhgs/client/native/repvol/mydir/quota$i bs=1024k count=10; done\n\n\n\n\nExit \nclient1\n returning to node \nrhgs1\n.\n\n\nexit\n\n\n\n\nTake another look at the \nquota list\n output for the volume, noting the quota limits exceeded.\n\n\nsudo gluster volume quota repvol list\n\n\n\n\n\n                  Path                   Hard-limit  Soft-limit      Used  Available  Soft-limit exceeded? Hard-limit exceeded?\n-------------------------------------------------------------------------------------------------------------------------------\n/mydir                                   200.0MB     80%(160.0MB)  200.0MB  0Bytes             Yes                  Yes\n\n\n\n\nEnd of Module 3\n\n\nThis concludes \nGluster Test Drive Module 3 - Volume Operations and Administration\n. You may continue now with Module 4, or return at any time to access the modules in any order you wish.",
            "title": "Module 3 - Volume Operations and Administration"
        },
        {
            "location": "/gluster-module-3/#lab-guide-gluster-test-drive-module-3-volume-operations-and-administration",
            "text": "",
            "title": "Lab Guide  Gluster Test Drive Module 3  Volume Operations and Administration"
        },
        {
            "location": "/gluster-module-3/#lab-agenda",
            "text": "Welcome to the Gluster Test Drive Module 3 - Volume Operations and Administration. In this lab you will:   Initiate and observe volume self-heal behavior  Expand a distribute-replicate volume and observe rebalance  Understand basic brick monitoring and performance diagnostics  Set and observe volume and directory quotas",
            "title": "Lab Agenda"
        },
        {
            "location": "/gluster-module-3/#getting-started",
            "text": "If you have not already done so, click the   button in the navigation bar above to launch your lab. If you are prompted for a token, use the one distributed to you (or credits you\u2019ve purchased).   NOTE  It may take  up to 10 minutes  for your lab systems to start up before you can access them.",
            "title": "Getting Started"
        },
        {
            "location": "/gluster-module-3/#lab-setup",
            "text": "",
            "title": "Lab Setup"
        },
        {
            "location": "/gluster-module-3/#connect-to-the-lab",
            "text": "Connect to the  rhgs1  server instance using its public IP address from the  Addl. Info  tab to the right (Linux/Mac example below).  ssh gluster@<rhgs1PublicIP>",
            "title": "Connect to the Lab"
        },
        {
            "location": "/gluster-module-3/#if-needed-create-the-repvol-volume",
            "text": "If you have not already done so as part of  Module 2 , deploy the  repvol  volume using the provided gdeploy configuraiton file.  gdeploy -c ~/repvol.conf  Confirm the volume configuration.  sudo gluster volume info repvol  Volume Name: repvol  Type: Replicate  Volume ID: 6fb61bd8-4642-44e9-a5ec-4f15c8740b6f  Status: Started  Number of Bricks: 1 x 2 = 2  Transport-type: tcp  Bricks:  Brick1: rhgs1:/rhgs/brick_xvdc/repvol  Brick2: rhgs2:/rhgs/brick_xvdc/repvol  Options Reconfigured:  performance.readdir-ahead: on",
            "title": "If Needed, Create the repvol Volume"
        },
        {
            "location": "/gluster-module-3/#volume-self-healing",
            "text": "",
            "title": "Volume Self-Healing"
        },
        {
            "location": "/gluster-module-3/#about-self-healing",
            "text": "A Gluster replicated volume maintains multiple copies of files synchronously on the volume bricks. This can provide high availability, load balancing, and increased read throughput. When a member of a replica set becomes unavailable for any reason, Gluster tracks the changes made to the online bricks in order to facilitate a set of self-healing processes when the offline bricks return to service.  There are two types of self-heal that operate concurrently:  client-side  and  server-side  (also known as  proactive ). Client-side heals are triggered when a client performs a file operation on a file or directory marked as needing healed. Server-side heals are managed by background processes that run on each Gluster node, periodically scouring the bricks and healing any files that they find as marked.",
            "title": "About Self-Healing"
        },
        {
            "location": "/gluster-module-3/#offlining-a-brick",
            "text": "Your  repvol  volume has two bricks and a replication value of 2, and therefore a single replica set between bricks on nodes  rhgs1  and  rhgs2 . On node  rhgs1  you will stop all Gluster services and processes to ensure its bricks are offline.  sudo systemctl stop glusterd.service\nsudo pkill glusterfs\nsudo pkill glusterfsd  Confirm there are no gluster processes running. The below command should return nothing.  sudo ps -ef |grep glusterfs | grep -v grep",
            "title": "Offlining a Brick"
        },
        {
            "location": "/gluster-module-3/#writing-files-to-the-degraded-volume",
            "text": "From  rhgs1  connect via SSH to  client1 .  ssh gluster@client1  If you did not already mount the  repvol  volume as part of Module 2, do it now.   NOTE  Below we mount the volume on  client1  using node  rhgs2  as the server because the Gluster services on node  rhgs1  were offlined above.   sudo mkdir -p /rhgs/client/native/repvol\nsudo mount -t glusterfs rhgs2:repvol /rhgs/client/native/repvol  Confirm the volume is mounted.  df -h | grep repvol  rhgs2:repvol     10G   34M   10G   1% /rhgs/client/native/repvol  Create a directory to hold your files and set permissions appropriately.  sudo mkdir /rhgs/client/native/repvol/mydir\nsudo chmod 777 /rhgs/client/native/repvol/mydir  Write 10 new files to the directory you created in the mounted volume.   NOTE  There is a default timeout of 42 seconds before which the client will continue trying to contact the offline brick. If that timeout has not expired before you issue a file operation on the mounted volume, you may experience a delay at the client.   for i in {001..010}; do echo hello$i > /rhgs/client/native/repvol/mydir/healme$i; done  Confirm the new files were written.  ls /rhgs/client/native/repvol/mydir/ | wc -l  10",
            "title": "Writing Files to the Degraded Volume"
        },
        {
            "location": "/gluster-module-3/#viewing-the-volume-state",
            "text": "Return to lab node  rhgs1 .  exit  On node  rhgs1 , note that the new directory you just created is not visible on the brick backend because this brick was offline at the time of the write.  ls /rhgs/brick_xvdc/repvol/mydir  ls: cannot access /rhgs/brick_xvdc/repvol/mydir: No such file or directory  Connect to node  rhgs2  via SSH.  ssh gluster@rhgs2  On node  rhgs2 , note that the  volume status  output only shows the processes for itself and nothing for node  rhgs1 .  sudo gluster volume status repvol  Status of volume: repvol  Gluster process                             TCP Port  RDMA Port  Online  Pid  ------------------------------------------------------------------------------  Brick rhgs2:/rhgs/brick_xvdc/repvol         49152     0          Y       11468  NFS Server on localhost                     2049      0          Y       11490  Self-heal Daemon on localhost               N/A       N/A        Y       11495     Task Status of Volume repvol  ------------------------------------------------------------------------------  There are no active volume tasks  Confirm that the files you created at the client are visible on the brick backend.  ls /rhgs/brick_xvdc/repvol/mydir | wc -l  10  You can view the volume\u2019s knowledge of the pending file heals with the below command.  sudo gluster volume heal repvol info  Brick rhgs1:/rhgs/brick_xvdc/repvol  Status: Transport endpoint is not connected   Brick rhgs2:/rhgs/brick_xvdc/repvol  /  /mydir  /mydir/healme001  /mydir/healme002  /mydir/healme003  /mydir/healme004  /mydir/healme005  /mydir/healme006  /mydir/healme007  /mydir/healme008  /mydir/healme009  /mydir/healme010  Number of entries: 12  Return to lab node  rhgs1 .  exit",
            "title": "Viewing the Volume State"
        },
        {
            "location": "/gluster-module-3/#triggering-the-self-heal",
            "text": "Re-start the gluster services. Note that starting the  glusterd  management daemon will automatically start the  glusterfsd  brick and  glusterfs  supporting processes.  sudo systemctl start glusterd.service  Connect again to  client1  via SSH.  ssh gluster@client1  Stat a file that you created above. This will trigger client-side self-heal.  stat /rhgs/client/native/repvol/mydir/healme001  Return to lab node  rhgs1 .  exit  Looking again at the brick backend for the  repvol  volume on node  rhgs1  we can now see the healed files are present.  ls /rhgs/brick_xvdc/repvol/mydir/ | wc -l  10",
            "title": "Triggering the Self-Heal"
        },
        {
            "location": "/gluster-module-3/#volume-expansion-and-rebalance",
            "text": "",
            "title": "Volume Expansion and Rebalance"
        },
        {
            "location": "/gluster-module-3/#about-rebalance",
            "text": "When a Gluster volume is scaled horizontally by adding additional bricks, the overall architecture of the volume changes fundamentally and affects data placement by the  Distributed Hash Algorithm . New file writes can easily account for the new bricks by including them in the random file placement calculation. However, any existing files will remain on their original bricks until a  rebalance  is initiated by the administrator.  A rebalance triggers a re-calculation of data placement for existing files in the volume. A background operation then takes responsibility for moving the files to their new bricks, as needed. This, of course, is one of the heavier operations that Gluster performs, consuming additional system resources across the trusted pool until the rebalance is complete.",
            "title": "About Rebalance"
        },
        {
            "location": "/gluster-module-3/#about-distributed-replicated-volumes",
            "text": "In the lab modules so far you have worked separately with  Distributed  and  Replicated  volumes. Here you will expand your  repvol  volume by adding additional bricks. The replica count remains 2, meaning that every file is written synchronously to two bricks. By adding additional bricks (which you must do in sets of 2 to maintain the replica count), you are creating a distribution set out of multiple replica sets. Upon a file write, first a hashing calculation will be made to determine under which branch of the distribute set to place the file, then the write is made synchronously to the replica peers in that branch. This will be further illustrated in the commands below.",
            "title": "About Distributed-Replicated Volumes"
        },
        {
            "location": "/gluster-module-3/#add-files-to-the-repvol-volume",
            "text": "From  rhgs1  connect via SSH to  client1 .  ssh gluster@client1  Add 200 new files to the  repvol  volume.  for i in {001..200}; do echo hello$i > /rhgs/client/native/repvol/mydir/rebalanceme$i; done  Confim the file count from the client. Note that we added 10 files in the self-heal section above, so the total file count should be 210.  ls /rhgs/client/native/repvol/mydir | wc -l  210  Exit  client1 , returning to node  rhgs1 .  exit  On node  rhgs1 , list the backend brick contents and notice that the file count matches that of the client\u2019s view of the volume. Currently, there is only one branch to the distribute set, so all files will be located on this brick and on its replica peer,  rhgs2 .  ls /rhgs/brick_xvdc/repvol/mydir | wc -l  210",
            "title": "Add Files to the repvol Volume"
        },
        {
            "location": "/gluster-module-3/#expand-the-repvol-volume",
            "text": "First, examine the existing configuration for the  repvol  volume. Notice in particular the  Type  and  Number of Bricks  fields.  sudo gluster volume info repvol  Volume Name: repvol  Type: Replicate  Volume ID: 2ec69e5b-0d04-4a3e-94c3-337b4302fbe8  Status: Started  Number of Bricks: 1 x 2 = 2  Transport-type: tcp  Bricks:  Brick1: rhgs1:/rhgs/brick_xvdc/repvol  Brick2: rhgs2:/rhgs/brick_xvdc/repvol  Options Reconfigured:  performance.readdir-ahead: on  You will use the  gdeploy  command to create the new brick backends for nodes rhgs3 through rhgs6, and to add these new bricks to the layout of the  repvol  volume. Take a look at the contents of the configuration file.  cat ~/repvol-expand.conf  [hosts]  rhgs3  rhgs4  rhgs5  rhgs6   [backend-setup]  devices=xvdc  vgs=rhgs_vg2  pools=rhgs_thinpool2  lvs=rhgs_lv2  mountpoints=/rhgs/brick_xvdc  brick_dirs=/rhgs/brick_xvdc/repvol   [volume]  action=add-brick  volname=rhgs1:repvol  bricks=rhgs3:/rhgs/brick_xvdc/repvol,rhgs4:/rhgs/brick_xvdc/repvol,rhgs5:/rhgs/brick_xvdc/repvol,rhgs6:/rhgs/brick_xvdc/repvol  Use  gdeploy  to make the volume change.  gdeploy -c ~/repvol-expand.conf   Take a look at the updated volume configuration. Note the changes to  Type  and  Number of Bricks , as well as the additional bricks listed.  sudo gluster volume info repvol  Volume Name: repvol  Type: Distributed-Replicate  Volume ID: 2ec69e5b-0d04-4a3e-94c3-337b4302fbe8  Status: Started  Number of Bricks: 3 x 2 = 6  Transport-type: tcp  Bricks:  Brick1: rhgs1:/rhgs/brick_xvdc/repvol  Brick2: rhgs2:/rhgs/brick_xvdc/repvol  Brick3: rhgs3:/rhgs/brick_xvdc/repvol  Brick4: rhgs4:/rhgs/brick_xvdc/repvol  Brick5: rhgs5:/rhgs/brick_xvdc/repvol  Brick6: rhgs6:/rhgs/brick_xvdc/repvol  Options Reconfigured:  performance.readdir-ahead: on  Start the rebalance operation on the  repvol  volume.  sudo gluster volume rebalance repvol start  volume rebalance: repvol: success: Rebalance on repvol has been started successfully. Use rebalance status command to check status of the rebalance process.  ID: e13d3c36-9531-4411-98aa-c974de5e2219  Take a look at the status of the rebalance. If you run this command quickly after the  rebalance start  above you may catch the  localhost  in the  in progress  status, but due to the limited scale of the lab the rebalance may complete before you run this command and instead show the  completed  status for this node.  sudo gluster volume rebalance repvol status  Node Rebalanced-files          size       scanned      failures       skipped               status   run time in secs  ---------      -----------   -----------   -----------   -----------   -----------         ------------     --------------  localhost               79         1.1KB           210             0             0          in progress               1.00  rhgs2                0        0Bytes             0             0             0            completed               0.00  rhgs3                0        0Bytes             0             0             0            completed               0.00  rhgs4                0        0Bytes             0             0             0            completed               0.00  rhgs5                0        0Bytes             0             0             0            completed               0.00  rhgs6                0        0Bytes             0             0             0            completed               0.00  volume rebalance: repvol: success  Now take a look again at the count of files in the brick backend for  repvol  on node  rhgs1 . You will find that the number of files has reduced considerably.  ls /rhgs/brick_xvdc/repvol/mydir | wc -l  75  To further illustrate the effect of the rebalance, connect to node  rhgs5  via SSH and look at the file count in the brick backend there.  ssh gluster@rhgs5  ls /rhgs/brick_xvdc/repvol/mydir | wc -l  66  Return to node  rhgs1 .  exit",
            "title": "Expand the repvol Volume"
        },
        {
            "location": "/gluster-module-3/#the-gstatus-command",
            "text": "A good summary view of your volume is available through the  gstatus  command. Passing the  -l  flag to this command will also provide a visual of the volume layout in ASCII format, which can be very handy for understanding the distribute branching and replica pairing.  sudo gstatus -v repvol -l -w   \n     Product: RHGS Server v3.1Update3  Capacity:  60.00 GiB(raw bricks)\n      Status: HEALTHY                      201.00 MiB(raw used)\n   Glusterfs: 3.7.5                         30.00 GiB(usable from volumes)\n  OverCommit: No                Snapshots:   0\n\nVolume Information\n    repvol           UP - 6/6 bricks up - Distributed-Replicate\n                     Capacity: (0% used) 100.00 MiB/30.00 GiB (used/total)\n                     Snapshots: 0\n                     Self Heal:  6/ 6\n                     Tasks Active: None\n                     Protocols: glusterfs:on  NFS:on  SMB:on\n                     Gluster Connectivty: 7 hosts, 78 tcp connections\n\n    repvol---------- +\n                     |\n                Distribute (dht)\n                         |\n                         +-- Replica Set0 (afr)\n                         |     |\n                         |     +--rhgs1:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB \n                         |     |\n                         |     +--rhgs2:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB \n                         |\n                         +-- Replica Set1 (afr)\n                         |     |\n                         |     +--rhgs3:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB \n                         |     |\n                         |     +--rhgs4:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB \n                         |\n                         +-- Replica Set2 (afr)\n                               |\n                               +--rhgs5:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB \n                               |\n                               +--rhgs6:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB",
            "title": "The gstatus Command"
        },
        {
            "location": "/gluster-module-3/#analyzing-volume-performance",
            "text": "",
            "title": "Analyzing Volume Performance"
        },
        {
            "location": "/gluster-module-3/#volume-profiling",
            "text": "The  volume profile  command provides an interface to get the per-brick or NFS server I/O information for each File Operation (FOP) of a volume. This information helps in identifying the bottlenecks in the storage system.   NOTE  The  volume profile  tool consumes system resources when it is started. It is recommended that this only be enabled when needed for diagnostic purposes.   Start profiling for the  repvol  volume.  sudo gluster volume profile repvol start  Starting volume profile on repvol has been successful  Connect to node  client1  via SSH from node  rhgs1 .  ssh gluster@client1  For your convenience, a script called  fopmaker.sh  has been included to generate some interesting file operations on the volume. Run the command, passing the  repvol  volume name to it.  /home/gluster/fopmaker.sh repvol  Generating interesting file operations. Please wait...  Done!  Exit node  client1 , returning to node  rhgs1 .  exit  The  profile info  Gluster command will provide many statistics about the file operations performed on the individual bricks. The output can be lengthy, so only a truncated sample is included below.  sudo gluster volume profile repvol info  \nBrick: rhgs1:/rhgs/brick_xvdc/repvol\n------------------------------------\nCumulative Stats:\n   Block Size:                  8b+                  32b+                  64b+ \n No. of Reads:                  135                     0                     0 \nNo. of Writes:                  210                     7                     7\n\n   Block Size:                128b+                 256b+                 512b+ \n No. of Reads:                    0                     0                     0 \nNo. of Writes:                  341                    20                    14 \n\n   Block Size:               1024b+                2048b+                4096b+ \n No. of Reads:                    0                     0                     0 \nNo. of Writes:                  446                    11                     6 \n\n~~~ OUTPUT TRUNCATED ~~~  When finished, stop the volume profiling.  sudo gluster volume profile repvol stop  Stopping volume profile on repvol has been successful",
            "title": "Volume Profiling"
        },
        {
            "location": "/gluster-module-3/#volume-top",
            "text": "The  volume top  command allows you to view the brick performance metrics, including read, write, file open calls, file read calls, file write calls, directory open calls, and directory real calls. The volume top command displays up to 100 results.  There are many different data sets you can view with the  volume top  command. A couple of examples are included here, but feel free to try other command options. Lengthy command outputs have been truncated in the examples provided.   TIP  Run the  sudo gluster volume top help  command to see a command reference.   View the files with the highest read calls across all bricks in the  repvol  volume.  sudo gluster volume top repvol read  \nBrick: rhgs1:/rhgs/brick_xvdc/repvol\nCount       filename\n=======================\n5       /mydir/profile256\n2       /mydir/profile32/file32\n1       /mydir/rebalanceme199\n1       /mydir/rebalanceme198\n1       /mydir/rebalanceme196\n\n~~~ OUTPUT TRUNCATED ~~~  Running the above command without specifying a brick will return results for all bricks, and the output will be lenghty.  View the files with the highest open calls on brick  rhgs3:/rhgs/brick_xvdc/repvol .  sudo gluster volume top repvol open brick rhgs3:/rhgs/brick_xvdc/repvol  \nBrick: rhgs3:/rhgs/brick_xvdc/repvol\nCurrent open fds: 0, Max open fds: 4, Max openfd time: 2016-11-02 18:51:09.259480\nCount       filename\n=======================\n3       /mydir/profile64k\n1       /mydir/rebalanceme199\n1       /mydir/rebalanceme198\n1       /mydir/rebalanceme194\n1       /mydir/rebalanceme190\n\n~~~ OUTPUT TRUNCATED ~~~  The  volume top  command can additionally perform some basic direct performance tests on the bricks.   NOTE  Using the performance analysis functionality of the  volume top  command will impact the volume performance for users. It is recommended that this only be used for diagnostic purposes.   sudo gluster volume top repvol write-perf bs 4096 count 128 brick rhgs1:/rhgs/brick_xvdc/repvol  \nBrick: rhgs1:/rhgs/brick_xvdc/repvol\nThroughput 1307.45 MBps time 0.0004 secs\nMBps Filename                                        Time                      \n==== ========                                        ====                      \n1081 /mydir/profile2k                                2016-11-02 19:19:41.113567\n 365 /mydir/profile256                               2016-11-02 19:19:41.86411 \n 148 /mydir/profile32/file32                         2016-11-02 19:40:57.134769\n  81 /mydir/profile256/file256                       2016-11-02 19:40:57.323656\n  66 /mydir/profile32                                2016-11-02 19:19:51.352239\n\n~~~ OUTPUT TRUNCATED ~~~",
            "title": "Volume Top"
        },
        {
            "location": "/gluster-module-3/#administration-of-directory-quotas",
            "text": "",
            "title": "Administration of Directory Quotas"
        },
        {
            "location": "/gluster-module-3/#about-quotas",
            "text": "Directory quotas allow you to set limits on disk space used by directories or the volume. Storage administrators can control the disk space utilization at the directory or the volume level, or both. This is particularly useful in cloud deployments to facilitate the use of utility billing models.  sudo gluster volume quota repvol enable  volume quota : success   NOTE  Due to the limited scale of this lab and the distributed nature of Gluster volumes, it may be easy to exceed relatively low quota values. Because of this, and  only for the purpose of this lab , you will set the quota timeout values to 0 below.   sudo gluster volume set repvol features.quota-timeout 0\nsudo gluster volume set repvol features.hard-timeout 0\nsudo gluster volume set repvol features.soft-timeout 0   NOTE  Directory quotas can only be set on directories that have previously been created by a Gluster client. Below you will set a quota on the  mydir  subdirectory of the  repvol  volume, which was created previously in the steps above. If this directory does not currently exist in your voulume, you will need to connect to  client1  and create it before proceeding.   Set a quota hard limit of 200MB for the  mydir  subdirectory of the  repvol  volume, and then view the settings.  sudo gluster volume quota repvol limit-usage /mydir 200MB  volume quota : success  sudo gluster volume quota repvol list  \n                  Path                   Hard-limit  Soft-limit      Used  Available  Soft-limit exceeded? Hard-limit exceeded?\n-------------------------------------------------------------------------------------------------------------------------------\n/mydir                                   200.0MB     80%(160.0MB)   82.4MB 117.6MB              No                   No  Connect again to node  client1  via SSH.  ssh gluster@client1  Attempt to create 200 10MB files in the  mydir  subdirectory of the volume. At some point during this command loop, the writes should fail with a  Disk quota exceeded  error message.  for i in {001..200}; do dd if=/dev/zero of=/rhgs/client/native/repvol/mydir/quota$i bs=1024k count=10; done  Exit  client1  returning to node  rhgs1 .  exit  Take another look at the  quota list  output for the volume, noting the quota limits exceeded.  sudo gluster volume quota repvol list  \n                  Path                   Hard-limit  Soft-limit      Used  Available  Soft-limit exceeded? Hard-limit exceeded?\n-------------------------------------------------------------------------------------------------------------------------------\n/mydir                                   200.0MB     80%(160.0MB)  200.0MB  0Bytes             Yes                  Yes",
            "title": "About Quotas"
        },
        {
            "location": "/gluster-module-3/#end-of-module-3",
            "text": "This concludes  Gluster Test Drive Module 3 - Volume Operations and Administration . You may continue now with Module 4, or return at any time to access the modules in any order you wish.",
            "title": "End of Module 3"
        },
        {
            "location": "/gluster-module-4/",
            "text": "THIS LAB IS A WORK IN PROGRESS\n\n\nLab Guide \n Gluster Test Drive Module 4 \n Disperse Volumes (Erasure Coding)\n\n\nLab Agenda\n\n\nWelcome to the Gluster Test Drive Module 4 - Disperse Volumes (Erasure Coding). In this lab you will:\n\n\n\n\nUnderstand the basic concepts of erasure coding\n\n\nCreate a gdeploy configuration for a disperse volume\n\n\nDeploy a disperse volume using gdeploy\n\n\nManually fail bricks and observe the high availability of erasure coding\n\n\n\n\nGetting Started\n\n\nIf you have not already done so, click the \n button in the navigation bar above to launch your lab. If you are prompted for a token, use the one distributed to you (or credits you\u2019ve purchased).\n\n\n\n\nNOTE\n It may take \nup to 10 minutes\n for your lab systems to start up before you can access them.\n\n\n\n\nLab Setup\n\n\nConnect to the Lab\n\n\nConnect to the \nrhgs1\n server instance using its public IP address from the \nAddl. Info\n tab to the right (Linux/Mac example below).\n\n\nssh gluster@<rhgs1PublicIP>\n\n\n\n\nAbout Disperse Volumes\n\n\nGluster disperse volumes utilize \nerasure coding\n technology to provide data protection with a lower overall infrastructure investment. Unlike standard replication, in which to protect \nn\n bricks against \nr\n failures you must invest in a total of \nn+(n*r)\n bricks, with erasure coding you can design for the same level of protection with an investment of only \nn+(n/r)\n or even less.\n\n\nFor example, assume you need \nn=4\n bricks to hold your data set and you require protection from failure of \nr=2\n bricks. If you use standard \nreplication\n, your investment in bricks will be \n4+(4*2)\n or \n12 total bricks\n. If instead you utilize \nerasure coding\n, your investment in bricks can be \n4+(4/2)\n or \n6 total bricks\n.\n\n\n\n\nErasure Coding and Parity\n\n\nInstead of writing multiple exact copies of data, erasure coding algorithms write a combination of \ndata\n and \nparity\n across bricks in the volume. A number of encoding algorithms are available offering varying levels of protection in terms of a \nredundancy to data\n ratio. For our lab, we will be using a 4:2 ratio \u2013 2 levels of redundancy for every 4 data bricks, meaning that each disperse set requires exactly 6 bricks.\n\n\n\n\nNOTE\n This functionality is very similar to RAID at the block level. You can relate our 4:2 erasure coding ratio to RAID level 6.\n\n\n\n\nWhen data is written to the disperse volume, it is broken into calculated chunks and written across all of the bricks in the disperse set. When data is read, it can be algorithmically reassembled from \nany n of the bricks where n is the number of data bricks in the set\n. Here with our 4:2 ratio, therefore, any 4 of the 6 bricks can be used to retrieve the data.\n\n\nUse Cases\n\n\nDisperse volumes offer space-efficient and capacity-optimized architectures. However, there are a couple of primary tradeoffs when considering this method of data protection.\n\n\n\n\nThe overhead of the algorithms can lead to decreased performance for your data set, particularly for reads and for small files.\n\n\nThe files are no longer stored in their whole original form on the brick backends, making offline or backend retrieval of data impossible.\n\n\n\n\n\n\nNOTE\n Interestingly, it has been shown that, under most conditions with the Gluster native client, write performance is unaffected by erasure coding and even sometimes improved.\n\n\n\n\nDisperse volumes should be used when capacity is of greater value than performance. Larger file workloads (1GB+) will experience the least performance degredation versus replicated volumes. \nYou should avoid using the NFS client with disperse volumes.",
            "title": "Module 4 - Disperse Volumes (Erasure Coding)"
        },
        {
            "location": "/gluster-module-4/#this-lab-is-a-work-in-progress",
            "text": "",
            "title": "THIS LAB IS A WORK IN PROGRESS"
        },
        {
            "location": "/gluster-module-4/#lab-guide-gluster-test-drive-module-4-disperse-volumes-erasure-coding",
            "text": "",
            "title": "Lab Guide  Gluster Test Drive Module 4  Disperse Volumes (Erasure Coding)"
        },
        {
            "location": "/gluster-module-4/#lab-agenda",
            "text": "Welcome to the Gluster Test Drive Module 4 - Disperse Volumes (Erasure Coding). In this lab you will:   Understand the basic concepts of erasure coding  Create a gdeploy configuration for a disperse volume  Deploy a disperse volume using gdeploy  Manually fail bricks and observe the high availability of erasure coding",
            "title": "Lab Agenda"
        },
        {
            "location": "/gluster-module-4/#getting-started",
            "text": "If you have not already done so, click the   button in the navigation bar above to launch your lab. If you are prompted for a token, use the one distributed to you (or credits you\u2019ve purchased).   NOTE  It may take  up to 10 minutes  for your lab systems to start up before you can access them.",
            "title": "Getting Started"
        },
        {
            "location": "/gluster-module-4/#lab-setup",
            "text": "",
            "title": "Lab Setup"
        },
        {
            "location": "/gluster-module-4/#connect-to-the-lab",
            "text": "Connect to the  rhgs1  server instance using its public IP address from the  Addl. Info  tab to the right (Linux/Mac example below).  ssh gluster@<rhgs1PublicIP>",
            "title": "Connect to the Lab"
        },
        {
            "location": "/gluster-module-4/#about-disperse-volumes",
            "text": "Gluster disperse volumes utilize  erasure coding  technology to provide data protection with a lower overall infrastructure investment. Unlike standard replication, in which to protect  n  bricks against  r  failures you must invest in a total of  n+(n*r)  bricks, with erasure coding you can design for the same level of protection with an investment of only  n+(n/r)  or even less.  For example, assume you need  n=4  bricks to hold your data set and you require protection from failure of  r=2  bricks. If you use standard  replication , your investment in bricks will be  4+(4*2)  or  12 total bricks . If instead you utilize  erasure coding , your investment in bricks can be  4+(4/2)  or  6 total bricks .",
            "title": "About Disperse Volumes"
        },
        {
            "location": "/gluster-module-4/#erasure-coding-and-parity",
            "text": "Instead of writing multiple exact copies of data, erasure coding algorithms write a combination of  data  and  parity  across bricks in the volume. A number of encoding algorithms are available offering varying levels of protection in terms of a  redundancy to data  ratio. For our lab, we will be using a 4:2 ratio \u2013 2 levels of redundancy for every 4 data bricks, meaning that each disperse set requires exactly 6 bricks.   NOTE  This functionality is very similar to RAID at the block level. You can relate our 4:2 erasure coding ratio to RAID level 6.   When data is written to the disperse volume, it is broken into calculated chunks and written across all of the bricks in the disperse set. When data is read, it can be algorithmically reassembled from  any n of the bricks where n is the number of data bricks in the set . Here with our 4:2 ratio, therefore, any 4 of the 6 bricks can be used to retrieve the data.",
            "title": "Erasure Coding and Parity"
        },
        {
            "location": "/gluster-module-4/#use-cases",
            "text": "Disperse volumes offer space-efficient and capacity-optimized architectures. However, there are a couple of primary tradeoffs when considering this method of data protection.   The overhead of the algorithms can lead to decreased performance for your data set, particularly for reads and for small files.  The files are no longer stored in their whole original form on the brick backends, making offline or backend retrieval of data impossible.    NOTE  Interestingly, it has been shown that, under most conditions with the Gluster native client, write performance is unaffected by erasure coding and even sometimes improved.   Disperse volumes should be used when capacity is of greater value than performance. Larger file workloads (1GB+) will experience the least performance degredation versus replicated volumes.  You should avoid using the NFS client with disperse volumes.",
            "title": "Use Cases"
        },
        {
            "location": "/gluster-module-5/",
            "text": "THIS LAB IS A WORK IN PROGRESS\n\n\nLab Guide \n Gluster Test Drive Module 5 \n Tiered Volumes (Cache Tiering)\n\n\nLab Agenda\n\n\nWelcome to the Gluster Test Drive Module 5 - Tiered Volumes (Cache Tiering). In this lab you will:\n\n\n\n\nnarf\n\n\n\n\nGetting Started\n\n\nIf you have not already done so, click the \n button in the navigation bar above to launch your lab. If you are prompted for a token, use the one distributed to you (or credits you\u2019ve purchased).\n\n\n\n\nNOTE\n It may take \nup to 10 minutes\n for your lab systems to start up before you can access them.",
            "title": "Module 5 - Tiered Volumes (Cache Tiering)"
        },
        {
            "location": "/gluster-module-5/#this-lab-is-a-work-in-progress",
            "text": "",
            "title": "THIS LAB IS A WORK IN PROGRESS"
        },
        {
            "location": "/gluster-module-5/#lab-guide-gluster-test-drive-module-5-tiered-volumes-cache-tiering",
            "text": "",
            "title": "Lab Guide  Gluster Test Drive Module 5  Tiered Volumes (Cache Tiering)"
        },
        {
            "location": "/gluster-module-5/#lab-agenda",
            "text": "Welcome to the Gluster Test Drive Module 5 - Tiered Volumes (Cache Tiering). In this lab you will:   narf",
            "title": "Lab Agenda"
        },
        {
            "location": "/gluster-module-5/#getting-started",
            "text": "If you have not already done so, click the   button in the navigation bar above to launch your lab. If you are prompted for a token, use the one distributed to you (or credits you\u2019ve purchased).   NOTE  It may take  up to 10 minutes  for your lab systems to start up before you can access them.",
            "title": "Getting Started"
        }
    ]
}