{
    "docs": [
        {
            "location": "/",
            "text": "Introduction\n\u00b6\n\n\nWelcome to the Red Hat Gluster Storage Hands-on Lab. To make your Gluster experience awesome, the contents of this test drive have been divided into following modules.\n\n\n\n\nModule 1 :\n Introduction to Gluster concepts\n\n\nModule 2 :\n Volume Setup and Client Access\n\n\nModule 3 :\n Volume Operations and Administration\n\n\nModule 4 :\n Disperse Volumes (Erasure Coding)\n\n\nModule 5 :\n Tiered Volumes (Cache Tiering)\n\n\nModule 6 :\n Gluster Internals\n\n\n\n\nWhat is Gluster?\n\u00b6\n\n\nGluster provides a scalable, reliable, and cost-effective data management platform, streamlining file and object access across physical, virtual, and cloud environments. You can read more about gluster here https://www.redhat.com/en/technologies/storage\n\n\nPrerequisites\n\u00b6\n\n\nIf you are a Windows user, you will need a Secure Shell client like PuTTY to connect to your instance. If you do not have it already, you can download the PuTTY client here: http://the.earth.li/~sgtatham/putty/latest/x86/putty.exe\n\n\nMac and Linux users, you will use your preferred terminal application to connect to EC2 (this should already be installed on your machine). For accessing the Windows system interface via RDP, you will need a RDP client program such as \nVinagre\n on Linux or the \nMicrosoft Remote Desktop\n client for Mac. Vinagre is likely available for your Linux distribution using your standard package manager. You can download the Mac RDP client here: https://www.microsoft.com/en-us/download/details.aspx?id=18140\n\n\nGetting to know your lab environment\n\u00b6\n\n\nStarting the lab\n\u00b6\n\n\nOn the \nLab Details\n tab, notice the lab properties:\n\n\n\n\n\n\nSetup Time -\n The estimated time for the lab to start your instance so you can access the lab environment.\n\n\nDuration -\n The estimated time the lab should take to complete.\n\n\nAccess -\n The time the lab will run before automatically shutting down.\n\n\n\n\n\n\nClick \nStart Lab\n to launch your \nqwik\nLABS. If you are prompted for a token, use the one distributed to you (or credits you\u2019ve purchased).\n\n\n\n\n\n\nA status bar shows the progress of the lab environment creation process. Your lab resources may not be fully available until the process is complete.\n\n\n\n\n\n\n[2] The \nConnect\n panel is automatically opened when the lab starts. Practice closing and re-opening it. While open it may obscure some of these lab instructions temporarily. \n\n\n\n\nTip\n If the \nConnect\n tab is unavailable, make sure you click \nStart Lab\n at the top of your screen.\n\n\nAccessing the lab\n\u00b6\n\n\nWindows users, please proceed to the next step. Mac and Linux users, skip to \nthe next section\n.\n\n\n\n\nWindows Users\n: Connecting to your Amazon EC2 Linux Instance via SSH\n\u00b6\n\n\nThis section describes how to use PuTTY Secure Shell (SSH) client to connect to your Linux lab systems. \nThe connection you are making to a lab system under these instructions are for testing purposes only to confirm functionality and is not part of the lab itself.\n\n\nNote\n This section is for Windows users only. If you are running OSX or Linux, please skip to the next section.\n\n\nDownload your key\n\u00b6\n\n\n\n\n[3] On the \nqwik\nLABS web page, click the \nAddl. Info\n tab.\n\n\nCopy the \nIP address\n of the instance to which you are connecting to your clipboard if you have not done so already.\n\n\nUnder the \nConnect\n tab, open the \nDownload PEM/PPK\n drop-down list, and click \nDownload PPK\n.\n\n\nSave the file to the directory of your choice.\n\n\n\n\nConnect to the server using SSH and PuTTY\n\u00b6\n\n\n\n\n[7] Open PuTTY.exe. \n\n\n\n\nNote\n If you have not already downloaded PuTTY, you can do so here: http://the.earth.li/~sgtatham/putty/latest/x86/putty.exe)\n\n\n\n\n[8] For \nHost Name\n, type \nec2-user@\\<public_ip>\n, where \\<public_ip> is the IP address of your lab instance from the \nAddl. Info\n tab. Your host name should look something like this: \nec2-user@54.209.158.243\n.\n\n\nIn the \nCategory\n list, click \n+\n to expand \nSSH\n.\n\n\nClick \nAuth\n (don\u2019t expand it).\n\n\nIn the \nPrivate key file for authentication\n field, browse to the PPK file that you downloaded and double-click it.\n\n\nClick \nOpen\n.\n\n\nIf you are prompted, click \nYes\n to allow a first connection to this remote SSH server.\n\n\n\n\nNote\n Because you are using a key pair for authentication, you will not be prompted by the lab system for a password.\n\n\nWhen you see a terminal screen and Linux command line prompt, it means that you are connected to your lab instance. Congratulations! If you know any Linux commands, feel free to explore. \n\n\nTroubleshooting\n\u00b6\n\n\nIf PuTTY fails to connect to your instance, verify that:\n\n\n\n\nYou typed \nec2-user@\n\\<public_ip> as your Host Name.\n\n\nYou downloaded the PPK file for this lab from \nqwik\nLABS (\nnot\n the PEM file).\n\n\nYou are using the downloaded PPK file in PuTTY.\n\n\nThe network you are on allows for outbound TCP connections to destination port 22.\n\n\n\n\n\n\nOSX and Linux Users\n: Connecting to your Amazon EC2 Linux Instance via SSH\n\u00b6\n\n\nNote\n This section is for OSX and Linux users only. If you are running Windows, please see \nthe previous section\n.\n\n\nDownload your key\n\u00b6\n\n\n\n\n[14] On the \nqwik\nLABS page, open the \nAddl. Info\n tab.\n\n\nCopy the \nIP address\n of the instance to which you are connecting to your clipboard if you have not done so already.\n\n\nUnder \nConnect\n tab, open the \nDownload PEM/PPK\n drop-down list, and click \nDownload PEM\n.\n\n\nSave the file to the directory of your choice.\n\n\n\n\nConnect to the RHEL instance using the OpenSSH CLI client\n\u00b6\n\n\n\n\n[18] Open your terminal application.\n\n\nEnter the following command, substituting \n\\<path-to-pem>\n for the path to the key you downloaded: \n\n\n\n\nchmod 600 <path-to-pem>\n\n\n\n\n\n\n[20] Enter the following command. Substitute \n\\<path-to-pem>\n for the PEM file you downloaded, and \n\\<public_ip>\n for the IP address of your lab instance from the \nAddl. Info\n tab.\n\n\n\n\nssh -i <path-to-pem> ec2-user@<public_ip>\n\n\n\n\nNote\n Do not include the \u201c\\< >\u201d brackets in your commands.\n\n\nWhen you see a terminal screen and Linux command line prompt, it means that you are connected to your Linux instance. Congratulations! If you know any Linux commands, feel free to explore.",
            "title": "Introduction"
        },
        {
            "location": "/#introduction",
            "text": "Welcome to the Red Hat Gluster Storage Hands-on Lab. To make your Gluster experience awesome, the contents of this test drive have been divided into following modules.   Module 1 :  Introduction to Gluster concepts  Module 2 :  Volume Setup and Client Access  Module 3 :  Volume Operations and Administration  Module 4 :  Disperse Volumes (Erasure Coding)  Module 5 :  Tiered Volumes (Cache Tiering)  Module 6 :  Gluster Internals",
            "title": "Introduction"
        },
        {
            "location": "/#what-is-gluster",
            "text": "Gluster provides a scalable, reliable, and cost-effective data management platform, streamlining file and object access across physical, virtual, and cloud environments. You can read more about gluster here https://www.redhat.com/en/technologies/storage",
            "title": "What is Gluster?"
        },
        {
            "location": "/#prerequisites",
            "text": "If you are a Windows user, you will need a Secure Shell client like PuTTY to connect to your instance. If you do not have it already, you can download the PuTTY client here: http://the.earth.li/~sgtatham/putty/latest/x86/putty.exe  Mac and Linux users, you will use your preferred terminal application to connect to EC2 (this should already be installed on your machine). For accessing the Windows system interface via RDP, you will need a RDP client program such as  Vinagre  on Linux or the  Microsoft Remote Desktop  client for Mac. Vinagre is likely available for your Linux distribution using your standard package manager. You can download the Mac RDP client here: https://www.microsoft.com/en-us/download/details.aspx?id=18140",
            "title": "Prerequisites"
        },
        {
            "location": "/#getting-to-know-your-lab-environment",
            "text": "",
            "title": "Getting to know your lab environment"
        },
        {
            "location": "/#starting-the-lab",
            "text": "On the  Lab Details  tab, notice the lab properties:    Setup Time -  The estimated time for the lab to start your instance so you can access the lab environment.  Duration -  The estimated time the lab should take to complete.  Access -  The time the lab will run before automatically shutting down.    Click  Start Lab  to launch your  qwik LABS. If you are prompted for a token, use the one distributed to you (or credits you\u2019ve purchased).    A status bar shows the progress of the lab environment creation process. Your lab resources may not be fully available until the process is complete.    [2] The  Connect  panel is automatically opened when the lab starts. Practice closing and re-opening it. While open it may obscure some of these lab instructions temporarily.    Tip  If the  Connect  tab is unavailable, make sure you click  Start Lab  at the top of your screen.",
            "title": "Starting the lab"
        },
        {
            "location": "/#accessing-the-lab",
            "text": "Windows users, please proceed to the next step. Mac and Linux users, skip to  the next section .",
            "title": "Accessing the lab"
        },
        {
            "location": "/#windows-users-connecting-to-your-amazon-ec2-linux-instance-via-ssh",
            "text": "This section describes how to use PuTTY Secure Shell (SSH) client to connect to your Linux lab systems.  The connection you are making to a lab system under these instructions are for testing purposes only to confirm functionality and is not part of the lab itself.  Note  This section is for Windows users only. If you are running OSX or Linux, please skip to the next section.",
            "title": "Windows Users: Connecting to your Amazon EC2 Linux Instance via SSH"
        },
        {
            "location": "/#download-your-key",
            "text": "[3] On the  qwik LABS web page, click the  Addl. Info  tab.  Copy the  IP address  of the instance to which you are connecting to your clipboard if you have not done so already.  Under the  Connect  tab, open the  Download PEM/PPK  drop-down list, and click  Download PPK .  Save the file to the directory of your choice.",
            "title": "Download your key"
        },
        {
            "location": "/#connect-to-the-server-using-ssh-and-putty",
            "text": "[7] Open PuTTY.exe.    Note  If you have not already downloaded PuTTY, you can do so here: http://the.earth.li/~sgtatham/putty/latest/x86/putty.exe)   [8] For  Host Name , type  ec2-user@\\<public_ip> , where \\<public_ip> is the IP address of your lab instance from the  Addl. Info  tab. Your host name should look something like this:  ec2-user@54.209.158.243 .  In the  Category  list, click  +  to expand  SSH .  Click  Auth  (don\u2019t expand it).  In the  Private key file for authentication  field, browse to the PPK file that you downloaded and double-click it.  Click  Open .  If you are prompted, click  Yes  to allow a first connection to this remote SSH server.   Note  Because you are using a key pair for authentication, you will not be prompted by the lab system for a password.  When you see a terminal screen and Linux command line prompt, it means that you are connected to your lab instance. Congratulations! If you know any Linux commands, feel free to explore.",
            "title": "Connect to the server using SSH and PuTTY"
        },
        {
            "location": "/#troubleshooting",
            "text": "If PuTTY fails to connect to your instance, verify that:   You typed  ec2-user@ \\<public_ip> as your Host Name.  You downloaded the PPK file for this lab from  qwik LABS ( not  the PEM file).  You are using the downloaded PPK file in PuTTY.  The network you are on allows for outbound TCP connections to destination port 22.",
            "title": "Troubleshooting"
        },
        {
            "location": "/#osx-and-linux-users-connecting-to-your-amazon-ec2-linux-instance-via-ssh",
            "text": "Note  This section is for OSX and Linux users only. If you are running Windows, please see  the previous section .",
            "title": "OSX and Linux Users: Connecting to your Amazon EC2 Linux Instance via SSH"
        },
        {
            "location": "/#download-your-key_1",
            "text": "[14] On the  qwik LABS page, open the  Addl. Info  tab.  Copy the  IP address  of the instance to which you are connecting to your clipboard if you have not done so already.  Under  Connect  tab, open the  Download PEM/PPK  drop-down list, and click  Download PEM .  Save the file to the directory of your choice.",
            "title": "Download your key"
        },
        {
            "location": "/#connect-to-the-rhel-instance-using-the-openssh-cli-client",
            "text": "[18] Open your terminal application.  Enter the following command, substituting  \\<path-to-pem>  for the path to the key you downloaded:    chmod 600 <path-to-pem>   [20] Enter the following command. Substitute  \\<path-to-pem>  for the PEM file you downloaded, and  \\<public_ip>  for the IP address of your lab instance from the  Addl. Info  tab.   ssh -i <path-to-pem> ec2-user@<public_ip>  Note  Do not include the \u201c\\< >\u201d brackets in your commands.  When you see a terminal screen and Linux command line prompt, it means that you are connected to your Linux instance. Congratulations! If you know any Linux commands, feel free to explore.",
            "title": "Connect to the RHEL instance using the OpenSSH CLI client"
        },
        {
            "location": "/gluster-module-1/",
            "text": "Gluster Test Drive Module 1 \n Introduction to Gluster Concepts\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Placement\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Accessibility\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeployment\n\u00b6",
            "title": "Module 1 - Introduction to Gluster concepts"
        },
        {
            "location": "/gluster-module-1/#gluster-test-drive-module-1-introduction-to-gluster-concepts",
            "text": "",
            "title": "Gluster Test Drive Module 1  Introduction to Gluster Concepts"
        },
        {
            "location": "/gluster-module-1/#data-placement",
            "text": "",
            "title": "Data Placement"
        },
        {
            "location": "/gluster-module-1/#data-accessibility",
            "text": "",
            "title": "Data Accessibility"
        },
        {
            "location": "/gluster-module-1/#deployment",
            "text": "",
            "title": "Deployment"
        },
        {
            "location": "/gluster-module-2/",
            "text": "Lab Guide \n Gluster Test Drive Module 2 \n Volume Setup and Client Access\n\u00b6\n\n\nLab Agenda\n\u00b6\n\n\nWelcome to the Gluster Test Drive Module 2 - Volume Setup and Client Access. In this lab we will:\n\n\n\n\nCreate a Gluster trusted pool\n\n\nManually create a Gluster distributed volume\n\n\nAutomatically create a Gluster replicated volume using gdeploy\n\n\nUnderstand basic Gluster CLI commands\n\n\nWrite files to our volumes with the Gluter native client, NFS, and SMB\n\n\nObserve the data layout on the Gluster bricks\n\n\n\n\nCreating the Trusted Pool\n\u00b6\n\n\nConnect to the \nrhgs1\n server instance using its public IP address from the \nAddl. Info\n tab (Linux/Mac example below).\n\n\nssh -i <path-to-pem> ec2-user@<public_ip>\n\n\n\n\nThe first step in creating a Gluster trusted pool is node peering. From one node, all of the other nodes should be peered using the gluster CLI. As our lab only has two nodes, you only need to run one peer probe command from node \nrhgs1\n. Run the below command to probe the peer.\n\n\nsudo gluster peer probe rhgs2\n\n\n\n\nNote the success Message:\n\n\npeer probe: success.\n\n\nA \ntrusted pool\n is defined as a group of Gluster nodes peered together for the purpose of sharing their local storage and compute resources for one or more logical filesystem namespaces. The state of a trusted pool and its members can be viewed with two important commands: \ngluster peer status\n and \ngluster pool list\n.\n\n\nsudo gluster peer status\n\n\n\n\nNote the \npeer status\n command only reports the remote peers of the local node from which the command is run, excluding itself (localhost) from the list. This can be confusing for first-time users.\n\n\nNumber of Peers: 1\n\n\nHostname: rhgs2\n\n\nUuid: 15a57a7f-b895-4b5c-8031-943bd8bcb0d1\n\n\nState: Peer in Cluster (Connected)\n\n\nThe \npool list\n command provides similar output in a tabular format and includes the local node, thus giving a complete view of the Gluster trusted pool.\n\n\nsudo gluster pool list\n\n\n\n\nUUID                  Hostname    State\n\n\n15a57a7f-b895-4b5c-8031-943bd8bcb0d1  rhgs2       Connected\n\n\nda4fe9c8-596a-4a13-9f49-f7fa057879d6  localhost   Connected\n\n\nCreating a Distributed Volume\n\u00b6\n\n\nA unit of storage on a node is referred to as a \nbrick\n. A brick is simply a local filesystem that has been presented to the Gluster system for consumption. Each brick has an associated \nglusterfsd\n process on its system.\n\n\nWhen a Gluster volume is created, its default architecture is \ndistribution\n. A distributed volume simply groups storage from different bricks together into one unified namespace, resulting in a Gluster volume as large as the sum of all of the bricks. This architecture uses a hashing algorithm for pseudo-random distribution of files across the the bricks, resulting in statistically even distribution of files across the bricks.\n\n\nWe will use the gluster CLI to create a 2-brick distributed volume named \ndistvol\n. Note that for the sake of this lab the backing filesystems on the nodes have been pre-configured and mounted to \n/rhgs/brick_vdb\n. You can view the LVM and filesystem configurations with the commands below.\n\n\nsudo vgdisplay -v /dev/rhgs_vg\n\n\n\n\nUsing volume group(s) on command line.\n\n\n--- Volume group ---\n\n\nVG Name               rhgs_vg\n\n\nSystem ID\n\n\nFormat                lvm2\n\n\nMetadata Areas        1\n\n\nMetadata Sequence No  7\n\n\nVG Access             read/write\n\n\nVG Status             resizable\n\n\nMAX LV                0\n\n\nCur LV                2\n\n\nOpen LV               1\n\n\nMax PV                0\n\n\nCur PV                1\n\n\nAct PV                1\n\n\nVG Size               10.00 GiB\n\n\nPE Size               4.00 MiB\n\n\nTotal PE              2559\n\n\nAlloc PE / Size       2559 / 10.00 GiB\n\n\nFree  PE / Size       0 / 0\n\n\nVG UUID               sHGNaI-ODzz-aZV3-j602-aDZQ-DUUU-ddbK7X\n\n\n--- Logical volume ---\n\n\nLV Name                rhgs_thinpool\n\n\nVG Name                rhgs_vg\n\n\nLV UUID                A3FlQv-X8K7-IQE0-cYUZ-eEJk-ROVe-hkboxV\n\n\nLV Write Access        read/write\n\n\nLV Creation host, time ip-172-31-2-112.ec2.internal, 2016-07-27 10:56:26 -0400\n\n\nLV Pool metadata       rhgs_thinpool_tmeta\n\n\nLV Pool data           rhgs_thinpool_tdata\n\n\nLV Status              available\n\n\n# open                 2\n\n\nLV Size                9.97 GiB\n\n\nAllocated pool data    0.11%\n\n\nAllocated metadata     0.65%\n\n\nCurrent LE             2553\n\n\nSegments               1\n\n\nAllocation             inherit\n\n\nRead ahead sectors     auto\n\n\n- currently set to     8192\n\n\nBlock device           253:2\n\n\n--- Logical volume ---\n\n\nLV Path                /dev/rhgs_vg/rhgs_lv\n\n\nLV Name                rhgs_lv\n\n\nVG Name                rhgs_vg\n\n\nLV UUID                L2f6yD-NhfH-2Mm7-gX5S-b8Ee-2dXL-Pb0HGK\n\n\nLV Write Access        read/write\n\n\nLV Creation host, time ip-172-31-2-112.ec2.internal, 2016-07-27 10:56:49 -0400\n\n\nLV Pool name           rhgs_thinpool\n\n\nLV Status              available\n\n\n# open                 1\n\n\nLV Size                10.00 GiB\n\n\nMapped size            0.11%\n\n\nCurrent LE             2560\n\n\nSegments               1\n\n\nAllocation             inherit\n\n\nRead ahead sectors     auto\n\n\n- currently set to     8192\n\n\nBlock device           253:4\n\n\n--- Physical volumes ---\n\n\nPV Name               /dev/xvdb\n\n\nPV UUID               OmPpve-V6qZ-fcvx-DFMq-nrPr-Aeoh-0X2FFg\n\n\nPV Status             allocatable\n\n\nTotal PE / Free PE    2559 / 0\n\n\nsudo df -h /rhgs/brick_vdb\n\n\n\n\nFilesystem                   Size  Used Avail Use% Mounted on\n\n\n/dev/mapper/rhgs_vg-rhgs_lv   10G   33M   10G   1% /rhgs/brick_vdb\n\n\nNow create the 2-brick \ndistvol\n Gluster volume.\n\n\nsudo gluster volume create distvol rhgs1:/rhgs/brick_vdb/distvol rhgs2:/rhgs/brick_vdb/distvol\n\n\n\n\nNote the output, volume created successfuly and we have to start the volume:\n\n\nvolume create: distvol: success: please start the volume to access data\n\n\nStart the volume\n\n\nsudo gluster volume start distvol\n\n\n\n\nvolume start: distvol: success\n\n\nAutomated Creation of a Replicate Volume\n\u00b6\n\n\nGluster volume configurations can become much more complicated than the basic example above as the scale grows and additional features are leveraged. In order to simplify and automate the deployment process, Red Hat has introduced an \nAnsible\n-based deployment tool called \ngdeploy\n. With gdeploy, an end-to-end Gluster architecture can be defined in a configuration file, and the entire deployment can be executed with a single command.\n\n\nWe will now build a \nreplicated\n volume using the gdeploy method. A replicated volume architecture groups Gluster bricks into replica peers, storing multiple copies of the files as they are being written by the Gluster clients.\n\n\nFor this replicated volume deployment, the backing filesystems have \nnot\n been pre-configured as in the above distributed volume example. The provided gdeploy configuration file includes all of the information needed to setup the \n/dev/vdc\n block devices with \nLVM\n thin provisioning and format and mount an \nXFS\n filesystem. It then further defines the Gluster volume architecture for the rep01 volume.\n\n\nView the gdeploy configuration file with the below command.\n\n\ncat ~/rep01.conf\n\n\n\n\n#\n\n\n# Usage:\n\n\n#       gdeploy -c rep01.conf\n\n\n#\n\n\n# This does backend setup first and then create the volume using the\n\n\n# setup bricks.\n\n\n#\n\n\n#\n\n\n[hosts]\n\n\nn1\n\n\nn2\n\n\n# Common backend setup for 2 of the hosts.\n\n\n[backend-setup]\n\n\ndevices=vdc\n\n\nvgs=rhgs_vg2\n\n\npools=rhgs_thinpool2\n\n\nlvs=rhgs_lv2\n\n\nmountpoints=/rhgs/brick_vdc\n\n\nbrick_dirs=/rhgs/brick_vdc/rep01\n\n\n[volume]\n\n\naction=create\n\n\nvolname=rep01\n\n\nreplica=yes\n\n\nreplica_count=2\n\n\nforce=yes\n\n\nIn order to use \ngdeploy\n, the node from which it is run requires passwordless ssh access to the root account on all nodes in the Gluster trusted pool (including itself, if the gdeploy node is also a Gluster pool node, as it is in this example).\n\n\nNOTE:\n \nAmazon AWS by default uses only keypairs for SSH authentication and configures no password-based access to the instances. Because of this, we have pre-populated keys to allow the ec2-user user on each node to login as the root user on all nodes using the \n~/.ssh/id_rsa\n private key.\n \nThe commands below are for reference only and do not need to be run for this lab.\n\n\nssh-keygen -f ~/.ssh/id_rsa -t rsa -N ''\n\n\nfor i in 1 2; do ssh-copy-id -i ~/.ssh/id_rsa root@n$i; done\n\n\nWith passwordless ssh configured, we can deploy the \nrep01\n volume using the \ngdeploy\n command (NOTE because we rely on the ssh keys, we do not need sudo for this command).\n\n\ngdeploy -vv -c ~/rep01.conf\n\n\n\n\nViewing Volume Details\n\u00b6\n\n\nInformation about Gluster volumes, including their architecture, configuration specifics, processes, and ports can be viewed with the below two commands.\n\n\nThe \ngluster volume info\n command shows configuration and operational details about your Gluster volumes. You can also pass a specific volume name at the end of the command to show output for only one volume.\n\n\nsudo gluster volume info\n\n\n\n\nVolume Name: distvol\n\n\nType: Distribute\n\n\nVolume ID: f53e4874-585a-4b2f-9949-c92ad5bc31b6\n\n\nStatus: Started\n\n\nNumber of Bricks: 2\n\n\nTransport-type: tcp\n\n\nBricks:\n\n\nBrick1: rhgs1:/rhgs/brick_vdb/distvol\n\n\nBrick2: rhgs2:/rhgs/brick_vdb/distvol\n\n\nOptions Reconfigured:\n\n\nperformance.readdir-ahead: on\n\n\nVolume Name: rep01\n\n\nType: Replicate\n\n\nVolume ID: 8cb60a83-0fec-4698-81fe-0dc2d0820d36\n\n\nStatus: Started\n\n\nNumber of Bricks: 1 x 2 = 2\n\n\nTransport-type: tcp\n\n\nBricks:\n\n\nBrick1: rhgs1:/rhgs/brick_vdc/rep01\n\n\nBrick2: rhgs2:/rhgs/brick_vdc/rep01\n\n\nOptions Reconfigured:\n\n\nperformance.readdir-ahead: on\n\n\nThe \ngluster volume status\n command provides other details about the operational state of volume, including process IDs and TCP ports. Here we view the output for volume \nrep01\n.\n\n\nsudo gluster volume status rep01\n\n\n\n\nStatus of volume: rep01\n\n\nGluster process                                TCP Port  RDMA Port  Online  Pid\n\n\n---------------------------------------------------------------------------------\n\n\nBrick rhgs1:/rhgs/brick_vdc/rep01              49153     0          Y       4613\n\n\nBrick rhgs2:/rhgs/brick_vdc/rep01              49153     0          Y       3969\n\n\nNFS Server on localhost                        2049      0          Y       4635\n\n\nSelf-heal Daemon on localhost                  N/A       N/A        Y       4640\n\n\nNFS Server on rhgs2                            2049      0          Y       3991\n\n\nSelf-heal Daemon on rhgs2                      N/A       N/A        Y       3996\n\n\nTask Status of Volume rep01\n\n\n---------------------------------------------------------------------------------\n\n\nThere are no active volume tasks\n\n\nNFS Client Access\n\u00b6\n\n\nA Gluster volume can be accessed through multiple standard client protocols, as well as through specialized methods including the OpenStack Swift protocol and a direct API.\n\n\nFor many common use cases, the well-established NFS protocol is used for ease of implementation and compatibility with existing applications and architectures. For some particular use cases, the NFS protocol may also offer a performance benefit over other access methods.\n\n\nYou can connect to the \nclient1\n system from your local ssh client using the lab PEM file (Linux/Mac example below).\n\n\nssh -i <PEM_FILE> ec2-user@<PUBLIC_IP>\n\n\n\n\nHere we mount the Gluster distvol volume we created above on a RHEL client.\n\n\nsudo mkdir -p /rhgs/client/nfs/distvol\n\n\n\n\nsudo mount -t nfs rhgs1:/distvol /rhgs/client/nfs/distvol\n\n\n\n\nCheck the mount and observe the output.\n\n\ndf -h /rhgs/client/nfs/distvol\n\n\n\n\nFilesystem      Size  Used Avail Use% Mounted on\n\n\nrhgs1:/distvol      20G   66M   20G   1% /rhgs/client/nfs/distvol\n\n\nmount | grep distvol\n\n\n\n\nrhgs1:/distvol on /rhgs/client/nfs/distvol type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.100.1.11,mountvers=3,mountport=38465,mountproto=tcp,local_lock=none,addr=10.100.1.11)\n\n\nCreate and set permissions on a directory to hold our data.\n\n\nsudo mkdir /rhgs/client/nfs/distvol/mydir\nsudo chmod 777 /rhgs/client/nfs/distvol/mydir\n\n\n\n\nAdd 100 files to the directory.\n\n\nfor i in {001..100}; do echo hello$i > /rhgs/client/nfs/distvol/mydir/file$i; done\n\n\n\n\nList the directory, counting its contents.\n\n\nls /rhgs/client/nfs/distvol/mydir/ | wc -l\n\n\n\n\n100\n\n\nNative Client Access\n\u00b6\n\n\nThe Gluster native client utilizes \nFilesystem in Userspace (FUSE)\n technology to implement a client access protocol that is POSIX-compatible and has the distinct advantage that the client is fully aware of the Gluster volume architecture. This means that with a redundant volume architecture (replicate or disperse), the client automatically has \nhighly-available\n access to the Gluster volume data without any special configuration.\n\n\nAdditionally, the native client can benefit from multiple data paths and specific protocol tunings, allowing for greater overall throughput and lower latency under most workloads.\n\n\nHere on the same client we mount the Gluster \ndistvol\n volume a second time, this time using the Gluster native client.\n\n\nsudo mkdir -p /rhgs/client/native/distvol\nsudo mount -t glusterfs rhgs1:distvol /rhgs/client/native/distvol/\n\n\n\n\nExamine the new mount.\n\n\ndf -h /rhgs/client/native/distvol/\n\n\n\n\nFilesystem      Size  Used Avail Use% Mounted on\n\n\nrhgs1:distvol       20G   67M   20G   1% /rhgs/client/native/distvol\n\n\nWe can see here that the Gluster volume is \nmounted twice by the two different protocols\n.\n\n\nmount | grep distvol\n\n\n\n\nrhgs1:/distvol on /rhgs/client/nfs/distvol type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.100.1.11,mountvers=3,mountport=38465,mountproto=tcp,local_lock=none,addr=10.100.1.11)\n\n\nrhgs1:distvol on /rhgs/client/native/distvol type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072)\n\n\nListing the files from our new native mount point, we can see that the files we created through the original NFS mount point are visible.\n\n\nls /rhgs/client/native/distvol/mydir/ | wc -l\n\n\n\n\n100\n\n\nNow we\u2019ll create an additional set of 100 files, this time through the new native client mount point.\n\n\nfor i in {101..200}; do echo hello$i > /rhgs/client/native/distvol/mydir/file$i; done\n\n\n\n\nWe can now see that the 200 total files are available through \nboth\n the native and NFS mount points.\n\n\nls /rhgs/client/native/distvol/mydir/ | wc -l\n\n\n\n\n200\n\n\nls /rhgs/client/nfs/distvol/mydir/ | wc -l\n\n\n\n\n200\n\n\nWindows Client Access\n\u00b6\n\n\nIn order to make our Gluster volume available to Windows clients, we need to make a few configuration changes. Re-connect to the \nrhgs1\n node from your local ssh client.\n\n\nssh -i <PEM_FILE> ec2-user@<PUBLIC_IP>\n\n\n\n\nFrom node \nrhgs1\n, run the below commands to modify the \nrep01\n volume.\n\n\nsudo gluster volume set rep01 stat-prefetch off\n\n\n\n\nvolume set: success\n\n\nsudo gluster volume set rep01 server.allow-insecure on\n\n\n\n\nvolume set: success\n\n\nsudo sed -i '/^end-volume/i option rpc-auth-allow-insecure on' /etc/glusterfs/glusterd.vol\nsudo systemctl restart glusterd.service\n\n\n\n\nsudo gluster volume set rep01 storage.batch-fsync-delay-usec 0\n\n\n\n\nvolume set: success\n\n\nsudo adduser samba-user\necho -ne \"redhat\\nredhat\\n\" | sudo smbpasswd -s -a samba-user\n\n\n\n\nAdded user samba-user.\n\n\nHere we temporarily mount the client interface on the rhgs1 server in order to create a directory for our Windows client to write to.\n\n\nsudo mount -t glusterfs rhgs1:rep01 /mnt\nsudo mkdir /mnt/mysmbdir\nsudo chmod 777 /mnt/mysmbdir\nsudo umount /mnt\n\n\n\n\nUsing your local RDP client, connect to the public IP address of the Windows client system. The username is \nAdministrator\n and the password is \nRedHat1\n. You can leave the domain field blank.\n\n\nUsing Windows PowerShell, we mount the Gluster volume to the Z: drive.\n\n\nnet use Z: \\\\10.100.1.11\\gluster-rep01 redhat /USER:samba-user\n\n\n\n\nWe create 100 new files in the mysmbdir subdirectory.\n\n\nfor($i=1; $i -le 100; $i++){echo hello$i > Z:\\mysmbdir\\winfile$i}\n\n\n\n\nAnd we can confirm that there are 100 new files in place.\n\n\ndir Z:\\mysmbdir | measure-object -line\n\n\n\n\nLines Words          Characters          Property\n\n\n----- -----          ----------          --------\n\n\n100\n\n\nNOTE:\n \nDue to locking incompatibilities, it is \nNOT\n supported to to use the SMB/CIFS protocol (Windows client) at the same time as another client access method on the same Gluster volume. Doing so will result in data corruption. For the sake of this lab, we do this only to illustrate Gluster\u2019s functionality.\n\n\nConnect to \nclient1\n again with your local ssh client.\n\n\nssh -i <PEM_FILE> ec2-user@<PUBLIC_IP>\n\n\n\n\nFrom here, you can mount the \nrep01\n volume and see the 100 new files added from the Windows client are visible.\n\n\nsudo mkdir -p /rhgs/client/native/rep01\nsudo mount -t glusterfs rhgs1:rep01 /rhgs/client/native/rep01\n\n\n\n\nls /rhgs/client/native/rep01/mysmbdir | wc -l\n\n\n\n\n100\n\n\nAnalysis of Volume Types\n\u00b6\n\n\nConnect to \nrhgs1\n again with your local ssh client.\n\n\nssh -i <PEM_FILE> ec2-user@<PUBLIC_IP>\n\n\n\n\nLooking at the brick backend for the \ndistvol\n volume on Gluster node \nrhgs1\n, we can see that only a subset of the 200 files that were created are present on this brick. (Note the numbers you see may be different than the examples below.)\n\n\nls /rhgs/brick_vdb/distvol/mydir/ | wc -l\n\n\n\n\n95\n\n\nNow connect to \nrhgs2\n \u2013 As a convenience, you can do this most simply directly from node rhgs1 (you will need to connect at the root user).\n\n\nssh root@rhgs2\n\n\n\n\nLooking at the brick backend for the \ndistvol\n volume on Gluster node \nrhgs2\n, we can see that the \nremainder of the 200 files are located on this brick\n.\n\n\nls /rhgs/brick_vdb/distvol/mydir/ | wc -l\n\n\n\n\n105\n\n\nThis is the natural effect of the \nDistributed Hash Algorithm\n. Given the bricks in a distribute volume, the files will be pseudo-randomly placed among those bricks in a statistically even pattern.\n\n\nWhile still on node \nrhgs2\n, take a look at the files in the brick backend for the \nrep01\n volume.\n\n\nls /rhgs/brick_vdc/rep01/mysmbdir/ | wc -l\n\n\n\n\n100\n\n\nAbove we created 100 files in this directory from the Windows client, and this time we see \nexactly 100 files on the brick backend\n. This is the effect of the \nAutomatic File Replication\n, which synchronously places copies of the files written by the clients on the replica peer bricks.\n\n\nExit from node \nrhgs2\n (simply type exit at the command line), returning to node \nrhgs1\n. Look at the \nrep01\n brick backend on this node and confirm that the file count matches that of node rhgs2.\n\n\nls /rhgs/brick_vdc/rep01/mysmbdir/ | wc -l\n\n\n\n\n100\n\n\nEnd Your Lab\n\u00b6\n\n\nThis concludes Gluster Test Drive Module 2 - Volume Setup and Client Access. Please follow these steps to end your lab and evaluate the experience.\n\n\n\n\n[1] Close your remote sessions.\n\n\nIn the \nqwik\nLABS page, click \nEnd Lab\n.\n\n\nIn the confirmation message, click \nOK\n.\n\n\n(Optional) Tell us about your lab experience!\n\n\n\n\nAdditional Resources\n\u00b6\n\n\n\n\nhttps://www.redhat.com/en/technologies/storage/gluster",
            "title": "Module 2 - Volume Setup and Client Access"
        },
        {
            "location": "/gluster-module-2/#lab-guide-gluster-test-drive-module-2-volume-setup-and-client-access",
            "text": "",
            "title": "Lab Guide  Gluster Test Drive Module 2  Volume Setup and Client Access"
        },
        {
            "location": "/gluster-module-2/#lab-agenda",
            "text": "Welcome to the Gluster Test Drive Module 2 - Volume Setup and Client Access. In this lab we will:   Create a Gluster trusted pool  Manually create a Gluster distributed volume  Automatically create a Gluster replicated volume using gdeploy  Understand basic Gluster CLI commands  Write files to our volumes with the Gluter native client, NFS, and SMB  Observe the data layout on the Gluster bricks",
            "title": "Lab Agenda"
        },
        {
            "location": "/gluster-module-2/#creating-the-trusted-pool",
            "text": "Connect to the  rhgs1  server instance using its public IP address from the  Addl. Info  tab (Linux/Mac example below).  ssh -i <path-to-pem> ec2-user@<public_ip>  The first step in creating a Gluster trusted pool is node peering. From one node, all of the other nodes should be peered using the gluster CLI. As our lab only has two nodes, you only need to run one peer probe command from node  rhgs1 . Run the below command to probe the peer.  sudo gluster peer probe rhgs2  Note the success Message:  peer probe: success.  A  trusted pool  is defined as a group of Gluster nodes peered together for the purpose of sharing their local storage and compute resources for one or more logical filesystem namespaces. The state of a trusted pool and its members can be viewed with two important commands:  gluster peer status  and  gluster pool list .  sudo gluster peer status  Note the  peer status  command only reports the remote peers of the local node from which the command is run, excluding itself (localhost) from the list. This can be confusing for first-time users.  Number of Peers: 1  Hostname: rhgs2  Uuid: 15a57a7f-b895-4b5c-8031-943bd8bcb0d1  State: Peer in Cluster (Connected)  The  pool list  command provides similar output in a tabular format and includes the local node, thus giving a complete view of the Gluster trusted pool.  sudo gluster pool list  UUID                  Hostname    State  15a57a7f-b895-4b5c-8031-943bd8bcb0d1  rhgs2       Connected  da4fe9c8-596a-4a13-9f49-f7fa057879d6  localhost   Connected",
            "title": "Creating the Trusted Pool"
        },
        {
            "location": "/gluster-module-2/#creating-a-distributed-volume",
            "text": "A unit of storage on a node is referred to as a  brick . A brick is simply a local filesystem that has been presented to the Gluster system for consumption. Each brick has an associated  glusterfsd  process on its system.  When a Gluster volume is created, its default architecture is  distribution . A distributed volume simply groups storage from different bricks together into one unified namespace, resulting in a Gluster volume as large as the sum of all of the bricks. This architecture uses a hashing algorithm for pseudo-random distribution of files across the the bricks, resulting in statistically even distribution of files across the bricks.  We will use the gluster CLI to create a 2-brick distributed volume named  distvol . Note that for the sake of this lab the backing filesystems on the nodes have been pre-configured and mounted to  /rhgs/brick_vdb . You can view the LVM and filesystem configurations with the commands below.  sudo vgdisplay -v /dev/rhgs_vg  Using volume group(s) on command line.  --- Volume group ---  VG Name               rhgs_vg  System ID  Format                lvm2  Metadata Areas        1  Metadata Sequence No  7  VG Access             read/write  VG Status             resizable  MAX LV                0  Cur LV                2  Open LV               1  Max PV                0  Cur PV                1  Act PV                1  VG Size               10.00 GiB  PE Size               4.00 MiB  Total PE              2559  Alloc PE / Size       2559 / 10.00 GiB  Free  PE / Size       0 / 0  VG UUID               sHGNaI-ODzz-aZV3-j602-aDZQ-DUUU-ddbK7X  --- Logical volume ---  LV Name                rhgs_thinpool  VG Name                rhgs_vg  LV UUID                A3FlQv-X8K7-IQE0-cYUZ-eEJk-ROVe-hkboxV  LV Write Access        read/write  LV Creation host, time ip-172-31-2-112.ec2.internal, 2016-07-27 10:56:26 -0400  LV Pool metadata       rhgs_thinpool_tmeta  LV Pool data           rhgs_thinpool_tdata  LV Status              available  # open                 2  LV Size                9.97 GiB  Allocated pool data    0.11%  Allocated metadata     0.65%  Current LE             2553  Segments               1  Allocation             inherit  Read ahead sectors     auto  - currently set to     8192  Block device           253:2  --- Logical volume ---  LV Path                /dev/rhgs_vg/rhgs_lv  LV Name                rhgs_lv  VG Name                rhgs_vg  LV UUID                L2f6yD-NhfH-2Mm7-gX5S-b8Ee-2dXL-Pb0HGK  LV Write Access        read/write  LV Creation host, time ip-172-31-2-112.ec2.internal, 2016-07-27 10:56:49 -0400  LV Pool name           rhgs_thinpool  LV Status              available  # open                 1  LV Size                10.00 GiB  Mapped size            0.11%  Current LE             2560  Segments               1  Allocation             inherit  Read ahead sectors     auto  - currently set to     8192  Block device           253:4  --- Physical volumes ---  PV Name               /dev/xvdb  PV UUID               OmPpve-V6qZ-fcvx-DFMq-nrPr-Aeoh-0X2FFg  PV Status             allocatable  Total PE / Free PE    2559 / 0  sudo df -h /rhgs/brick_vdb  Filesystem                   Size  Used Avail Use% Mounted on  /dev/mapper/rhgs_vg-rhgs_lv   10G   33M   10G   1% /rhgs/brick_vdb  Now create the 2-brick  distvol  Gluster volume.  sudo gluster volume create distvol rhgs1:/rhgs/brick_vdb/distvol rhgs2:/rhgs/brick_vdb/distvol  Note the output, volume created successfuly and we have to start the volume:  volume create: distvol: success: please start the volume to access data  Start the volume  sudo gluster volume start distvol  volume start: distvol: success",
            "title": "Creating a Distributed Volume"
        },
        {
            "location": "/gluster-module-2/#automated-creation-of-a-replicate-volume",
            "text": "Gluster volume configurations can become much more complicated than the basic example above as the scale grows and additional features are leveraged. In order to simplify and automate the deployment process, Red Hat has introduced an  Ansible -based deployment tool called  gdeploy . With gdeploy, an end-to-end Gluster architecture can be defined in a configuration file, and the entire deployment can be executed with a single command.  We will now build a  replicated  volume using the gdeploy method. A replicated volume architecture groups Gluster bricks into replica peers, storing multiple copies of the files as they are being written by the Gluster clients.  For this replicated volume deployment, the backing filesystems have  not  been pre-configured as in the above distributed volume example. The provided gdeploy configuration file includes all of the information needed to setup the  /dev/vdc  block devices with  LVM  thin provisioning and format and mount an  XFS  filesystem. It then further defines the Gluster volume architecture for the rep01 volume.  View the gdeploy configuration file with the below command.  cat ~/rep01.conf  #  # Usage:  #       gdeploy -c rep01.conf  #  # This does backend setup first and then create the volume using the  # setup bricks.  #  #  [hosts]  n1  n2  # Common backend setup for 2 of the hosts.  [backend-setup]  devices=vdc  vgs=rhgs_vg2  pools=rhgs_thinpool2  lvs=rhgs_lv2  mountpoints=/rhgs/brick_vdc  brick_dirs=/rhgs/brick_vdc/rep01  [volume]  action=create  volname=rep01  replica=yes  replica_count=2  force=yes  In order to use  gdeploy , the node from which it is run requires passwordless ssh access to the root account on all nodes in the Gluster trusted pool (including itself, if the gdeploy node is also a Gluster pool node, as it is in this example).  NOTE:   Amazon AWS by default uses only keypairs for SSH authentication and configures no password-based access to the instances. Because of this, we have pre-populated keys to allow the ec2-user user on each node to login as the root user on all nodes using the  ~/.ssh/id_rsa  private key.   The commands below are for reference only and do not need to be run for this lab.  ssh-keygen -f ~/.ssh/id_rsa -t rsa -N ''  for i in 1 2; do ssh-copy-id -i ~/.ssh/id_rsa root@n$i; done  With passwordless ssh configured, we can deploy the  rep01  volume using the  gdeploy  command (NOTE because we rely on the ssh keys, we do not need sudo for this command).  gdeploy -vv -c ~/rep01.conf",
            "title": "Automated Creation of a Replicate Volume"
        },
        {
            "location": "/gluster-module-2/#viewing-volume-details",
            "text": "Information about Gluster volumes, including their architecture, configuration specifics, processes, and ports can be viewed with the below two commands.  The  gluster volume info  command shows configuration and operational details about your Gluster volumes. You can also pass a specific volume name at the end of the command to show output for only one volume.  sudo gluster volume info  Volume Name: distvol  Type: Distribute  Volume ID: f53e4874-585a-4b2f-9949-c92ad5bc31b6  Status: Started  Number of Bricks: 2  Transport-type: tcp  Bricks:  Brick1: rhgs1:/rhgs/brick_vdb/distvol  Brick2: rhgs2:/rhgs/brick_vdb/distvol  Options Reconfigured:  performance.readdir-ahead: on  Volume Name: rep01  Type: Replicate  Volume ID: 8cb60a83-0fec-4698-81fe-0dc2d0820d36  Status: Started  Number of Bricks: 1 x 2 = 2  Transport-type: tcp  Bricks:  Brick1: rhgs1:/rhgs/brick_vdc/rep01  Brick2: rhgs2:/rhgs/brick_vdc/rep01  Options Reconfigured:  performance.readdir-ahead: on  The  gluster volume status  command provides other details about the operational state of volume, including process IDs and TCP ports. Here we view the output for volume  rep01 .  sudo gluster volume status rep01  Status of volume: rep01  Gluster process                                TCP Port  RDMA Port  Online  Pid  ---------------------------------------------------------------------------------  Brick rhgs1:/rhgs/brick_vdc/rep01              49153     0          Y       4613  Brick rhgs2:/rhgs/brick_vdc/rep01              49153     0          Y       3969  NFS Server on localhost                        2049      0          Y       4635  Self-heal Daemon on localhost                  N/A       N/A        Y       4640  NFS Server on rhgs2                            2049      0          Y       3991  Self-heal Daemon on rhgs2                      N/A       N/A        Y       3996  Task Status of Volume rep01  ---------------------------------------------------------------------------------  There are no active volume tasks",
            "title": "Viewing Volume Details"
        },
        {
            "location": "/gluster-module-2/#nfs-client-access",
            "text": "A Gluster volume can be accessed through multiple standard client protocols, as well as through specialized methods including the OpenStack Swift protocol and a direct API.  For many common use cases, the well-established NFS protocol is used for ease of implementation and compatibility with existing applications and architectures. For some particular use cases, the NFS protocol may also offer a performance benefit over other access methods.  You can connect to the  client1  system from your local ssh client using the lab PEM file (Linux/Mac example below).  ssh -i <PEM_FILE> ec2-user@<PUBLIC_IP>  Here we mount the Gluster distvol volume we created above on a RHEL client.  sudo mkdir -p /rhgs/client/nfs/distvol  sudo mount -t nfs rhgs1:/distvol /rhgs/client/nfs/distvol  Check the mount and observe the output.  df -h /rhgs/client/nfs/distvol  Filesystem      Size  Used Avail Use% Mounted on  rhgs1:/distvol      20G   66M   20G   1% /rhgs/client/nfs/distvol  mount | grep distvol  rhgs1:/distvol on /rhgs/client/nfs/distvol type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.100.1.11,mountvers=3,mountport=38465,mountproto=tcp,local_lock=none,addr=10.100.1.11)  Create and set permissions on a directory to hold our data.  sudo mkdir /rhgs/client/nfs/distvol/mydir\nsudo chmod 777 /rhgs/client/nfs/distvol/mydir  Add 100 files to the directory.  for i in {001..100}; do echo hello$i > /rhgs/client/nfs/distvol/mydir/file$i; done  List the directory, counting its contents.  ls /rhgs/client/nfs/distvol/mydir/ | wc -l  100",
            "title": "NFS Client Access"
        },
        {
            "location": "/gluster-module-2/#native-client-access",
            "text": "The Gluster native client utilizes  Filesystem in Userspace (FUSE)  technology to implement a client access protocol that is POSIX-compatible and has the distinct advantage that the client is fully aware of the Gluster volume architecture. This means that with a redundant volume architecture (replicate or disperse), the client automatically has  highly-available  access to the Gluster volume data without any special configuration.  Additionally, the native client can benefit from multiple data paths and specific protocol tunings, allowing for greater overall throughput and lower latency under most workloads.  Here on the same client we mount the Gluster  distvol  volume a second time, this time using the Gluster native client.  sudo mkdir -p /rhgs/client/native/distvol\nsudo mount -t glusterfs rhgs1:distvol /rhgs/client/native/distvol/  Examine the new mount.  df -h /rhgs/client/native/distvol/  Filesystem      Size  Used Avail Use% Mounted on  rhgs1:distvol       20G   67M   20G   1% /rhgs/client/native/distvol  We can see here that the Gluster volume is  mounted twice by the two different protocols .  mount | grep distvol  rhgs1:/distvol on /rhgs/client/nfs/distvol type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.100.1.11,mountvers=3,mountport=38465,mountproto=tcp,local_lock=none,addr=10.100.1.11)  rhgs1:distvol on /rhgs/client/native/distvol type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072)  Listing the files from our new native mount point, we can see that the files we created through the original NFS mount point are visible.  ls /rhgs/client/native/distvol/mydir/ | wc -l  100  Now we\u2019ll create an additional set of 100 files, this time through the new native client mount point.  for i in {101..200}; do echo hello$i > /rhgs/client/native/distvol/mydir/file$i; done  We can now see that the 200 total files are available through  both  the native and NFS mount points.  ls /rhgs/client/native/distvol/mydir/ | wc -l  200  ls /rhgs/client/nfs/distvol/mydir/ | wc -l  200",
            "title": "Native Client Access"
        },
        {
            "location": "/gluster-module-2/#windows-client-access",
            "text": "In order to make our Gluster volume available to Windows clients, we need to make a few configuration changes. Re-connect to the  rhgs1  node from your local ssh client.  ssh -i <PEM_FILE> ec2-user@<PUBLIC_IP>  From node  rhgs1 , run the below commands to modify the  rep01  volume.  sudo gluster volume set rep01 stat-prefetch off  volume set: success  sudo gluster volume set rep01 server.allow-insecure on  volume set: success  sudo sed -i '/^end-volume/i option rpc-auth-allow-insecure on' /etc/glusterfs/glusterd.vol\nsudo systemctl restart glusterd.service  sudo gluster volume set rep01 storage.batch-fsync-delay-usec 0  volume set: success  sudo adduser samba-user\necho -ne \"redhat\\nredhat\\n\" | sudo smbpasswd -s -a samba-user  Added user samba-user.  Here we temporarily mount the client interface on the rhgs1 server in order to create a directory for our Windows client to write to.  sudo mount -t glusterfs rhgs1:rep01 /mnt\nsudo mkdir /mnt/mysmbdir\nsudo chmod 777 /mnt/mysmbdir\nsudo umount /mnt  Using your local RDP client, connect to the public IP address of the Windows client system. The username is  Administrator  and the password is  RedHat1 . You can leave the domain field blank.  Using Windows PowerShell, we mount the Gluster volume to the Z: drive.  net use Z: \\\\10.100.1.11\\gluster-rep01 redhat /USER:samba-user  We create 100 new files in the mysmbdir subdirectory.  for($i=1; $i -le 100; $i++){echo hello$i > Z:\\mysmbdir\\winfile$i}  And we can confirm that there are 100 new files in place.  dir Z:\\mysmbdir | measure-object -line  Lines Words          Characters          Property  ----- -----          ----------          --------  100  NOTE:   Due to locking incompatibilities, it is  NOT  supported to to use the SMB/CIFS protocol (Windows client) at the same time as another client access method on the same Gluster volume. Doing so will result in data corruption. For the sake of this lab, we do this only to illustrate Gluster\u2019s functionality.  Connect to  client1  again with your local ssh client.  ssh -i <PEM_FILE> ec2-user@<PUBLIC_IP>  From here, you can mount the  rep01  volume and see the 100 new files added from the Windows client are visible.  sudo mkdir -p /rhgs/client/native/rep01\nsudo mount -t glusterfs rhgs1:rep01 /rhgs/client/native/rep01  ls /rhgs/client/native/rep01/mysmbdir | wc -l  100",
            "title": "Windows Client Access"
        },
        {
            "location": "/gluster-module-2/#analysis-of-volume-types",
            "text": "Connect to  rhgs1  again with your local ssh client.  ssh -i <PEM_FILE> ec2-user@<PUBLIC_IP>  Looking at the brick backend for the  distvol  volume on Gluster node  rhgs1 , we can see that only a subset of the 200 files that were created are present on this brick. (Note the numbers you see may be different than the examples below.)  ls /rhgs/brick_vdb/distvol/mydir/ | wc -l  95  Now connect to  rhgs2  \u2013 As a convenience, you can do this most simply directly from node rhgs1 (you will need to connect at the root user).  ssh root@rhgs2  Looking at the brick backend for the  distvol  volume on Gluster node  rhgs2 , we can see that the  remainder of the 200 files are located on this brick .  ls /rhgs/brick_vdb/distvol/mydir/ | wc -l  105  This is the natural effect of the  Distributed Hash Algorithm . Given the bricks in a distribute volume, the files will be pseudo-randomly placed among those bricks in a statistically even pattern.  While still on node  rhgs2 , take a look at the files in the brick backend for the  rep01  volume.  ls /rhgs/brick_vdc/rep01/mysmbdir/ | wc -l  100  Above we created 100 files in this directory from the Windows client, and this time we see  exactly 100 files on the brick backend . This is the effect of the  Automatic File Replication , which synchronously places copies of the files written by the clients on the replica peer bricks.  Exit from node  rhgs2  (simply type exit at the command line), returning to node  rhgs1 . Look at the  rep01  brick backend on this node and confirm that the file count matches that of node rhgs2.  ls /rhgs/brick_vdc/rep01/mysmbdir/ | wc -l  100",
            "title": "Analysis of Volume Types"
        },
        {
            "location": "/gluster-module-2/#end-your-lab",
            "text": "This concludes Gluster Test Drive Module 2 - Volume Setup and Client Access. Please follow these steps to end your lab and evaluate the experience.   [1] Close your remote sessions.  In the  qwik LABS page, click  End Lab .  In the confirmation message, click  OK .  (Optional) Tell us about your lab experience!",
            "title": "End Your Lab"
        },
        {
            "location": "/gluster-module-2/#additional-resources",
            "text": "https://www.redhat.com/en/technologies/storage/gluster",
            "title": "Additional Resources"
        }
    ]
}