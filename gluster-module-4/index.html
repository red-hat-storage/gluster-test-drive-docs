<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../images/favicon.ico">
        

	<title>Module 4 - Disperse Volumes (Erasure Coding) - RHGS Test Drive Instructions</title>

        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../css/highlight.css">
        <link href="../css/base.css" rel="stylesheet">
        <link href="../extra.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="..">RHGS Test Drive Instructions</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            
                <li >
                    <a href="..">Introduction</a>
                </li>
            
            
            
                <li class="dropdown active">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">Modules <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
<li >
    <a href="../gluster-module-1/">Module 1 - Introduction to Gluster concepts</a>
</li>

                    
                        
<li >
    <a href="../gluster-module-2/">Module 2 - Volume Setup and Client Access</a>
</li>

                    
                        
<li >
    <a href="../gluster-module-3/">Module 3 - Volume Operations and Administration</a>
</li>

                    
                        
<li class="active">
    <a href="./">Module 4 - Disperse Volumes (Erasure Coding)</a>
</li>

                    
                        
<li >
    <a href="../gluster-module-5/">Module 5 - Tiered Volumes (Cache Tiering)</a>
</li>

                    
                    </ul>
                </li>
            
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                <li >
                    <a rel="next" href="../gluster-module-3/">
                        <i class="fa fa-arrow-left"></i> Previous
                    </a>
                </li>
                <li >
                    <a rel="prev" href="../gluster-module-5/">
                        Next <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#lab-guide-gluster-test-drive-module-4-disperse-volumes-erasure-coding">Lab Guide  Gluster Test Drive Module 4  Disperse Volumes (Erasure Coding)</a></li>
        
            <li><a href="#lab-agenda">Lab Agenda</a></li>
        
            <li><a href="#getting-started">Getting Started</a></li>
        
            <li><a href="#lab-setup">Lab Setup</a></li>
        
            <li><a href="#about-disperse-volumes">About Disperse Volumes</a></li>
        
            <li><a href="#build-your-gdeploy-configuration">Build your gdeploy Configuration</a></li>
        
            <li><a href="#deploy-and-review-your-disperse-volume">Deploy and Review your Disperse Volume</a></li>
        
            <li><a href="#file-operations-on-a-disperse-volume">File Operations on a Disperse Volume</a></li>
        
    
        <li class="main "><a href="#end-of-module-4">End of Module 4</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">

<h1 id="lab-guide-gluster-test-drive-module-4-disperse-volumes-erasure-coding">Lab Guide <br/> Gluster Test Drive Module 4 <br/> Disperse Volumes (Erasure Coding)</h1>
<h2 id="lab-agenda">Lab Agenda</h2>
<p>Welcome to the Gluster Test Drive Module 4 - Disperse Volumes (Erasure Coding). In this lab you will:</p>
<ul>
<li>Understand the basic concepts of erasure coding</li>
<li>Create a gdeploy configuration for a disperse volume</li>
<li>Deploy a disperse volume using gdeploy</li>
<li>Write files to a disperse volume and observe the backend</li>
<li>Manually fail bricks and observe the high availability of erasure coding</li>
</ul>
<h2 id="getting-started">Getting Started</h2>
<p>If you have not already done so, click the <img src="http://us-west-2-aws-training.s3.amazonaws.com/awsu-spl/spl02-working-ebs/media/image005.png"> button in the navigation bar above to launch your lab. If you are prompted for a token, use the one distributed to you (or credits you&rsquo;ve purchased).</p>
<blockquote>
<p><strong>NOTE</strong> It may take <strong>up to 10 minutes</strong> for your lab systems to start up before you can access them.</p>
</blockquote>
<h2 id="lab-setup">Lab Setup</h2>
<h3 id="connect-to-the-lab">Connect to the Lab</h3>
<p>Connect to the <strong>rhgs1</strong> server instance using its public IP address from the <strong>Addl. Info</strong> tab to the right (Linux/Mac example below).</p>
<pre class="codehilite"><code class="language-bash">ssh gluster@&lt;rhgs1PublicIP&gt;</code></pre>


<h2 id="about-disperse-volumes">About Disperse Volumes</h2>
<p>Gluster disperse volumes utilize <strong>erasure coding</strong> technology to provide data protection with a lower overall infrastructure investment. Unlike standard replication, in which to protect <em>n</em> bricks against <em>r</em> failures you must invest in a total of <em>n+(n*r)</em> bricks, with erasure coding you can design for the same level of protection with an investment of only <em>n+(n/r)</em> or even less.</p>
<p>For example, assume you need <em>n=4</em> bricks to hold your data set and you require protection from failure of <em>r=2</em> bricks. If you use standard <strong>replication</strong>, your investment in bricks will be <em>4+(4*2)</em> or <strong>12 total bricks</strong>. If instead you utilize <strong>erasure coding</strong>, your investment in bricks can be <em>4+(4/2)</em> or <strong>6 total bricks</strong>.</p>
<p><img alt="" src="../images/RHS_Gluster_Test_Drive-Module_1-dblack-201610-19.png" /></p>
<h3 id="erasure-coding-and-parity">Erasure Coding and Parity</h3>
<p>Instead of writing multiple exact copies of data, erasure coding algorithms write a combination of <em>data</em> and <em>parity</em> across bricks in the volume. A number of encoding algorithms are available offering varying levels of protection in terms of a <em>redundancy-to-data</em> ratio. For our lab, you will be using a 4:2 ratio &ndash; 2 levels of redundancy for every 4 data bricks, meaning that each disperse set requires exactly 6 bricks.</p>
<blockquote>
<p><strong>NOTE</strong> This functionality is very similar to how RAID works at the block level. You can relate our 4:2 erasure coding ratio to RAID level 6.</p>
</blockquote>
<p>When a file is written to the disperse volume, it is broken into calculated chunks of data and written across all of the bricks in the disperse set. When a file is read, it can be <strong>algorithmically reassembled from any n of the bricks where n is the number of data bricks in the set</strong>. Here with our 4:2 ratio, therefore, any 4 of the 6 bricks can be used to retrieve the files.</p>
<h3 id="use-cases">Use Cases</h3>
<p>Disperse volumes offer <em>space-efficient</em> and <em>capacity-optimized</em> storage architectures. However, there are a couple of primary tradeoffs when considering this method of data protection.</p>
<ul>
<li>The overhead of the algorithms can lead to decreased performance for your data set, particularly for reads and for small files.</li>
<li>The files are no longer stored in their whole original form on the brick backends, making offline or backend retrieval of data impossible.</li>
</ul>
<blockquote>
<p><strong>NOTE</strong> Interestingly, it has been shown that, under most conditions with the Gluster native client, write performance is unaffected by erasure coding and is even sometimes modestly improved.</p>
</blockquote>
<p>Disperse volumes should be used when capacity is of greater value than performance. Larger file workloads (1GB+) will experience the least performance degredation versus replicated volumes. <em>You should avoid using the NFS client with disperse volumes.</em></p>
<h2 id="build-your-gdeploy-configuration">Build your gdeploy Configuration</h2>
<p>You will use the <strong>Ansible</strong>-based deployment tool <code>gdeploy</code> in order to create your disperse volume. For this, you will need to build a custom configuration file. Store this file as <code>/home/gluster/ecvol.conf</code>.</p>
<blockquote>
<p><strong>NOTE</strong> For your convenience, the <code>vim</code> and <code>emacs</code> editing tools are installed on all lab systems.</p>
</blockquote>
<h3 id="hosts-section">Hosts Section</h3>
<p>First you need to define in the config file the set of Gluster hosts that will participate in the disperse volume. You will use all 6 of your local lab server nodes.</p>
<pre class="codehilite"><code class="language-ini">[hosts]
rhgs1
rhgs2
rhgs3
rhgs4
rhgs5
rhgs6</code></pre>


<h3 id="backend-setup-section">Backend-Setup Section</h3>
<p>The backend setup across all of the Gluster nodes is identical, and therefore we need only one universal backend-setup section in the config file. In this section, you define the block devices that will host the bricks filesystems (<strong>the block device should be xvdd</strong>), the LVM structure (thin provisioning in this case), the filesystem mount point, and the subdirectory that will be used to host the brick.</p>
<pre class="codehilite"><code class="language-ini">[backend-setup]
devices=xvdd
vgs=rhgs_vg3
pools=rhgs_thinpool3
lvs=rhgs_lv3
mountpoints=/rhgs/brick_xvdd
brick_dirs=/rhgs/brick_xvdd/ecvol</code></pre>


<h3 id="volume-section">Volume Section</h3>
<p>Next you define the specific architecture and configuration of the disperse volume you are creating.</p>
<pre class="codehilite"><code class="language-ini">[volume]
action=create
volname=ecvol
disperse=yes
disperse_count=4
redundancy_count=2
force=yes</code></pre>


<h3 id="clients-section">Clients Section</h3>
<p>Finally, you can automatically mount the new <code>ecvol</code> volume to the client systems.</p>
<pre class="codehilite"><code class="language-ini">[clients]
action=mount
volname=ecvol
hosts=client1,client2
fstype=glusterfs
client_mount_points=/rhgs/client/native/ecvol</code></pre>


<blockquote>
<p><strong>NOTE</strong> You can check your work against the provided <code>/home/gluster/example/ecvol.conf</code> file.</p>
</blockquote>
<h2 id="deploy-and-review-your-disperse-volume">Deploy and Review your Disperse Volume</h2>
<p>Using <code>gdeploy</code>, automate the deploymentof your <code>ecvol</code> volume.</p>
<blockquote>
<p><strong>NOTE</strong> Sudo is not needed for the <code>gdeploy</code> command.</p>
</blockquote>
<pre class="codehilite"><code class="language-bash">gdeploy -c /home/gluster/ecvol.conf</code></pre>


<p>When the deployment completes, you should find that you have a properly-configured and started disperse Gluster volume.</p>
<pre class="codehilite"><code class="language-bash">sudo gluster volume info ecvol</code></pre>


<div><code>
Volume Name: ecvol
Type: Disperse
Volume ID: 981dddb3-d99d-47bb-821e-b6fcb918a4f9
Status: Started
Number of Bricks: 1 x (4 + 2) = 6
Transport-type: tcp
Bricks:
Brick1: rhgs1:/rhgs/brick_xvdd/ecvol
Brick2: rhgs2:/rhgs/brick_xvdd/ecvol
Brick3: rhgs3:/rhgs/brick_xvdd/ecvol
Brick4: rhgs4:/rhgs/brick_xvdd/ecvol
Brick5: rhgs5:/rhgs/brick_xvdd/ecvol
Brick6: rhgs6:/rhgs/brick_xvdd/ecvol
Options Reconfigured:
performance.readdir-ahead: on
</code></div>

<p>Also take a look at the layout with the <code>gstatus</code> command.</p>
<pre class="codehilite"><code class="language-bash">sudo gstatus -l -w -v ecvol</code></pre>


<div><code> 
     Product: RHGS Server v3.1Update3  Capacity:  60.00 GiB(raw bricks)
      Status: HEALTHY                      198.00 MiB(raw used)
   Glusterfs: 3.7.5                         40.00 GiB(usable from volumes)
  OverCommit: No                Snapshots:   0

Volume Information
    ecvol            UP - 6/6 bricks up - Disperse
                     Capacity: (0% used) 132.00 MiB/40.00 GiB (used/total)
                     Snapshots: 0
                     Self Heal:  6/ 6
                     Tasks Active: None
                     Protocols: glusterfs:on  NFS:on  SMB:on
                     Gluster Connectivty: 8 hosts, 84 tcp connections

    ecvol----------- +
                     |
                Disperse (ida)
                         |
                         +-- Disperse set0 (ida)
                               |
                               +--rhgs1:/rhgs/brick_xvdd/ecvol(UP) 33.00 MiB/10.00 GiB 
                               |
                               +--rhgs2:/rhgs/brick_xvdd/ecvol(UP) 33.00 MiB/10.00 GiB 
                               |
                               +--rhgs3:/rhgs/brick_xvdd/ecvol(UP) 33.00 MiB/10.00 GiB 
                               |
                               +--rhgs4:/rhgs/brick_xvdd/ecvol(UP) 33.00 MiB/10.00 GiB 
                               |
                               +--rhgs5:/rhgs/brick_xvdd/ecvol(UP) 33.00 MiB/10.00 GiB 
                               |
                               +--rhgs6:/rhgs/brick_xvdd/ecvol(UP) 33.00 MiB/10.00 GiB
</code></div>

<h2 id="file-operations-on-a-disperse-volume">File Operations on a Disperse Volume</h2>
<h3 id="create-files-from-the-client">Create Files from the Client</h3>
<p>From <strong>rhgs1</strong> connect via SSH to <strong>client1</strong>.</p>
<pre class="codehilite"><code class="language-bash">ssh gluster@client1</code></pre>


<p>Confirm that your <code>ecvol</code> volume is mounted.</p>
<pre class="codehilite"><code class="language-bash">mount | grep ecvol</code></pre>


<p><code>rhgs1:ecvol on /rhgs/client/native/ecvol type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072)</code></p>
<pre class="codehilite"><code class="language-bash">df -h /rhgs/client/native/ecvol</code></pre>


<p><code>Filesystem      Size  Used Avail Use% Mounted on</code><br />
<code>rhgs1:ecvol      40G  133M   40G   1% /rhgs/client/native/ecvol</code></p>
<p>Write a few 10MB files of <strong>plain text</strong> to the <code>ecvol</code> volume.</p>
<pre class="codehilite"><code class="language-bash">sudo mkdir /rhgs/client/native/ecvol/mydir
sudo chmod 777 /rhgs/client/native/ecvol/mydir/
for i in {0..5}; do base64 /dev/urandom | head -c 10240k &gt; /rhgs/client/native/ecvol/mydir/ecfile$i; done</code></pre>


<pre class="codehilite"><code class="language-bash">ls -lh /rhgs/client/native/ecvol/mydir/</code></pre>


<div><code>
total 60M
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:11 ecfile0
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:11 ecfile1
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:11 ecfile2
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:11 ecfile3
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:11 ecfile4
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:11 ecfile5
</code></div>

<p>Validate that you have created ASCII text files.</p>
<pre class="codehilite"><code class="language-bash">file /rhgs/client/native/ecvol/mydir/ecfile0 </code></pre>


<p><code>/rhgs/client/native/ecvol/mydir/ecfile0: ASCII text</code></p>
<p>Return to lab node <strong>rhgs1</strong>.</p>
<pre class="codehilite"><code class="language-bash">exit</code></pre>


<p>Now take a look at your files on the brick backend. You will find that all of the files exist, but are smaller in size.</p>
<pre class="codehilite"><code class="language-bash">ls -lh /rhgs/brick_xvdd/ecvol/mydir/</code></pre>


<div><code>
total 15M
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:16 ecfile0
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile1
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile2
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile3
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile4
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile5
</code></div>

<p>Each one of these files in a fragment of the file created at the client combined with the parity data required to calculate the reassembly of the file.</p>
<p>Note that the file types here are <code>data</code> instead of <code>ASCII text</code>.</p>
<pre class="codehilite"><code class="language-bash">file /rhgs/brick_xvdd/ecvol/mydir/ecfile0 </code></pre>


<p><code>/rhgs/brick_xvdd/ecvol/mydir/ecfile0: data</code></p>
<h3 id="test-disperse-volume-resilliency">Test Disperse Volume Resilliency</h3>
<p>Choose any 2 of the gluster nodes other than rhgs1 (rhgs2 through rhgs6), and stop all gluster processes on those nodes. <em>In the example here, we have chosen nodes rhgs2 and rhgs5.</em></p>
<pre class="codehilite"><code class="language-bash">for i in 2 5; do ssh root@rhgs$i &quot;systemctl stop glusterd.service; pkill glusterfs; pkill glusterfsd&quot;; done</code></pre>


<p>Take a look at the volume now with the <code>gstatus</code> tool. You will notice it is marked as <code>UNHEALTHY</code> and <code>DEGRADED</code>.</p>
<pre class="codehilite"><code class="language-bash">sudo gstatus -w -v ecvol</code></pre>


<div><code> 
     Product: RHGS Server v3.1Update3  Capacity:  40.00 GiB(raw bricks)
      Status: UNHEALTHY(4)                 192.00 MiB(raw used)
   Glusterfs: 3.7.5                         40.00 GiB(usable from volumes)
  OverCommit: No                Snapshots:   0

Volume Information
    ecvol            UP(DEGRADED) - 4/6 bricks up - Disperse
                     Capacity: (0% used) 192.00 MiB/40.00 GiB (used/total)
                     Snapshots: 0
                     Self Heal:  4/ 6
                     Tasks Active: None
                     Protocols: glusterfs:on  NFS:on  SMB:on
                     Gluster Connectivty: 6 hosts, 40 tcp connections
</code></div>

<p>Return again to <strong>client1</strong> via SSH.</p>
<pre class="codehilite"><code class="language-bash">ssh gluster@client1</code></pre>


<p>Perform file operations on the <code>ecvol</code> volume mount point to confirm that your data is still fully accessible with the volume in a degraded state.</p>
<pre class="codehilite"><code class="language-bash">ls -lh /rhgs/client/native/ecvol/mydir/</code></pre>


<div><code>
total 60M
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:16 ecfile0
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:17 ecfile1
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:17 ecfile2
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:17 ecfile3
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:17 ecfile4
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:17 ecfile5
</code></div>

<pre class="codehilite"><code class="language-bash">file /rhgs/client/native/ecvol/mydir/ecfile3 </code></pre>


<p><code>/rhgs/client/native/ecvol/mydir/ecfile3: ASCII text</code></p>
<pre class="codehilite"><code class="language-bash">stat /rhgs/client/native/ecvol/mydir/ecfile5</code></pre>


<div><code>
  File: ‘/rhgs/client/native/ecvol/mydir/ecfile5’
  Size: 10485760    Blocks: 20480      IO Block: 131072 regular file
Device: 24h/36d Inode: 9763979935191258614  Links: 1
Access: (0664/-rw-rw-r--)  Uid: ( 1001/ gluster)   Gid: ( 1001/ gluster)
Context: system_u:object_r:fusefs_t:s0
Access: 2016-11-10 14:17:05.379530355 -0500
Modify: 2016-11-10 14:17:06.790558942 -0500
Change: 2016-11-10 14:17:06.799559124 -0500
 Birth: -
</code></div>

<pre class="codehilite"><code class="language-bash">cat /rhgs/client/native/ecvol/mydir/ecfile2 &gt; /dev/null</code></pre>


<p>Create a new plain text file in the <code>ecvol</code> volume mount point.</p>
<pre class="codehilite"><code class="language-bash">base64 /dev/urandom | head -c 10240k &gt; /rhgs/client/native/ecvol/mydir/ecfile6</code></pre>


<pre class="codehilite"><code class="language-bash">ls -lh /rhgs/client/native/ecvol/mydir/</code></pre>


<div><code>
total 70M
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:16 ecfile0
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:17 ecfile1
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:17 ecfile2
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:17 ecfile3
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:17 ecfile4
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:17 ecfile5
-rw-rw-r--. 1 gluster gluster 10M Nov 10 14:29 ecfile6
</code></div>

<p>Return to lab node <strong>rhgs1</strong>.</p>
<pre class="codehilite"><code class="language-bash">exit</code></pre>


<p>Note that the new file you created is available on node <strong>rhgs1</strong> where the Gluster processes were not shut down.</p>
<pre class="codehilite"><code class="language-bash">ls -lh /rhgs/brick_xvdd/ecvol/mydir/</code></pre>


<div><code>
total 18M
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:16 ecfile0
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile1
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile2
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile3
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile4
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile5
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:29 ecfile6
</code></div>

<p>Connect via ssh to one of the nodes on which you stopped the Gluster processes above.</p>
<pre class="codehilite"><code class="language-bash">ssh rhgs2</code></pre>


<p>Note that on this node the new file you created does not exist.</p>
<pre class="codehilite"><code class="language-bash">ls -lh /rhgs/brick_xvdd/ecvol/mydir/</code></pre>


<div><code>
total 15M
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:16 ecfile0
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile1
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile2
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile3
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile4
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile5
</code></div>

<p>Re-start the Gluster services on the nodes where you stopped them above.</p>
<pre class="codehilite"><code class="language-bash">for i in 2 5; do ssh root@rhgs$i &quot;systemctl start glusterd.service&quot;; done</code></pre>


<p>Confirm that the <code>ecvol</code> volume is no longer in a <code>DEGRADED</code> state.</p>
<pre class="codehilite"><code class="language-bash">sudo gstatus -w -v ecvol</code></pre>


<div><code>
     Product: RHGS Server v3.1Update3  Capacity:  60.00 GiB(raw bricks)
      Status: HEALTHY                      298.00 MiB(raw used)
   Glusterfs: 3.7.5                         40.00 GiB(usable from volumes)
  OverCommit: No                Snapshots:   0

Volume Information
    ecvol            UP - 6/6 bricks up - Disperse
                     Capacity: (0% used) 199.00 MiB/40.00 GiB (used/total)
                     Snapshots: 0
                     Self Heal:  6/ 6
                     Tasks Active: None
                     Protocols: glusterfs:on  NFS:on  SMB:on
                     Gluster Connectivty: 8 hosts, 84 tcp connections
</code></div>

<p>Allow a minute or two for the <em>self-heal daemon</em> to catch up, and then note that the new file you created is now available on this brick backend.</p>
<pre class="codehilite"><code class="language-bash">ls -lh /rhgs/brick_xvdd/ecvol/mydir/</code></pre>


<div><code>
total 18M
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:16 ecfile0
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile1
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile2
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile3
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile4
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:17 ecfile5
-rw-rw-r--. 2 gluster gluster 2.5M Nov 10 14:29 ecfile6
</code></div>

<p>Return to node <strong>rhgs1</strong>.</p>
<pre class="codehilite"><code class="language-bash">exit</code></pre>


<h1 id="end-of-module-4">End of Module 4</h1>
<p>This concludes <strong>Gluster Test Drive Module 4 - Disperse Volumes (Erasure Coding)</strong>. You may continue now with Module 5, or return at any time to access the modules in any order you wish.</p></div>
        </div>

        <footer class="col-md-12">
            <hr>
            
                <center>Copyright ©2016 Red Hat, Inc.</center>
            
            <center>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</center>
        </footer>

        <script src="../js/jquery-1.10.2.min.js"></script>
        <script src="../js/bootstrap-3.0.3.min.js"></script>
        <script src="../js/highlight.pack.js"></script>
        <script>var base_url = '..';</script>
        <script data-main="../mkdocs/js/search.js" src="../mkdocs/js/require.js"></script>
        <script src="../js/base.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="modal-header">
                        <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                        <h4 class="modal-title" id="exampleModalLabel">Search</h4>
                    </div>
                    <div class="modal-body">
                        <p>
                            From here you can search these documents. Enter
                            your search terms below.
                        </p>
                        <form role="form">
                            <div class="form-group">
                                <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                            </div>
                        </form>
                        <div id="mkdocs-search-results"></div>
                    </div>
                    <div class="modal-footer">
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>