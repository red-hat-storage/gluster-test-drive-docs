<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../images/favicon.ico">
        

	<title>Module 3 - Volume Operations and Administration - RHGS Test Drive Instructions</title>

        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../css/highlight.css">
        <link href="../css/base.css" rel="stylesheet">
        <link href="../extra.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="..">RHGS Test Drive Instructions</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            
                <li >
                    <a href="..">Introduction</a>
                </li>
            
            
            
                <li class="dropdown active">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">Modules <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
<li >
    <a href="../gluster-module-1/">Module 1 - Introduction to Gluster concepts</a>
</li>

                    
                        
<li >
    <a href="../gluster-module-2/">Module 2 - Volume Setup and Client Access</a>
</li>

                    
                        
<li class="active">
    <a href="./">Module 3 - Volume Operations and Administration</a>
</li>

                    
                        
<li >
    <a href="../gluster-module-4/">Module 4 - Disperse Volumes (Erasure Coding)</a>
</li>

                    
                        
<li >
    <a href="../gluster-module-5/">Module 5 - Tiered Volumes (Cache Tiering)</a>
</li>

                    
                    </ul>
                </li>
            
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                <li >
                    <a rel="next" href="../gluster-module-2/">
                        <i class="fa fa-arrow-left"></i> Previous
                    </a>
                </li>
                <li >
                    <a rel="prev" href="../gluster-module-4/">
                        Next <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#this-lab-is-a-work-in-progress">THIS LAB IS A WORK IN PROGRESS</a></li>
        
    
        <li class="main "><a href="#lab-guide-gluster-test-drive-module-3-volume-operations-and-administration">Lab Guide  Gluster Test Drive Module 3  Volume Operations and Administration</a></li>
        
            <li><a href="#lab-agenda">Lab Agenda</a></li>
        
            <li><a href="#lab-setup">Lab Setup</a></li>
        
            <li><a href="#volume-self-healing">Volume Self-Healing</a></li>
        
            <li><a href="#volume-expansion-and-rebalance">Volume Expansion and Rebalance</a></li>
        
            <li><a href="#changing-volume-configuration">Changing Volume Configuration</a></li>
        
            <li><a href="#analyzing-volume-performance">Analyzing Volume Performance</a></li>
        
            <li><a href="#administration-of-volume-quotas">Administration of Volume Quotas</a></li>
        
    
        <li class="main "><a href="#end-of-module-3">End of Module 3</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">

<h1 id="this-lab-is-a-work-in-progress"><strong>THIS LAB IS A WORK IN PROGRESS</strong></h1>
<h1 id="lab-guide-gluster-test-drive-module-3-volume-operations-and-administration">Lab Guide <br/> Gluster Test Drive Module 3 <br/> Volume Operations and Administration</h1>
<h2 id="lab-agenda">Lab Agenda</h2>
<p>Welcome to the Gluster Test Drive Module 3 - Volume Operations and Administration. In this lab you will:</p>
<ul>
<li>Initiate and observe volume self-heal behavior</li>
<li>Expand a distribute-replicate volume and observe rebalance</li>
<li>Change volume configurations and view changes</li>
<li>Understand the use of the <code>volume profile</code> command</li>
<li>Understand the use of the <code>volume top</code> command</li>
<li>Set and observe volume and directory quotas</li>
</ul>
<h2 id="lab-setup">Lab Setup</h2>
<h3 id="connect-to-the-lab">Connect to the Lab</h3>
<p>Connect to the <strong>rhgs1</strong> server instance using its public IP address from the <strong>Addl. Info</strong> tab to the right (Linux/Mac example below).</p>
<pre class="codehilite"><code class="language-bash">ssh gluster@&lt;rhgs1PublicIP&gt;</code></pre>


<h3 id="if-needed-create-the-repvol-volume">If Needed, Create the repvol Volume</h3>
<p>If you have not already done so as part of <strong>Module 2</strong>, deploy the <strong>repvol</strong> volume using the provided gdeploy configuraiton file.</p>
<pre class="codehilite"><code class="language-bash">gdeploy -c ~/repvol.conf</code></pre>


<p>Confirm the volume configuration.</p>
<pre class="codehilite"><code class="language-bash">sudo gluster volume info repvol</code></pre>


<p><code>Volume Name: repvol</code><br />
<code>Type: Replicate</code><br />
<code>Volume ID: 6fb61bd8-4642-44e9-a5ec-4f15c8740b6f</code><br />
<code>Status: Started</code><br />
<code>Number of Bricks: 1 x 2 = 2</code><br />
<code>Transport-type: tcp</code><br />
<code>Bricks:</code><br />
<code>Brick1: rhgs1:/rhgs/brick_xvdc/repvol</code><br />
<code>Brick2: rhgs2:/rhgs/brick_xvdc/repvol</code><br />
<code>Options Reconfigured:</code><br />
<code>performance.readdir-ahead: on</code></p>
<h2 id="volume-self-healing">Volume Self-Healing</h2>
<h3 id="about-self-healing">About Self-Healing</h3>
<p>A Gluster replicated volume maintains multiple copies of files synchronously on the volume bricks. This can provide high availability, load balancing, and increased read throughput. When a member of a replica set becomes unavailable for any reason, Gluster tracks the changes made to the online bricks in order to facilitate a set of self-healing processes when the offline bricks return to service.</p>
<p>There are two types of self-heal that operate concurrently: <em>client-side</em> and <em>server-side</em> (also known as <em>proactive</em>). Client-side heals are triggered when a client performs a file operation on a file or directory marked as needing healed. Server-side heals are managed by background processes that run on each Gluster node, periodically scouring the bricks and healing any files that they find as marked.</p>
<h3 id="offlining-a-brick">Offlining a Brick</h3>
<p>Your <strong>repvol</strong> volume has two bricks and a replication value of 2, and therefore a single replica set between bricks on nodes <strong>rhgs1</strong> and <strong>rhgs2</strong>. On node <strong>rhgs1</strong> you will stop all Gluster services and processes to ensure its bricks are offline.</p>
<pre class="codehilite"><code class="language-bash">sudo systemctl stop glusterd.service
sudo pkill glusterfs
sudo pkill glusterfsd</code></pre>


<p>Confirm there are no gluster processes running. The below command should return nothing.</p>
<pre class="codehilite"><code class="language-bash">sudo ps -ef |grep glusterfs | grep -v grep</code></pre>


<h3 id="writing-files-to-the-degraded-volume">Writing Files to the Degraded Volume</h3>
<p>From <strong>rhgs1</strong> connect via SSH to <strong>client1</strong>.</p>
<pre class="codehilite"><code class="language-bash">ssh gluster@client1</code></pre>


<p>If you did not already mount the <strong>repvol</strong> volume as part of Module 2, do it now.</p>
<blockquote>
<p><em>NOTE</em> Below we mount the volume on <strong>client1</strong> using node <strong>rhgs2</strong> as the server because the Gluster services on node <strong>rhgs1</strong> were offlined above.</p>
</blockquote>
<pre class="codehilite"><code class="language-bash">sudo mkdir -p /rhgs/client/native/repvol
sudo mount -t glusterfs rhgs2:repvol /rhgs/client/native/repvol</code></pre>


<p>Confirm the volume is mounted.</p>
<pre class="codehilite"><code class="language-bash">df -h | grep repvol</code></pre>


<p><code>rhgs2:repvol     10G   34M   10G   1% /rhgs/client/native/repvol</code></p>
<p>Create a directory to hold your files and set permissions appropriately.</p>
<pre class="codehilite"><code class="language-bash">sudo mkdir /rhgs/client/native/repvol/mydir
sudo chmod 777 /rhgs/client/native/repvol/mydir</code></pre>


<p>Write 10 new files to the directory you created in the mounted volume.</p>
<blockquote>
<p><strong>NOTE</strong> There is a default timeout of 42 seconds before which the client will continue trying to contact the offline brick. If that timeout has not expired before you issue a file operation on the mounted volume, you may experience a delay at the client.</p>
</blockquote>
<pre class="codehilite"><code class="language-bash">for i in {001..010}; do echo hello$i &gt; /rhgs/client/native/repvol/mydir/healme$i; done</code></pre>


<p>Confirm the new files were written.</p>
<pre class="codehilite"><code class="language-bash">ls /rhgs/client/native/repvol/mydir/ | wc -l</code></pre>


<p><code>10</code></p>
<h3 id="viewing-the-volume-state">Viewing the Volume State</h3>
<p>Return to lab node <strong>rhgs1</strong>.</p>
<pre class="codehilite"><code class="language-bash">exit</code></pre>


<p>On node <strong>rhgs1</strong>, note that the new directory you just created is not visible on the brick backend because this brick was offline at the time of the write.</p>
<pre class="codehilite"><code class="language-bash">ls /rhgs/brick_xvdc/repvol/mydir</code></pre>


<p><code>ls: cannot access /rhgs/brick_xvdc/repvol/mydir: No such file or directory</code></p>
<p>Connect to node <strong>rhgs2</strong> via SSH.</p>
<pre class="codehilite"><code class="language-bash">ssh gluster@rhgs2</code></pre>


<p>On node <strong>rhgs2</strong>, note that the <code>volume status</code> output only shows the processes for itself and nothing for node <strong>rhgs1</strong>.</p>
<pre class="codehilite"><code class="language-bash">sudo gluster volume status repvol</code></pre>


<p><code>Status of volume: repvol</code><br />
<code>Gluster process                             TCP Port  RDMA Port  Online  Pid</code><br />
<code>------------------------------------------------------------------------------</code><br />
<code>Brick rhgs2:/rhgs/brick_xvdc/repvol         49152     0          Y       11468</code><br />
<code>NFS Server on localhost                     2049      0          Y       11490</code><br />
<code>Self-heal Daemon on localhost               N/A       N/A        Y       11495</code><br />
<code></code> <br />
<code>Task Status of Volume repvol</code><br />
<code>------------------------------------------------------------------------------</code><br />
<code>There are no active volume tasks</code></p>
<p>Confirm that the files you created at the client are visible on the brick backend.</p>
<pre class="codehilite"><code class="language-bash">ls /rhgs/brick_xvdc/repvol/mydir | wc -l</code></pre>


<p><code>10</code></p>
<p>You can view the volume&rsquo;s knowledge of the pending file heals with the below command.</p>
<pre class="codehilite"><code class="language-bash">sudo gluster volume heal repvol info</code></pre>


<p><code>Brick rhgs1:/rhgs/brick_xvdc/repvol</code><br />
<code>Status: Transport endpoint is not connected</code><br />
<code></code><br />
<code>Brick rhgs2:/rhgs/brick_xvdc/repvol</code><br />
<code>/</code><br />
<code>/mydir</code><br />
<code>/mydir/healme001</code><br />
<code>/mydir/healme002</code><br />
<code>/mydir/healme003</code><br />
<code>/mydir/healme004</code><br />
<code>/mydir/healme005</code><br />
<code>/mydir/healme006</code><br />
<code>/mydir/healme007</code><br />
<code>/mydir/healme008</code><br />
<code>/mydir/healme009</code><br />
<code>/mydir/healme010</code><br />
<code>Number of entries: 12</code></p>
<h3 id="triggering-the-self-heal">Triggering the Self-Heal</h3>
<p>Return to lab node <strong>rhgs1</strong>.</p>
<pre class="codehilite"><code class="language-bash">exit</code></pre>


<p>Re-start the gluster services. Note that starting the <code>glusterd</code> management daemon will automatically start the <code>glusterfsd</code> brick and <code>glusterfs</code> supporting processes.</p>
<pre class="codehilite"><code class="language-bash">sudo systemctl start glusterd.service</code></pre>


<p>Connect again to <strong>client1</strong> via SSH.</p>
<pre class="codehilite"><code class="language-bash">ssh gluster@client1</code></pre>


<p>Stat a file that you created above. This will trigger client-side self-heal.</p>
<pre class="codehilite"><code class="language-bash">stat /rhgs/client/native/repvol/mydir/healme001</code></pre>


<p>Return to lab node <strong>rhgs1</strong>.</p>
<pre class="codehilite"><code class="language-bash">exit</code></pre>


<p>Looking again at the brick backend for the <strong>repvol</strong> volume on node <strong>rhgs1</strong> we can now see the healed files are present.</p>
<pre class="codehilite"><code class="language-bash">ls /rhgs/brick_xvdc/repvol/mydir/ | wc -l</code></pre>


<p><code>10</code></p>
<h2 id="volume-expansion-and-rebalance">Volume Expansion and Rebalance</h2>
<h3 id="about-rebalance">About Rebalance</h3>
<p>When a Gluster volume is scaled horizontally by adding additional bricks, the overall architecture of the volume changes fundamentally and affects data placement by the <em>Distributed Hash Algorithm</em>. New file writes can easily account for the new bricks by including them in the random file placement calculation. However, any existing files will remain on their original bricks until a <strong>rebalance</strong> is initiated by the administrator.</p>
<p>A rebalance triggers a re-calculation of data placement for existing files in the volume. A background operation then takes responsibility for moving the files to their new bricks, as needed. This, of course, is one of the heavier operations that Gluster performs, consuming additional system resources across the trusted pool until the rebalance is complete.</p>
<h3 id="about-distributed-replicated-volumes">About Distributed-Replicated Volumes</h3>
<p>In the lab modules so far you have worked separately with <em>Distributed</em> and <em>Replicated</em> volumes. Here you will expand your <strong>repvol</strong> volume by adding additional bricks. The replica count remains 2, meaning that every file is written synchronously to two bricks. By adding additional bricks (which you must do in sets of 2 to maintain the replica count), you are creating a distribution set out of multiple replica sets. Upon a file write, first a hashing calculation will be made to determine under which branch of the distribute set to place the file, then the write is made synchronously to the replica peers in that branch. This will be further illustrated in the commands below.</p>
<h3 id="add-files-to-the-repvol-volume">Add Files to the repvol Volume</h3>
<p>From <strong>rhgs1</strong> connect via SSH to <strong>client1</strong>.</p>
<pre class="codehilite"><code class="language-bash">ssh gluster@client1</code></pre>


<p>Add 200 new files to the <strong>repvol</strong> volume.</p>
<pre class="codehilite"><code class="language-bash">for i in {001..200}; do echo hello$i &gt; /rhgs/client/native/repvol/mydir/rebalanceme$i; done</code></pre>


<p>Confim the file count from the client. Note that we added 10 files in the self-heal section above, so the total file count should be 210.</p>
<pre class="codehilite"><code class="language-bash">ls /rhgs/client/native/repvol/mydir | wc -l</code></pre>


<p><code>210</code></p>
<p>Exit <strong>client1</strong>, returning to node <strong>rhgs1</strong>.</p>
<pre class="codehilite"><code class="language-bash">exit</code></pre>


<p>On node <strong>rhgs1</strong>, list the backend brick contents and notice that the file count matches that of the client&rsquo;s view of the volume. Currently, there is only one branch to the distribute set, so all files will be located on this brick and on its replica peer, <strong>rhgs2</strong>.</p>
<pre class="codehilite"><code class="language-bash">ls /rhgs/brick_xvdc/repvol/mydir | wc -l</code></pre>


<p><code>210</code></p>
<h3 id="expand-the-repvol-volume">Expand the repvol Volume</h3>
<p>First, examine the existing configuration for the <strong>repvol</strong> volume. Notice in particular the <strong>Type</strong> and <strong>Number of Bricks</strong> fields.</p>
<pre class="codehilite"><code class="language-bash">sudo gluster volume info repvol</code></pre>


<p><code>Volume Name: repvol</code><br />
<code>Type: Replicate</code><br />
<code>Volume ID: 2ec69e5b-0d04-4a3e-94c3-337b4302fbe8</code><br />
<code>Status: Started</code><br />
<code>Number of Bricks: 1 x 2 = 2</code><br />
<code>Transport-type: tcp</code><br />
<code>Bricks:</code><br />
<code>Brick1: rhgs1:/rhgs/brick_xvdc/repvol</code><br />
<code>Brick2: rhgs2:/rhgs/brick_xvdc/repvol</code><br />
<code>Options Reconfigured:</code><br />
<code>performance.readdir-ahead: on</code></p>
<p>You will use the <code>gdeploy</code> command to create the new brick backends for nodes rhgs3 through rhgs6, and to add these new bricks to the layout of the <strong>repvol</strong> volume. Take a look at the contents of the configuration file.</p>
<pre class="codehilite"><code class="language-bash">cat ~/repvol-expand.conf</code></pre>


<p><code>[hosts]</code><br />
<code>rhgs3</code><br />
<code>rhgs4</code><br />
<code>rhgs5</code><br />
<code>rhgs6</code><br />
<code></code><br />
<code>[backend-setup]</code><br />
<code>devices=xvdc</code><br />
<code>vgs=rhgs_vg2</code><br />
<code>pools=rhgs_thinpool2</code><br />
<code>lvs=rhgs_lv2</code><br />
<code>mountpoints=/rhgs/brick_xvdc</code><br />
<code>brick_dirs=/rhgs/brick_xvdc/repvol</code><br />
<code></code><br />
<code>[volume]</code><br />
<code>action=add-brick</code><br />
<code>volname=rhgs1:repvol</code><br />
<code>bricks=rhgs3:/rhgs/brick_xvdc/repvol,rhgs4:/rhgs/brick_xvdc/repvol,rhgs5:/rhgs/brick_xvdc/repvol,rhgs6:/rhgs/brick_xvdc/repvol</code></p>
<p>Use <code>gdeploy</code> to make the volume change.</p>
<pre class="codehilite"><code class="language-bash">gdeploy -c ~/repvol-expand.conf </code></pre>


<p>Take a look at the updated volume configuration. Note the changes to <strong>Type</strong> and <strong>Number of Bricks</strong>, as well as the additional bricks listed.</p>
<pre class="codehilite"><code class="language-bash">sudo gluster volume info repvol</code></pre>


<p><code>Volume Name: repvol</code><br />
<code>Type: Distributed-Replicate</code><br />
<code>Volume ID: 2ec69e5b-0d04-4a3e-94c3-337b4302fbe8</code><br />
<code>Status: Started</code><br />
<code>Number of Bricks: 3 x 2 = 6</code><br />
<code>Transport-type: tcp</code><br />
<code>Bricks:</code><br />
<code>Brick1: rhgs1:/rhgs/brick_xvdc/repvol</code><br />
<code>Brick2: rhgs2:/rhgs/brick_xvdc/repvol</code><br />
<code>Brick3: rhgs3:/rhgs/brick_xvdc/repvol</code><br />
<code>Brick4: rhgs4:/rhgs/brick_xvdc/repvol</code><br />
<code>Brick5: rhgs5:/rhgs/brick_xvdc/repvol</code><br />
<code>Brick6: rhgs6:/rhgs/brick_xvdc/repvol</code><br />
<code>Options Reconfigured:</code><br />
<code>performance.readdir-ahead: on</code></p>
<p>Start the rebalance operation on the <strong>repvol</strong> volume.</p>
<pre class="codehilite"><code class="language-bash">sudo gluster volume rebalance repvol start</code></pre>


<p><code>volume rebalance: repvol: success: Rebalance on repvol has been started successfully. Use rebalance status command to check status of the rebalance process.</code><br />
<code>ID: e13d3c36-9531-4411-98aa-c974de5e2219</code></p>
<p>Take a look at the status of the rebalance. If you run this command quickly after the <code>rebalance start</code> above you may catch the <em>localhost</em> in the <em>in progress</em> status, but due to the limited scale of the lab the rebalance may complete before you run this command and instead show the <em>completed</em> status for this node.</p>
<pre class="codehilite"><code class="language-bash">sudo gluster volume rebalance repvol status</code></pre>


<p><code>Node Rebalanced-files          size       scanned      failures       skipped               status   run time in secs</code><br />
<code>---------      -----------   -----------   -----------   -----------   -----------         ------------     --------------</code><br />
<code>localhost               79         1.1KB           210             0             0          in progress               1.00</code><br />
<code>rhgs2                0        0Bytes             0             0             0            completed               0.00</code><br />
<code>rhgs3                0        0Bytes             0             0             0            completed               0.00</code><br />
<code>rhgs4                0        0Bytes             0             0             0            completed               0.00</code><br />
<code>rhgs5                0        0Bytes             0             0             0            completed               0.00</code><br />
<code>rhgs6                0        0Bytes             0             0             0            completed               0.00</code><br />
<code>volume rebalance: repvol: success</code></p>
<p>Now take a look again at the count of files in the brick backend for <strong>repvol</strong> on node <strong>rhgs1</strong>. You will find that the number of files has reduced considerably.</p>
<pre class="codehilite"><code class="language-bash">ls /rhgs/brick_xvdc/repvol/mydir | wc -l</code></pre>


<p><code>75</code></p>
<p>To further illustrate the effect of the rebalance, connect to node <strong>rhgs5</strong> via SSH and look at the file count in the brick backend there.</p>
<pre class="codehilite"><code class="language-bash">ssh gluster@rhgs5</code></pre>


<pre class="codehilite"><code class="language-bash">ls /rhgs/brick_xvdc/repvol/mydir | wc -l</code></pre>


<p><code>66</code></p>
<p>Return to node <strong>rhgs1</strong>.</p>
<pre class="codehilite"><code class="language-bash">exit</code></pre>


<h3 id="the-gstatus-command">The gstatus Command</h3>
<p>A good summary view of your volume is available through the <code>gstatus</code> command. Passing the <code>-l</code> flag to this command will also provide a visual of the volume layout in ASCII format, which can be very handy for understanding the distribute branching and replica pairing.</p>
<pre class="codehilite"><code class="language-bash">sudo gstatus -v repvol -l -w</code></pre>


<p><code>Product: RHGS Server v3.1Update3  Capacity:  60.00 GiB(raw bricks)</code><br />
<code>Status: HEALTHY                      201.00 MiB(raw used)</code><br />
<code>Glusterfs: 3.7.5                         30.00 GiB(usable from volumes)</code><br />
<code>OverCommit: No                Snapshots:   0</code><br />
<code></code><br />
<code>Volume Information</code><br />
<code>repvol           UP - 6/6 bricks up - Distributed-Replicate</code><br />
<code>Capacity: (0% used) 100.00 MiB/30.00 GiB (used/total)</code><br />
<code>Snapshots: 0</code><br />
<code>Self Heal:  6/ 6</code><br />
<code>Tasks Active: None</code><br />
<code>Protocols: glusterfs:on  NFS:on  SMB:on</code><br />
<code>Gluster Connectivty: 7 hosts, 78 tcp connections</code><br />
<code></code><br />
<code>repvol---------- +</code><br />
<code>|</code><br />
<code>Distribute (dht)</code><br />
<code>|</code><br />
<code>+-- Replica Set0 (afr)</code><br />
<code>|     |</code><br />
<code>|     +--rhgs1:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB</code><br />
<code>|     |</code><br />
<code>|     +--rhgs2:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB</code><br />
<code>|</code><br />
<code>+-- Replica Set1 (afr)</code><br />
<code>|     |</code><br />
<code>|     +--rhgs3:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB</code><br />
<code>|     |</code><br />
<code>|     +--rhgs4:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB</code><br />
<code>|</code><br />
<code>+-- Replica Set2 (afr)</code><br />
<code>|</code><br />
<code>+--rhgs5:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB</code><br />
<code>|</code><br />
<code>+--rhgs6:/rhgs/brick_xvdc/repvol(UP) 33.00 MiB/10.00 GiB</code></p>
<h2 id="changing-volume-configuration">Changing Volume Configuration</h2>
<h2 id="analyzing-volume-performance">Analyzing Volume Performance</h2>
<h2 id="administration-of-volume-quotas">Administration of Volume Quotas</h2>
<h1 id="end-of-module-3">End of Module 3</h1>
<p>This concludes <strong>Gluster Test Drive Module 3 - Volume Operations and Administration</strong>. You may continue now with Module 4, or return at any time to access the modules in any order you wish.</p></div>
        </div>

        <footer class="col-md-12">
            <hr>
            
                <center>Copyright ©2016 Red Hat, Inc.</center>
            
            <center>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</center>
        </footer>

        <script src="../js/jquery-1.10.2.min.js"></script>
        <script src="../js/bootstrap-3.0.3.min.js"></script>
        <script src="../js/highlight.pack.js"></script>
        <script>var base_url = '..';</script>
        <script data-main="../mkdocs/js/search.js" src="../mkdocs/js/require.js"></script>
        <script src="../js/base.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="modal-header">
                        <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                        <h4 class="modal-title" id="exampleModalLabel">Search</h4>
                    </div>
                    <div class="modal-body">
                        <p>
                            From here you can search these documents. Enter
                            your search terms below.
                        </p>
                        <form role="form">
                            <div class="form-group">
                                <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                            </div>
                        </form>
                        <div id="mkdocs-search-results"></div>
                    </div>
                    <div class="modal-footer">
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>